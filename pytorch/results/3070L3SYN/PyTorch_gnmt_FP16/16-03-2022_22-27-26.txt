0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070
Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2461
0: Scheduler decay interval: 308
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/1848]	Time 0.204 (0.000)	Data 1.10e-01 (0.00e+00)	Tok/s 17741 (0)	Loss/tok 10.5912 (10.5912)	LR 2.047e-05
0: TRAIN [0][10/1848]	Time 0.121 (0.122)	Data 6.94e-05 (5.04e-05)	Tok/s 29523 (29036)	Loss/tok 9.7189 (10.1349)	LR 2.576e-05
0: TRAIN [0][20/1848]	Time 0.097 (0.122)	Data 4.36e-05 (4.93e-05)	Tok/s 22062 (27516)	Loss/tok 9.0378 (9.8274)	LR 3.244e-05
0: TRAIN [0][30/1848]	Time 0.153 (0.125)	Data 4.91e-05 (4.92e-05)	Tok/s 32466 (28040)	Loss/tok 9.0055 (9.5785)	LR 4.083e-05
0: TRAIN [0][40/1848]	Time 0.124 (0.121)	Data 4.72e-05 (4.95e-05)	Tok/s 28780 (27368)	Loss/tok 8.7691 (9.4263)	LR 5.141e-05
0: TRAIN [0][50/1848]	Time 0.153 (0.120)	Data 4.34e-05 (4.90e-05)	Tok/s 32513 (27256)	Loss/tok 8.9142 (9.2936)	LR 6.472e-05
0: TRAIN [0][60/1848]	Time 0.125 (0.122)	Data 4.34e-05 (4.86e-05)	Tok/s 29259 (27644)	Loss/tok 8.6687 (9.1615)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][70/1848]	Time 0.151 (0.125)	Data 4.36e-05 (4.93e-05)	Tok/s 32646 (28316)	Loss/tok 8.3456 (9.0707)	LR 1.026e-04
0: TRAIN [0][80/1848]	Time 0.127 (0.125)	Data 4.82e-05 (4.95e-05)	Tok/s 28338 (28296)	Loss/tok 8.0534 (8.9627)	LR 1.291e-04
0: TRAIN [0][90/1848]	Time 0.095 (0.124)	Data 4.34e-05 (4.99e-05)	Tok/s 23550 (28002)	Loss/tok 7.9235 (8.8701)	LR 1.626e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][100/1848]	Time 0.154 (0.123)	Data 4.58e-05 (5.12e-05)	Tok/s 32995 (28039)	Loss/tok 7.9150 (8.7798)	LR 2.047e-04
0: TRAIN [0][110/1848]	Time 0.124 (0.125)	Data 7.44e-05 (5.21e-05)	Tok/s 28415 (28303)	Loss/tok 7.7304 (8.6859)	LR 2.576e-04
0: TRAIN [0][120/1848]	Time 0.096 (0.124)	Data 4.98e-05 (5.19e-05)	Tok/s 22597 (28070)	Loss/tok 7.4818 (8.6246)	LR 3.244e-04
0: TRAIN [0][130/1848]	Time 0.150 (0.124)	Data 4.48e-05 (5.21e-05)	Tok/s 33099 (27947)	Loss/tok 7.8260 (8.5623)	LR 4.083e-04
0: TRAIN [0][140/1848]	Time 0.070 (0.124)	Data 4.48e-05 (5.16e-05)	Tok/s 15649 (27917)	Loss/tok 7.3713 (8.5050)	LR 5.141e-04
0: TRAIN [0][150/1848]	Time 0.124 (0.123)	Data 4.41e-05 (5.19e-05)	Tok/s 29424 (27810)	Loss/tok 7.8309 (8.4559)	LR 6.472e-04
0: TRAIN [0][160/1848]	Time 0.124 (0.124)	Data 6.77e-05 (5.19e-05)	Tok/s 29272 (27858)	Loss/tok 7.6131 (8.4078)	LR 8.148e-04
0: TRAIN [0][170/1848]	Time 0.124 (0.124)	Data 4.43e-05 (5.18e-05)	Tok/s 28760 (27897)	Loss/tok 7.8473 (8.3650)	LR 1.026e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][180/1848]	Time 0.064 (0.124)	Data 4.22e-05 (5.16e-05)	Tok/s 33407 (27976)	Loss/tok 7.3849 (8.3314)	LR 1.291e-03
0: TRAIN [0][190/1848]	Time 0.125 (0.124)	Data 4.43e-05 (5.13e-05)	Tok/s 28939 (28060)	Loss/tok 7.5243 (8.2955)	LR 1.626e-03
0: TRAIN [0][200/1848]	Time 0.099 (0.125)	Data 4.89e-05 (5.15e-05)	Tok/s 22989 (28048)	Loss/tok 7.4214 (8.2686)	LR 2.000e-03
0: TRAIN [0][210/1848]	Time 0.125 (0.124)	Data 4.77e-05 (5.16e-05)	Tok/s 27933 (27983)	Loss/tok 7.5716 (8.2378)	LR 2.000e-03
0: TRAIN [0][220/1848]	Time 0.123 (0.125)	Data 7.06e-05 (5.14e-05)	Tok/s 29121 (28063)	Loss/tok 7.4339 (8.2012)	LR 2.000e-03
0: TRAIN [0][230/1848]	Time 0.153 (0.125)	Data 4.39e-05 (5.14e-05)	Tok/s 32618 (28090)	Loss/tok 7.6559 (8.1664)	LR 2.000e-03
0: TRAIN [0][240/1848]	Time 0.097 (0.126)	Data 4.79e-05 (5.13e-05)	Tok/s 22696 (28114)	Loss/tok 6.9944 (8.1337)	LR 2.000e-03
0: TRAIN [0][250/1848]	Time 0.151 (0.127)	Data 8.99e-05 (5.13e-05)	Tok/s 32838 (28201)	Loss/tok 7.2919 (8.0956)	LR 2.000e-03
0: TRAIN [0][260/1848]	Time 0.122 (0.126)	Data 4.34e-05 (5.12e-05)	Tok/s 29961 (28183)	Loss/tok 7.1171 (8.0606)	LR 2.000e-03
0: TRAIN [0][270/1848]	Time 0.123 (0.126)	Data 4.32e-05 (5.09e-05)	Tok/s 28768 (28057)	Loss/tok 7.1266 (8.0318)	LR 2.000e-03
0: TRAIN [0][280/1848]	Time 0.154 (0.126)	Data 7.34e-05 (5.12e-05)	Tok/s 33148 (28110)	Loss/tok 7.2064 (7.9966)	LR 2.000e-03
0: TRAIN [0][290/1848]	Time 0.152 (0.126)	Data 6.68e-05 (5.13e-05)	Tok/s 32839 (28122)	Loss/tok 7.2112 (7.9651)	LR 2.000e-03
0: TRAIN [0][300/1848]	Time 0.153 (0.126)	Data 4.43e-05 (5.10e-05)	Tok/s 33833 (28136)	Loss/tok 7.0520 (7.9311)	LR 2.000e-03
0: TRAIN [0][310/1848]	Time 0.154 (0.127)	Data 4.46e-05 (5.11e-05)	Tok/s 33245 (28172)	Loss/tok 7.1268 (7.9002)	LR 2.000e-03
0: TRAIN [0][320/1848]	Time 0.188 (0.127)	Data 7.06e-05 (5.12e-05)	Tok/s 34386 (28235)	Loss/tok 7.0834 (7.8658)	LR 2.000e-03
0: TRAIN [0][330/1848]	Time 0.098 (0.128)	Data 7.63e-05 (5.13e-05)	Tok/s 22919 (28331)	Loss/tok 6.4515 (7.8298)	LR 2.000e-03
0: TRAIN [0][340/1848]	Time 0.191 (0.129)	Data 4.36e-05 (5.11e-05)	Tok/s 34600 (28404)	Loss/tok 7.1153 (7.7989)	LR 2.000e-03
0: TRAIN [0][350/1848]	Time 0.096 (0.128)	Data 4.32e-05 (5.10e-05)	Tok/s 22450 (28415)	Loss/tok 6.5800 (7.7709)	LR 2.000e-03
0: TRAIN [0][360/1848]	Time 0.097 (0.128)	Data 4.41e-05 (5.08e-05)	Tok/s 22333 (28407)	Loss/tok 6.3589 (7.7435)	LR 2.000e-03
0: TRAIN [0][370/1848]	Time 0.124 (0.128)	Data 4.34e-05 (5.08e-05)	Tok/s 28719 (28436)	Loss/tok 6.4887 (7.7135)	LR 2.000e-03
0: TRAIN [0][380/1848]	Time 0.098 (0.129)	Data 4.41e-05 (5.08e-05)	Tok/s 22353 (28458)	Loss/tok 6.3129 (7.6840)	LR 2.000e-03
0: TRAIN [0][390/1848]	Time 0.126 (0.129)	Data 4.27e-05 (5.08e-05)	Tok/s 28356 (28467)	Loss/tok 6.4967 (7.6562)	LR 2.000e-03
0: TRAIN [0][400/1848]	Time 0.100 (0.128)	Data 4.51e-05 (5.08e-05)	Tok/s 21756 (28414)	Loss/tok 6.2939 (7.6318)	LR 2.000e-03
0: TRAIN [0][410/1848]	Time 0.188 (0.129)	Data 4.39e-05 (5.08e-05)	Tok/s 34708 (28426)	Loss/tok 6.7111 (7.6047)	LR 2.000e-03
0: TRAIN [0][420/1848]	Time 0.126 (0.129)	Data 4.46e-05 (5.07e-05)	Tok/s 28810 (28434)	Loss/tok 6.5727 (7.5789)	LR 2.000e-03
0: TRAIN [0][430/1848]	Time 0.125 (0.129)	Data 4.63e-05 (5.07e-05)	Tok/s 28685 (28447)	Loss/tok 6.3869 (7.5537)	LR 2.000e-03
0: TRAIN [0][440/1848]	Time 0.125 (0.129)	Data 7.30e-05 (5.06e-05)	Tok/s 28745 (28416)	Loss/tok 6.3602 (7.5306)	LR 2.000e-03
0: TRAIN [0][450/1848]	Time 0.183 (0.128)	Data 7.20e-05 (5.07e-05)	Tok/s 35926 (28393)	Loss/tok 6.6693 (7.5075)	LR 2.000e-03
0: TRAIN [0][460/1848]	Time 0.125 (0.128)	Data 4.63e-05 (5.07e-05)	Tok/s 28686 (28362)	Loss/tok 6.4741 (7.4851)	LR 2.000e-03
0: TRAIN [0][470/1848]	Time 0.189 (0.128)	Data 4.32e-05 (5.05e-05)	Tok/s 34745 (28380)	Loss/tok 6.6504 (7.4606)	LR 2.000e-03
0: TRAIN [0][480/1848]	Time 0.123 (0.128)	Data 4.32e-05 (5.05e-05)	Tok/s 28658 (28374)	Loss/tok 6.1768 (7.4381)	LR 2.000e-03
0: TRAIN [0][490/1848]	Time 0.122 (0.128)	Data 4.43e-05 (5.04e-05)	Tok/s 29190 (28363)	Loss/tok 6.1371 (7.4150)	LR 2.000e-03
0: TRAIN [0][500/1848]	Time 0.155 (0.128)	Data 4.32e-05 (5.03e-05)	Tok/s 32136 (28321)	Loss/tok 6.3861 (7.3946)	LR 2.000e-03
0: TRAIN [0][510/1848]	Time 0.185 (0.128)	Data 4.91e-05 (5.04e-05)	Tok/s 34960 (28314)	Loss/tok 6.5801 (7.3730)	LR 2.000e-03
0: TRAIN [0][520/1848]	Time 0.125 (0.128)	Data 4.55e-05 (5.03e-05)	Tok/s 28396 (28269)	Loss/tok 6.0859 (7.3539)	LR 2.000e-03
0: TRAIN [0][530/1848]	Time 0.155 (0.128)	Data 4.48e-05 (5.04e-05)	Tok/s 32344 (28232)	Loss/tok 6.4177 (7.3334)	LR 2.000e-03
0: TRAIN [0][540/1848]	Time 0.152 (0.128)	Data 4.86e-05 (5.04e-05)	Tok/s 33891 (28234)	Loss/tok 6.2003 (7.3120)	LR 2.000e-03
0: TRAIN [0][550/1848]	Time 0.125 (0.128)	Data 4.32e-05 (5.05e-05)	Tok/s 29044 (28281)	Loss/tok 5.9827 (7.2873)	LR 2.000e-03
0: TRAIN [0][560/1848]	Time 0.152 (0.128)	Data 6.99e-05 (5.06e-05)	Tok/s 33220 (28269)	Loss/tok 6.1611 (7.2663)	LR 2.000e-03
0: TRAIN [0][570/1848]	Time 0.125 (0.128)	Data 4.70e-05 (5.06e-05)	Tok/s 29057 (28248)	Loss/tok 5.8091 (7.2457)	LR 2.000e-03
0: TRAIN [0][580/1848]	Time 0.155 (0.128)	Data 4.32e-05 (5.06e-05)	Tok/s 32999 (28275)	Loss/tok 6.1816 (7.2234)	LR 2.000e-03
0: TRAIN [0][590/1848]	Time 0.070 (0.128)	Data 4.41e-05 (5.07e-05)	Tok/s 15271 (28240)	Loss/tok 5.2959 (7.2050)	LR 2.000e-03
0: TRAIN [0][600/1848]	Time 0.152 (0.128)	Data 4.48e-05 (5.06e-05)	Tok/s 32516 (28216)	Loss/tok 6.1959 (7.1872)	LR 2.000e-03
0: TRAIN [0][610/1848]	Time 0.192 (0.128)	Data 4.34e-05 (5.07e-05)	Tok/s 33576 (28238)	Loss/tok 6.3031 (7.1658)	LR 2.000e-03
0: TRAIN [0][620/1848]	Time 0.124 (0.128)	Data 6.77e-05 (5.08e-05)	Tok/s 28419 (28211)	Loss/tok 5.8464 (7.1474)	LR 2.000e-03
0: TRAIN [0][630/1848]	Time 0.125 (0.128)	Data 4.41e-05 (5.07e-05)	Tok/s 28675 (28223)	Loss/tok 5.8580 (7.1262)	LR 2.000e-03
0: TRAIN [0][640/1848]	Time 0.126 (0.128)	Data 4.32e-05 (5.07e-05)	Tok/s 29315 (28228)	Loss/tok 5.8992 (7.1063)	LR 2.000e-03
0: TRAIN [0][650/1848]	Time 0.154 (0.128)	Data 4.98e-05 (5.07e-05)	Tok/s 32168 (28210)	Loss/tok 5.9798 (7.0885)	LR 2.000e-03
0: TRAIN [0][660/1848]	Time 0.154 (0.128)	Data 4.41e-05 (5.07e-05)	Tok/s 32179 (28251)	Loss/tok 6.0136 (7.0669)	LR 2.000e-03
0: TRAIN [0][670/1848]	Time 0.096 (0.128)	Data 4.34e-05 (5.07e-05)	Tok/s 22126 (28251)	Loss/tok 5.3723 (7.0480)	LR 2.000e-03
0: TRAIN [0][680/1848]	Time 0.122 (0.128)	Data 4.41e-05 (5.07e-05)	Tok/s 29501 (28236)	Loss/tok 5.4973 (7.0292)	LR 2.000e-03
0: TRAIN [0][690/1848]	Time 0.123 (0.128)	Data 4.39e-05 (5.06e-05)	Tok/s 29230 (28240)	Loss/tok 5.6754 (7.0102)	LR 2.000e-03
0: TRAIN [0][700/1848]	Time 0.154 (0.128)	Data 4.48e-05 (5.05e-05)	Tok/s 32747 (28254)	Loss/tok 5.8203 (6.9911)	LR 2.000e-03
0: TRAIN [0][710/1848]	Time 0.126 (0.128)	Data 4.29e-05 (5.05e-05)	Tok/s 28350 (28225)	Loss/tok 5.5801 (6.9750)	LR 2.000e-03
0: TRAIN [0][720/1848]	Time 0.189 (0.128)	Data 4.27e-05 (5.05e-05)	Tok/s 34460 (28226)	Loss/tok 6.0243 (6.9571)	LR 2.000e-03
0: TRAIN [0][730/1848]	Time 0.122 (0.128)	Data 4.43e-05 (5.04e-05)	Tok/s 29362 (28246)	Loss/tok 5.6233 (6.9379)	LR 2.000e-03
0: TRAIN [0][740/1848]	Time 0.191 (0.128)	Data 4.36e-05 (5.04e-05)	Tok/s 34668 (28248)	Loss/tok 5.9569 (6.9205)	LR 2.000e-03
0: TRAIN [0][750/1848]	Time 0.125 (0.128)	Data 4.51e-05 (5.04e-05)	Tok/s 29022 (28265)	Loss/tok 5.4538 (6.9012)	LR 2.000e-03
0: TRAIN [0][760/1848]	Time 0.122 (0.128)	Data 6.84e-05 (5.04e-05)	Tok/s 29223 (28292)	Loss/tok 5.4952 (6.8811)	LR 2.000e-03
0: TRAIN [0][770/1848]	Time 0.097 (0.129)	Data 4.48e-05 (5.03e-05)	Tok/s 21826 (28322)	Loss/tok 5.2063 (6.8615)	LR 2.000e-03
0: TRAIN [0][780/1848]	Time 0.151 (0.129)	Data 4.36e-05 (5.04e-05)	Tok/s 33360 (28317)	Loss/tok 5.6348 (6.8446)	LR 2.000e-03
0: TRAIN [0][790/1848]	Time 0.122 (0.129)	Data 7.03e-05 (5.05e-05)	Tok/s 29424 (28354)	Loss/tok 5.4622 (6.8242)	LR 2.000e-03
0: TRAIN [0][800/1848]	Time 0.125 (0.129)	Data 6.91e-05 (5.05e-05)	Tok/s 28525 (28355)	Loss/tok 5.3788 (6.8074)	LR 2.000e-03
0: TRAIN [0][810/1848]	Time 0.126 (0.129)	Data 4.34e-05 (5.06e-05)	Tok/s 29050 (28360)	Loss/tok 5.4292 (6.7913)	LR 2.000e-03
0: TRAIN [0][820/1848]	Time 0.097 (0.129)	Data 4.96e-05 (5.06e-05)	Tok/s 22466 (28336)	Loss/tok 5.0057 (6.7764)	LR 2.000e-03
0: TRAIN [0][830/1848]	Time 0.123 (0.129)	Data 4.43e-05 (5.05e-05)	Tok/s 29283 (28316)	Loss/tok 5.3679 (6.7621)	LR 2.000e-03
0: TRAIN [0][840/1848]	Time 0.096 (0.129)	Data 4.36e-05 (5.06e-05)	Tok/s 22878 (28332)	Loss/tok 4.9596 (6.7453)	LR 2.000e-03
0: TRAIN [0][850/1848]	Time 0.099 (0.129)	Data 4.82e-05 (5.06e-05)	Tok/s 21467 (28339)	Loss/tok 4.9000 (6.7286)	LR 2.000e-03
0: TRAIN [0][860/1848]	Time 0.092 (0.129)	Data 7.01e-05 (5.06e-05)	Tok/s 22926 (28325)	Loss/tok 4.9165 (6.7137)	LR 2.000e-03
0: TRAIN [0][870/1848]	Time 0.096 (0.129)	Data 4.24e-05 (5.06e-05)	Tok/s 22406 (28304)	Loss/tok 4.6536 (6.6994)	LR 2.000e-03
0: TRAIN [0][880/1848]	Time 0.190 (0.129)	Data 4.48e-05 (5.06e-05)	Tok/s 34391 (28301)	Loss/tok 5.6398 (6.6839)	LR 2.000e-03
0: TRAIN [0][890/1848]	Time 0.097 (0.129)	Data 4.39e-05 (5.06e-05)	Tok/s 22192 (28300)	Loss/tok 4.5970 (6.6687)	LR 2.000e-03
0: TRAIN [0][900/1848]	Time 0.153 (0.129)	Data 4.32e-05 (5.05e-05)	Tok/s 33477 (28326)	Loss/tok 5.3331 (6.6508)	LR 2.000e-03
0: TRAIN [0][910/1848]	Time 0.151 (0.129)	Data 4.29e-05 (5.05e-05)	Tok/s 33704 (28334)	Loss/tok 5.3438 (6.6349)	LR 2.000e-03
0: TRAIN [0][920/1848]	Time 0.070 (0.129)	Data 4.34e-05 (5.05e-05)	Tok/s 14827 (28347)	Loss/tok 4.5247 (6.6189)	LR 2.000e-03
0: TRAIN [0][930/1848]	Time 0.121 (0.129)	Data 4.43e-05 (5.05e-05)	Tok/s 30248 (28332)	Loss/tok 4.9400 (6.6045)	LR 2.000e-03
0: TRAIN [0][940/1848]	Time 0.099 (0.129)	Data 4.27e-05 (5.04e-05)	Tok/s 22786 (28316)	Loss/tok 4.9175 (6.5908)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][950/1848]	Time 0.093 (0.129)	Data 6.68e-05 (5.05e-05)	Tok/s 39334 (28335)	Loss/tok 4.9541 (6.5755)	LR 2.000e-03
0: TRAIN [0][960/1848]	Time 0.124 (0.129)	Data 4.60e-05 (5.05e-05)	Tok/s 29219 (28365)	Loss/tok 5.2586 (6.5584)	LR 2.000e-03
0: TRAIN [0][970/1848]	Time 0.126 (0.129)	Data 4.29e-05 (5.04e-05)	Tok/s 28896 (28370)	Loss/tok 5.1138 (6.5442)	LR 2.000e-03
0: TRAIN [0][980/1848]	Time 0.094 (0.129)	Data 4.36e-05 (5.05e-05)	Tok/s 22794 (28359)	Loss/tok 4.5799 (6.5303)	LR 2.000e-03
0: TRAIN [0][990/1848]	Time 0.125 (0.129)	Data 4.24e-05 (5.04e-05)	Tok/s 29350 (28403)	Loss/tok 5.0091 (6.5139)	LR 2.000e-03
0: TRAIN [0][1000/1848]	Time 0.154 (0.129)	Data 4.46e-05 (5.03e-05)	Tok/s 32833 (28409)	Loss/tok 5.1888 (6.4993)	LR 2.000e-03
0: TRAIN [0][1010/1848]	Time 0.152 (0.129)	Data 4.43e-05 (5.03e-05)	Tok/s 32955 (28428)	Loss/tok 5.2473 (6.4841)	LR 2.000e-03
0: TRAIN [0][1020/1848]	Time 0.070 (0.130)	Data 4.46e-05 (5.03e-05)	Tok/s 15214 (28440)	Loss/tok 4.4806 (6.4696)	LR 2.000e-03
0: TRAIN [0][1030/1848]	Time 0.150 (0.130)	Data 4.34e-05 (5.02e-05)	Tok/s 33123 (28459)	Loss/tok 5.2196 (6.4545)	LR 2.000e-03
0: TRAIN [0][1040/1848]	Time 0.153 (0.130)	Data 4.29e-05 (5.02e-05)	Tok/s 32875 (28462)	Loss/tok 5.1622 (6.4406)	LR 2.000e-03
0: TRAIN [0][1050/1848]	Time 0.099 (0.130)	Data 4.32e-05 (5.02e-05)	Tok/s 21395 (28470)	Loss/tok 4.8277 (6.4270)	LR 2.000e-03
0: TRAIN [0][1060/1848]	Time 0.157 (0.130)	Data 4.48e-05 (5.02e-05)	Tok/s 32151 (28480)	Loss/tok 4.9584 (6.4129)	LR 2.000e-03
0: TRAIN [0][1070/1848]	Time 0.125 (0.130)	Data 4.32e-05 (5.02e-05)	Tok/s 28171 (28474)	Loss/tok 4.7874 (6.4002)	LR 2.000e-03
0: TRAIN [0][1080/1848]	Time 0.125 (0.130)	Data 4.70e-05 (5.02e-05)	Tok/s 28360 (28466)	Loss/tok 4.7519 (6.3877)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1090/1848]	Time 0.124 (0.130)	Data 4.36e-05 (5.02e-05)	Tok/s 40449 (28468)	Loss/tok 5.1197 (6.3751)	LR 2.000e-03
0: TRAIN [0][1100/1848]	Time 0.151 (0.129)	Data 4.32e-05 (5.02e-05)	Tok/s 33073 (28452)	Loss/tok 5.1434 (6.3639)	LR 2.000e-03
0: TRAIN [0][1110/1848]	Time 0.123 (0.129)	Data 4.41e-05 (5.01e-05)	Tok/s 29367 (28449)	Loss/tok 4.8806 (6.3515)	LR 2.000e-03
0: TRAIN [0][1120/1848]	Time 0.065 (0.129)	Data 4.39e-05 (5.01e-05)	Tok/s 16279 (28414)	Loss/tok 4.3558 (6.3415)	LR 2.000e-03
0: TRAIN [0][1130/1848]	Time 0.190 (0.129)	Data 4.77e-05 (5.01e-05)	Tok/s 34087 (28431)	Loss/tok 5.1957 (6.3274)	LR 2.000e-03
0: TRAIN [0][1140/1848]	Time 0.157 (0.129)	Data 4.43e-05 (5.00e-05)	Tok/s 32027 (28421)	Loss/tok 5.0219 (6.3159)	LR 2.000e-03
0: TRAIN [0][1150/1848]	Time 0.186 (0.129)	Data 6.87e-05 (5.01e-05)	Tok/s 35287 (28413)	Loss/tok 5.1894 (6.3044)	LR 2.000e-03
0: TRAIN [0][1160/1848]	Time 0.124 (0.129)	Data 7.68e-05 (5.01e-05)	Tok/s 29384 (28408)	Loss/tok 4.7528 (6.2921)	LR 2.000e-03
0: TRAIN [0][1170/1848]	Time 0.125 (0.130)	Data 4.43e-05 (5.01e-05)	Tok/s 28185 (28433)	Loss/tok 4.8069 (6.2776)	LR 2.000e-03
0: TRAIN [0][1180/1848]	Time 0.152 (0.129)	Data 4.43e-05 (5.01e-05)	Tok/s 33316 (28420)	Loss/tok 4.7701 (6.2662)	LR 2.000e-03
0: TRAIN [0][1190/1848]	Time 0.124 (0.130)	Data 4.63e-05 (5.01e-05)	Tok/s 29121 (28443)	Loss/tok 4.6809 (6.2531)	LR 2.000e-03
0: TRAIN [0][1200/1848]	Time 0.155 (0.130)	Data 4.32e-05 (5.00e-05)	Tok/s 32580 (28453)	Loss/tok 4.9042 (6.2408)	LR 2.000e-03
0: TRAIN [0][1210/1848]	Time 0.153 (0.130)	Data 4.46e-05 (5.01e-05)	Tok/s 33410 (28455)	Loss/tok 4.8554 (6.2293)	LR 2.000e-03
0: TRAIN [0][1220/1848]	Time 0.185 (0.130)	Data 7.34e-05 (5.01e-05)	Tok/s 34935 (28456)	Loss/tok 5.2180 (6.2178)	LR 2.000e-03
0: TRAIN [0][1230/1848]	Time 0.097 (0.130)	Data 4.29e-05 (5.00e-05)	Tok/s 22022 (28460)	Loss/tok 4.6079 (6.2064)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1240/1848]	Time 0.096 (0.130)	Data 4.34e-05 (5.01e-05)	Tok/s 22446 (28466)	Loss/tok 4.4477 (6.1951)	LR 2.000e-03
0: TRAIN [0][1250/1848]	Time 0.097 (0.130)	Data 7.63e-05 (5.01e-05)	Tok/s 23476 (28465)	Loss/tok 4.6001 (6.1842)	LR 2.000e-03
0: TRAIN [0][1260/1848]	Time 0.094 (0.130)	Data 4.98e-05 (5.00e-05)	Tok/s 23128 (28467)	Loss/tok 4.2189 (6.1725)	LR 2.000e-03
0: TRAIN [0][1270/1848]	Time 0.125 (0.130)	Data 4.27e-05 (5.01e-05)	Tok/s 29084 (28483)	Loss/tok 4.7589 (6.1604)	LR 2.000e-03
0: TRAIN [0][1280/1848]	Time 0.126 (0.130)	Data 7.70e-05 (5.01e-05)	Tok/s 29065 (28481)	Loss/tok 4.7043 (6.1496)	LR 2.000e-03
0: TRAIN [0][1290/1848]	Time 0.098 (0.130)	Data 4.22e-05 (5.01e-05)	Tok/s 22378 (28482)	Loss/tok 4.2987 (6.1383)	LR 2.000e-03
0: TRAIN [0][1300/1848]	Time 0.097 (0.130)	Data 4.24e-05 (5.01e-05)	Tok/s 22537 (28476)	Loss/tok 4.4092 (6.1287)	LR 2.000e-03
0: TRAIN [0][1310/1848]	Time 0.123 (0.130)	Data 4.32e-05 (5.01e-05)	Tok/s 29207 (28481)	Loss/tok 4.6167 (6.1180)	LR 2.000e-03
0: TRAIN [0][1320/1848]	Time 0.124 (0.130)	Data 4.36e-05 (5.01e-05)	Tok/s 28964 (28490)	Loss/tok 4.5779 (6.1069)	LR 2.000e-03
0: TRAIN [0][1330/1848]	Time 0.190 (0.130)	Data 4.34e-05 (5.01e-05)	Tok/s 34293 (28480)	Loss/tok 5.1115 (6.0972)	LR 2.000e-03
0: TRAIN [0][1340/1848]	Time 0.152 (0.130)	Data 4.36e-05 (5.01e-05)	Tok/s 32988 (28463)	Loss/tok 4.8921 (6.0885)	LR 2.000e-03
0: TRAIN [0][1350/1848]	Time 0.126 (0.130)	Data 4.27e-05 (5.01e-05)	Tok/s 28736 (28466)	Loss/tok 4.6779 (6.0782)	LR 2.000e-03
0: TRAIN [0][1360/1848]	Time 0.152 (0.130)	Data 7.08e-05 (5.02e-05)	Tok/s 33185 (28488)	Loss/tok 4.8024 (6.0661)	LR 2.000e-03
0: TRAIN [0][1370/1848]	Time 0.126 (0.130)	Data 4.34e-05 (5.02e-05)	Tok/s 28799 (28499)	Loss/tok 4.6245 (6.0555)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1380/1848]	Time 0.157 (0.130)	Data 6.99e-05 (5.02e-05)	Tok/s 40913 (28503)	Loss/tok 5.2027 (6.0460)	LR 2.000e-03
0: TRAIN [0][1390/1848]	Time 0.153 (0.130)	Data 4.43e-05 (5.02e-05)	Tok/s 32902 (28525)	Loss/tok 4.8012 (6.0340)	LR 2.000e-03
0: TRAIN [0][1400/1848]	Time 0.155 (0.130)	Data 7.51e-05 (5.02e-05)	Tok/s 32973 (28540)	Loss/tok 4.7403 (6.0230)	LR 2.000e-03
0: TRAIN [0][1410/1848]	Time 0.151 (0.130)	Data 4.39e-05 (5.01e-05)	Tok/s 33775 (28543)	Loss/tok 4.7580 (6.0134)	LR 2.000e-03
0: TRAIN [0][1420/1848]	Time 0.155 (0.130)	Data 4.84e-05 (5.01e-05)	Tok/s 33043 (28541)	Loss/tok 4.7694 (6.0041)	LR 2.000e-03
0: TRAIN [0][1430/1848]	Time 0.153 (0.130)	Data 4.63e-05 (5.01e-05)	Tok/s 32722 (28545)	Loss/tok 4.7099 (5.9945)	LR 2.000e-03
0: TRAIN [0][1440/1848]	Time 0.155 (0.131)	Data 4.29e-05 (5.01e-05)	Tok/s 32579 (28548)	Loss/tok 4.6225 (5.9846)	LR 2.000e-03
0: TRAIN [0][1450/1848]	Time 0.097 (0.131)	Data 4.41e-05 (5.01e-05)	Tok/s 23499 (28557)	Loss/tok 4.4520 (5.9745)	LR 2.000e-03
0: TRAIN [0][1460/1848]	Time 0.121 (0.131)	Data 4.32e-05 (5.01e-05)	Tok/s 29673 (28544)	Loss/tok 4.2359 (5.9658)	LR 2.000e-03
0: TRAIN [0][1470/1848]	Time 0.192 (0.131)	Data 4.36e-05 (5.01e-05)	Tok/s 33844 (28535)	Loss/tok 4.9101 (5.9572)	LR 2.000e-03
0: TRAIN [0][1480/1848]	Time 0.193 (0.131)	Data 4.51e-05 (5.01e-05)	Tok/s 33606 (28537)	Loss/tok 4.9048 (5.9481)	LR 2.000e-03
0: TRAIN [0][1490/1848]	Time 0.126 (0.131)	Data 4.46e-05 (5.01e-05)	Tok/s 29314 (28547)	Loss/tok 4.4281 (5.9383)	LR 2.000e-03
0: TRAIN [0][1500/1848]	Time 0.099 (0.131)	Data 4.20e-05 (5.01e-05)	Tok/s 21742 (28543)	Loss/tok 4.2785 (5.9299)	LR 2.000e-03
0: TRAIN [0][1510/1848]	Time 0.097 (0.130)	Data 4.32e-05 (5.01e-05)	Tok/s 21971 (28525)	Loss/tok 4.2055 (5.9224)	LR 2.000e-03
0: TRAIN [0][1520/1848]	Time 0.121 (0.131)	Data 7.01e-05 (5.01e-05)	Tok/s 30332 (28533)	Loss/tok 4.3851 (5.9133)	LR 2.000e-03
0: TRAIN [0][1530/1848]	Time 0.155 (0.131)	Data 4.41e-05 (5.01e-05)	Tok/s 32390 (28542)	Loss/tok 4.6829 (5.9038)	LR 2.000e-03
0: TRAIN [0][1540/1848]	Time 0.193 (0.131)	Data 4.41e-05 (5.01e-05)	Tok/s 33592 (28552)	Loss/tok 4.9664 (5.8942)	LR 2.000e-03
0: TRAIN [0][1550/1848]	Time 0.125 (0.131)	Data 4.41e-05 (5.01e-05)	Tok/s 28943 (28560)	Loss/tok 4.4171 (5.8853)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1560/1848]	Time 0.122 (0.131)	Data 4.29e-05 (5.01e-05)	Tok/s 29505 (28550)	Loss/tok 4.3501 (5.8775)	LR 2.000e-03
0: TRAIN [0][1570/1848]	Time 0.097 (0.131)	Data 4.67e-05 (5.00e-05)	Tok/s 22476 (28538)	Loss/tok 4.0885 (5.8695)	LR 2.000e-03
0: TRAIN [0][1580/1848]	Time 0.152 (0.131)	Data 4.27e-05 (5.00e-05)	Tok/s 32805 (28522)	Loss/tok 4.6747 (5.8620)	LR 2.000e-03
0: TRAIN [0][1590/1848]	Time 0.123 (0.130)	Data 4.34e-05 (5.00e-05)	Tok/s 29775 (28521)	Loss/tok 4.5363 (5.8537)	LR 2.000e-03
0: TRAIN [0][1600/1848]	Time 0.122 (0.130)	Data 4.34e-05 (5.00e-05)	Tok/s 30097 (28506)	Loss/tok 4.4592 (5.8467)	LR 2.000e-03
0: TRAIN [0][1610/1848]	Time 0.188 (0.130)	Data 4.43e-05 (5.00e-05)	Tok/s 35117 (28515)	Loss/tok 4.8417 (5.8379)	LR 2.000e-03
0: TRAIN [0][1620/1848]	Time 0.155 (0.130)	Data 4.58e-05 (5.00e-05)	Tok/s 32076 (28518)	Loss/tok 4.4243 (5.8294)	LR 2.000e-03
0: TRAIN [0][1630/1848]	Time 0.154 (0.131)	Data 4.29e-05 (5.00e-05)	Tok/s 33201 (28524)	Loss/tok 4.3869 (5.8207)	LR 2.000e-03
0: TRAIN [0][1640/1848]	Time 0.126 (0.131)	Data 4.32e-05 (5.00e-05)	Tok/s 28858 (28521)	Loss/tok 4.4615 (5.8131)	LR 2.000e-03
0: TRAIN [0][1650/1848]	Time 0.155 (0.131)	Data 4.41e-05 (5.00e-05)	Tok/s 32105 (28516)	Loss/tok 4.6348 (5.8056)	LR 2.000e-03
0: TRAIN [0][1660/1848]	Time 0.123 (0.131)	Data 4.27e-05 (5.00e-05)	Tok/s 29341 (28526)	Loss/tok 4.4744 (5.7972)	LR 2.000e-03
0: TRAIN [0][1670/1848]	Time 0.094 (0.131)	Data 4.29e-05 (5.00e-05)	Tok/s 22960 (28525)	Loss/tok 4.1775 (5.7894)	LR 2.000e-03
0: TRAIN [0][1680/1848]	Time 0.123 (0.131)	Data 5.13e-05 (5.00e-05)	Tok/s 29254 (28521)	Loss/tok 4.3361 (5.7818)	LR 2.000e-03
0: TRAIN [0][1690/1848]	Time 0.125 (0.131)	Data 4.55e-05 (5.00e-05)	Tok/s 28938 (28520)	Loss/tok 4.2735 (5.7747)	LR 2.000e-03
0: TRAIN [0][1700/1848]	Time 0.152 (0.131)	Data 4.24e-05 (5.00e-05)	Tok/s 33306 (28524)	Loss/tok 4.7168 (5.7670)	LR 2.000e-03
0: TRAIN [0][1710/1848]	Time 0.121 (0.130)	Data 6.82e-05 (5.00e-05)	Tok/s 30517 (28518)	Loss/tok 4.3667 (5.7598)	LR 2.000e-03
0: TRAIN [0][1720/1848]	Time 0.125 (0.130)	Data 4.96e-05 (5.00e-05)	Tok/s 28566 (28517)	Loss/tok 4.3331 (5.7521)	LR 2.000e-03
0: TRAIN [0][1730/1848]	Time 0.152 (0.130)	Data 4.32e-05 (5.00e-05)	Tok/s 32731 (28515)	Loss/tok 4.7408 (5.7447)	LR 2.000e-03
0: TRAIN [0][1740/1848]	Time 0.094 (0.130)	Data 4.34e-05 (5.00e-05)	Tok/s 22933 (28510)	Loss/tok 4.0585 (5.7375)	LR 2.000e-03
0: TRAIN [0][1750/1848]	Time 0.094 (0.130)	Data 4.27e-05 (5.00e-05)	Tok/s 23084 (28500)	Loss/tok 4.1585 (5.7309)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1760/1848]	Time 0.123 (0.130)	Data 4.32e-05 (4.99e-05)	Tok/s 29267 (28511)	Loss/tok 4.2644 (5.7232)	LR 2.000e-03
0: TRAIN [0][1770/1848]	Time 0.099 (0.130)	Data 4.34e-05 (4.99e-05)	Tok/s 21859 (28502)	Loss/tok 3.8950 (5.7162)	LR 2.000e-03
0: TRAIN [0][1780/1848]	Time 0.099 (0.130)	Data 4.34e-05 (5.00e-05)	Tok/s 22181 (28504)	Loss/tok 4.2255 (5.7089)	LR 2.000e-03
0: TRAIN [0][1790/1848]	Time 0.153 (0.130)	Data 4.63e-05 (5.00e-05)	Tok/s 32566 (28512)	Loss/tok 4.4067 (5.7012)	LR 2.000e-03
0: TRAIN [0][1800/1848]	Time 0.123 (0.130)	Data 4.36e-05 (5.00e-05)	Tok/s 29897 (28500)	Loss/tok 4.3064 (5.6948)	LR 2.000e-03
0: TRAIN [0][1810/1848]	Time 0.125 (0.130)	Data 4.24e-05 (5.00e-05)	Tok/s 29564 (28505)	Loss/tok 4.3899 (5.6874)	LR 2.000e-03
0: TRAIN [0][1820/1848]	Time 0.155 (0.131)	Data 7.75e-05 (5.00e-05)	Tok/s 32607 (28512)	Loss/tok 4.5868 (5.6799)	LR 2.000e-03
0: TRAIN [0][1830/1848]	Time 0.067 (0.130)	Data 4.27e-05 (5.00e-05)	Tok/s 15781 (28497)	Loss/tok 3.8231 (5.6738)	LR 2.000e-03
0: TRAIN [0][1840/1848]	Time 0.128 (0.131)	Data 4.51e-05 (5.00e-05)	Tok/s 28308 (28510)	Loss/tok 4.3847 (5.6658)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/213]	Time 0.069 (0.000)	Data 9.37e-04 (0.00e+00)	Tok/s 64155 (0)	Loss/tok 6.0403 (6.0403)
0: VALIDATION [0][10/213]	Time 0.036 (0.040)	Data 7.89e-04 (7.85e-04)	Tok/s 76263 (76331)	Loss/tok 5.7067 (5.7784)
0: VALIDATION [0][20/213]	Time 0.030 (0.037)	Data 7.60e-04 (7.78e-04)	Tok/s 77519 (76373)	Loss/tok 5.7704 (5.6952)
0: VALIDATION [0][30/213]	Time 0.028 (0.034)	Data 7.81e-04 (7.76e-04)	Tok/s 75622 (76722)	Loss/tok 5.2775 (5.6564)
0: VALIDATION [0][40/213]	Time 0.027 (0.032)	Data 7.50e-04 (7.71e-04)	Tok/s 73684 (76458)	Loss/tok 5.6642 (5.6288)
0: VALIDATION [0][50/213]	Time 0.024 (0.031)	Data 7.39e-04 (7.67e-04)	Tok/s 76846 (76514)	Loss/tok 5.9632 (5.6075)
0: VALIDATION [0][60/213]	Time 0.022 (0.029)	Data 7.43e-04 (7.63e-04)	Tok/s 75468 (76491)	Loss/tok 5.1005 (5.5721)
0: VALIDATION [0][70/213]	Time 0.020 (0.028)	Data 7.51e-04 (7.65e-04)	Tok/s 78269 (76346)	Loss/tok 5.3605 (5.5443)
0: VALIDATION [0][80/213]	Time 0.020 (0.027)	Data 7.32e-04 (7.62e-04)	Tok/s 73629 (76170)	Loss/tok 5.3381 (5.5229)
0: VALIDATION [0][90/213]	Time 0.019 (0.026)	Data 7.37e-04 (7.60e-04)	Tok/s 70224 (75877)	Loss/tok 5.4546 (5.5063)
0: VALIDATION [0][100/213]	Time 0.019 (0.025)	Data 7.34e-04 (7.57e-04)	Tok/s 67940 (75557)	Loss/tok 4.9718 (5.4839)
0: VALIDATION [0][110/213]	Time 0.016 (0.025)	Data 7.33e-04 (7.55e-04)	Tok/s 73085 (75296)	Loss/tok 5.1617 (5.4710)
0: VALIDATION [0][120/213]	Time 0.015 (0.024)	Data 7.29e-04 (7.53e-04)	Tok/s 72587 (75049)	Loss/tok 5.1065 (5.4550)
0: VALIDATION [0][130/213]	Time 0.015 (0.023)	Data 7.30e-04 (7.52e-04)	Tok/s 69840 (74661)	Loss/tok 5.2141 (5.4415)
0: VALIDATION [0][140/213]	Time 0.014 (0.023)	Data 7.39e-04 (7.51e-04)	Tok/s 68193 (74187)	Loss/tok 4.9109 (5.4276)
0: VALIDATION [0][150/213]	Time 0.013 (0.022)	Data 7.26e-04 (7.50e-04)	Tok/s 69162 (73871)	Loss/tok 5.3556 (5.4174)
0: VALIDATION [0][160/213]	Time 0.012 (0.021)	Data 7.21e-04 (7.48e-04)	Tok/s 66817 (73496)	Loss/tok 5.0569 (5.4049)
0: VALIDATION [0][170/213]	Time 0.012 (0.021)	Data 7.21e-04 (7.47e-04)	Tok/s 60474 (73024)	Loss/tok 4.6844 (5.3924)
0: VALIDATION [0][180/213]	Time 0.010 (0.020)	Data 7.28e-04 (7.46e-04)	Tok/s 64599 (72599)	Loss/tok 5.2516 (5.3853)
0: VALIDATION [0][190/213]	Time 0.010 (0.020)	Data 7.21e-04 (7.45e-04)	Tok/s 59620 (72032)	Loss/tok 5.1331 (5.3734)
0: VALIDATION [0][200/213]	Time 0.008 (0.019)	Data 7.18e-04 (7.44e-04)	Tok/s 58090 (71371)	Loss/tok 4.6555 (5.3591)
0: VALIDATION [0][210/213]	Time 0.007 (0.019)	Data 7.20e-04 (7.43e-04)	Tok/s 47885 (70541)	Loss/tok 4.4670 (5.3473)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/126]	Time 0.3311 (0.3612)	Decoder iters 149.0 (149.0)	Tok/s 7889 (8616)
0: TEST [0][19/126]	Time 0.3096 (0.3238)	Decoder iters 149.0 (139.7)	Tok/s 7097 (8523)
0: TEST [0][29/126]	Time 0.3038 (0.3025)	Decoder iters 149.0 (133.6)	Tok/s 6530 (8466)
0: TEST [0][39/126]	Time 0.1419 (0.2861)	Decoder iters 59.0 (128.5)	Tok/s 11938 (8428)
0: TEST [0][49/126]	Time 0.1359 (0.2734)	Decoder iters 55.0 (124.3)	Tok/s 11075 (8356)
0: TEST [0][59/126]	Time 0.2797 (0.2568)	Decoder iters 149.0 (117.2)	Tok/s 5194 (8484)
0: TEST [0][69/126]	Time 0.1285 (0.2439)	Decoder iters 57.0 (111.9)	Tok/s 9278 (8540)
0: TEST [0][79/126]	Time 0.2751 (0.2337)	Decoder iters 149.0 (107.9)	Tok/s 3856 (8528)
0: TEST [0][89/126]	Time 0.0952 (0.2202)	Decoder iters 42.0 (101.7)	Tok/s 10164 (8711)
0: TEST [0][99/126]	Time 0.0684 (0.2106)	Decoder iters 28.0 (97.6)	Tok/s 11862 (8751)
0: TEST [0][109/126]	Time 0.0774 (0.1999)	Decoder iters 35.0 (92.8)	Tok/s 9095 (8845)
0: TEST [0][119/126]	Time 0.0578 (0.1882)	Decoder iters 26.0 (87.2)	Tok/s 8865 (8966)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 5.6604	Validation Loss: 5.3458	Test BLEU: 5.25
0: Performance: Epoch: 0	Training: 28519 Tok/s	Validation: 70216 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3588440356
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/1848]	Time 0.169 (0.000)	Data 1.05e-01 (0.00e+00)	Tok/s 12354 (0)	Loss/tok 3.6146 (3.6146)	LR 2.000e-03
0: TRAIN [1][10/1848]	Time 0.095 (0.113)	Data 4.48e-05 (5.46e-05)	Tok/s 22062 (26494)	Loss/tok 3.6874 (4.0079)	LR 2.000e-03
0: TRAIN [1][20/1848]	Time 0.155 (0.129)	Data 4.46e-05 (5.22e-05)	Tok/s 32141 (28542)	Loss/tok 4.1295 (4.1266)	LR 2.000e-03
0: TRAIN [1][30/1848]	Time 0.125 (0.124)	Data 4.32e-05 (5.21e-05)	Tok/s 28854 (27921)	Loss/tok 3.9898 (4.0635)	LR 2.000e-03
0: TRAIN [1][40/1848]	Time 0.155 (0.122)	Data 4.70e-05 (5.38e-05)	Tok/s 32388 (27578)	Loss/tok 4.2002 (4.0402)	LR 2.000e-03
0: TRAIN [1][50/1848]	Time 0.125 (0.126)	Data 7.58e-05 (5.26e-05)	Tok/s 28830 (28031)	Loss/tok 3.9195 (4.0895)	LR 2.000e-03
0: TRAIN [1][60/1848]	Time 0.097 (0.126)	Data 4.51e-05 (5.16e-05)	Tok/s 22236 (28183)	Loss/tok 3.5312 (4.0913)	LR 2.000e-03
0: TRAIN [1][70/1848]	Time 0.190 (0.127)	Data 4.51e-05 (5.17e-05)	Tok/s 34438 (28209)	Loss/tok 4.3611 (4.0957)	LR 2.000e-03
0: TRAIN [1][80/1848]	Time 0.095 (0.125)	Data 4.48e-05 (5.22e-05)	Tok/s 22134 (27727)	Loss/tok 3.8160 (4.0835)	LR 2.000e-03
0: TRAIN [1][90/1848]	Time 0.125 (0.125)	Data 4.41e-05 (5.22e-05)	Tok/s 29037 (27822)	Loss/tok 3.9163 (4.0746)	LR 2.000e-03
0: TRAIN [1][100/1848]	Time 0.121 (0.124)	Data 4.65e-05 (5.20e-05)	Tok/s 29911 (27708)	Loss/tok 4.1433 (4.0752)	LR 2.000e-03
0: TRAIN [1][110/1848]	Time 0.099 (0.126)	Data 4.51e-05 (5.13e-05)	Tok/s 21322 (27911)	Loss/tok 3.6634 (4.0979)	LR 2.000e-03
0: TRAIN [1][120/1848]	Time 0.154 (0.128)	Data 4.53e-05 (5.13e-05)	Tok/s 32919 (28158)	Loss/tok 4.1518 (4.1126)	LR 2.000e-03
0: TRAIN [1][130/1848]	Time 0.126 (0.129)	Data 4.65e-05 (5.13e-05)	Tok/s 29017 (28307)	Loss/tok 4.0484 (4.1160)	LR 2.000e-03
0: TRAIN [1][140/1848]	Time 0.096 (0.127)	Data 4.43e-05 (5.15e-05)	Tok/s 23125 (27994)	Loss/tok 3.8910 (4.1049)	LR 2.000e-03
0: TRAIN [1][150/1848]	Time 0.098 (0.127)	Data 4.65e-05 (5.19e-05)	Tok/s 22396 (27929)	Loss/tok 3.6854 (4.1076)	LR 2.000e-03
0: TRAIN [1][160/1848]	Time 0.122 (0.126)	Data 7.32e-05 (5.23e-05)	Tok/s 29639 (27846)	Loss/tok 3.7529 (4.1017)	LR 2.000e-03
0: TRAIN [1][170/1848]	Time 0.152 (0.127)	Data 4.36e-05 (5.27e-05)	Tok/s 32607 (27939)	Loss/tok 4.2315 (4.1134)	LR 2.000e-03
0: TRAIN [1][180/1848]	Time 0.154 (0.127)	Data 4.77e-05 (5.29e-05)	Tok/s 32664 (28019)	Loss/tok 4.1519 (4.1132)	LR 2.000e-03
0: TRAIN [1][190/1848]	Time 0.070 (0.127)	Data 4.46e-05 (5.26e-05)	Tok/s 15593 (28022)	Loss/tok 3.3069 (4.1136)	LR 2.000e-03
0: TRAIN [1][200/1848]	Time 0.183 (0.127)	Data 4.34e-05 (5.25e-05)	Tok/s 35504 (27966)	Loss/tok 4.4289 (4.1107)	LR 2.000e-03
0: TRAIN [1][210/1848]	Time 0.070 (0.128)	Data 4.77e-05 (5.24e-05)	Tok/s 15243 (28031)	Loss/tok 3.3693 (4.1147)	LR 2.000e-03
0: TRAIN [1][220/1848]	Time 0.151 (0.127)	Data 4.34e-05 (5.23e-05)	Tok/s 33067 (28002)	Loss/tok 4.1553 (4.1100)	LR 2.000e-03
0: TRAIN [1][230/1848]	Time 0.151 (0.127)	Data 4.55e-05 (5.24e-05)	Tok/s 33262 (27920)	Loss/tok 4.0689 (4.1065)	LR 2.000e-03
0: TRAIN [1][240/1848]	Time 0.125 (0.127)	Data 7.99e-05 (5.23e-05)	Tok/s 28730 (28035)	Loss/tok 3.8670 (4.1071)	LR 2.000e-03
0: TRAIN [1][250/1848]	Time 0.093 (0.126)	Data 4.41e-05 (5.24e-05)	Tok/s 23637 (27881)	Loss/tok 3.6533 (4.0980)	LR 2.000e-03
0: TRAIN [1][260/1848]	Time 0.097 (0.126)	Data 4.74e-05 (5.25e-05)	Tok/s 22199 (27878)	Loss/tok 3.5763 (4.0972)	LR 2.000e-03
0: TRAIN [1][270/1848]	Time 0.154 (0.128)	Data 4.32e-05 (5.23e-05)	Tok/s 33466 (28038)	Loss/tok 4.2023 (4.1053)	LR 2.000e-03
0: TRAIN [1][280/1848]	Time 0.189 (0.128)	Data 4.43e-05 (5.25e-05)	Tok/s 34583 (28048)	Loss/tok 4.2874 (4.1026)	LR 2.000e-03
0: TRAIN [1][290/1848]	Time 0.097 (0.128)	Data 5.58e-05 (5.25e-05)	Tok/s 22478 (28057)	Loss/tok 3.7648 (4.1034)	LR 2.000e-03
0: TRAIN [1][300/1848]	Time 0.094 (0.128)	Data 4.36e-05 (5.22e-05)	Tok/s 23342 (28035)	Loss/tok 3.7734 (4.1006)	LR 2.000e-03
0: TRAIN [1][310/1848]	Time 0.098 (0.128)	Data 4.48e-05 (5.23e-05)	Tok/s 22555 (28000)	Loss/tok 3.6993 (4.0998)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][320/1848]	Time 0.156 (0.128)	Data 5.77e-05 (5.27e-05)	Tok/s 32130 (28091)	Loss/tok 4.3890 (4.1043)	LR 2.000e-03
0: TRAIN [1][330/1848]	Time 0.123 (0.128)	Data 4.34e-05 (5.25e-05)	Tok/s 29442 (28134)	Loss/tok 3.8267 (4.1054)	LR 2.000e-03
0: TRAIN [1][340/1848]	Time 0.097 (0.128)	Data 4.82e-05 (5.24e-05)	Tok/s 22173 (28082)	Loss/tok 3.5236 (4.1025)	LR 2.000e-03
0: TRAIN [1][350/1848]	Time 0.126 (0.128)	Data 7.99e-05 (5.23e-05)	Tok/s 28407 (28109)	Loss/tok 3.9220 (4.1013)	LR 2.000e-03
0: TRAIN [1][360/1848]	Time 0.126 (0.128)	Data 4.34e-05 (5.22e-05)	Tok/s 29303 (28105)	Loss/tok 3.9688 (4.1004)	LR 2.000e-03
0: TRAIN [1][370/1848]	Time 0.152 (0.128)	Data 4.70e-05 (5.22e-05)	Tok/s 32713 (28068)	Loss/tok 4.1032 (4.0964)	LR 2.000e-03
0: TRAIN [1][380/1848]	Time 0.152 (0.128)	Data 4.34e-05 (5.20e-05)	Tok/s 33239 (28146)	Loss/tok 4.1003 (4.0978)	LR 2.000e-03
0: TRAIN [1][390/1848]	Time 0.123 (0.128)	Data 6.84e-05 (5.20e-05)	Tok/s 29473 (28092)	Loss/tok 4.0076 (4.0972)	LR 2.000e-03
0: TRAIN [1][400/1848]	Time 0.097 (0.128)	Data 5.44e-05 (5.20e-05)	Tok/s 22528 (28131)	Loss/tok 3.7781 (4.0980)	LR 2.000e-03
0: TRAIN [1][410/1848]	Time 0.123 (0.128)	Data 4.34e-05 (5.19e-05)	Tok/s 29094 (28127)	Loss/tok 3.9123 (4.0981)	LR 2.000e-03
0: TRAIN [1][420/1848]	Time 0.124 (0.128)	Data 4.46e-05 (5.18e-05)	Tok/s 29329 (28136)	Loss/tok 3.9973 (4.0967)	LR 2.000e-03
0: TRAIN [1][430/1848]	Time 0.122 (0.128)	Data 5.41e-05 (5.17e-05)	Tok/s 29648 (28108)	Loss/tok 3.9224 (4.0981)	LR 2.000e-03
0: TRAIN [1][440/1848]	Time 0.125 (0.128)	Data 4.46e-05 (5.16e-05)	Tok/s 28810 (28086)	Loss/tok 3.9716 (4.0944)	LR 2.000e-03
0: TRAIN [1][450/1848]	Time 0.097 (0.128)	Data 4.65e-05 (5.17e-05)	Tok/s 23312 (28031)	Loss/tok 3.6936 (4.0942)	LR 2.000e-03
0: TRAIN [1][460/1848]	Time 0.192 (0.128)	Data 4.51e-05 (5.15e-05)	Tok/s 34521 (28098)	Loss/tok 4.3213 (4.0945)	LR 2.000e-03
0: TRAIN [1][470/1848]	Time 0.190 (0.128)	Data 4.46e-05 (5.15e-05)	Tok/s 34412 (28156)	Loss/tok 4.2765 (4.0939)	LR 2.000e-03
0: TRAIN [1][480/1848]	Time 0.119 (0.128)	Data 4.58e-05 (5.15e-05)	Tok/s 29778 (28140)	Loss/tok 3.9535 (4.0900)	LR 2.000e-03
0: TRAIN [1][490/1848]	Time 0.155 (0.129)	Data 4.29e-05 (5.15e-05)	Tok/s 32717 (28195)	Loss/tok 4.0398 (4.0904)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][500/1848]	Time 0.152 (0.128)	Data 4.43e-05 (5.16e-05)	Tok/s 33040 (28190)	Loss/tok 4.0971 (4.0881)	LR 2.000e-03
0: TRAIN [1][510/1848]	Time 0.126 (0.128)	Data 5.72e-05 (5.15e-05)	Tok/s 28118 (28162)	Loss/tok 3.9680 (4.0858)	LR 2.000e-03
0: TRAIN [1][520/1848]	Time 0.189 (0.128)	Data 4.36e-05 (5.16e-05)	Tok/s 34421 (28180)	Loss/tok 4.2841 (4.0849)	LR 2.000e-03
0: TRAIN [1][530/1848]	Time 0.066 (0.128)	Data 4.36e-05 (5.15e-05)	Tok/s 16297 (28166)	Loss/tok 3.3661 (4.0831)	LR 2.000e-03
0: TRAIN [1][540/1848]	Time 0.154 (0.128)	Data 5.91e-05 (5.15e-05)	Tok/s 32212 (28186)	Loss/tok 4.1957 (4.0846)	LR 2.000e-03
0: TRAIN [1][550/1848]	Time 0.125 (0.129)	Data 4.32e-05 (5.15e-05)	Tok/s 28256 (28224)	Loss/tok 3.7857 (4.0856)	LR 2.000e-03
0: TRAIN [1][560/1848]	Time 0.122 (0.129)	Data 4.63e-05 (5.15e-05)	Tok/s 29206 (28232)	Loss/tok 4.0706 (4.0845)	LR 2.000e-03
0: TRAIN [1][570/1848]	Time 0.096 (0.129)	Data 5.34e-05 (5.14e-05)	Tok/s 22349 (28211)	Loss/tok 3.8828 (4.0836)	LR 2.000e-03
0: TRAIN [1][580/1848]	Time 0.122 (0.129)	Data 4.39e-05 (5.15e-05)	Tok/s 29199 (28214)	Loss/tok 3.9613 (4.0832)	LR 2.000e-03
0: TRAIN [1][590/1848]	Time 0.097 (0.129)	Data 4.48e-05 (5.15e-05)	Tok/s 23313 (28179)	Loss/tok 3.6263 (4.0814)	LR 2.000e-03
0: TRAIN [1][600/1848]	Time 0.125 (0.128)	Data 4.43e-05 (5.14e-05)	Tok/s 28953 (28157)	Loss/tok 4.0806 (4.0789)	LR 2.000e-03
0: TRAIN [1][610/1848]	Time 0.094 (0.128)	Data 4.46e-05 (5.14e-05)	Tok/s 23579 (28147)	Loss/tok 3.8663 (4.0788)	LR 2.000e-03
0: TRAIN [1][620/1848]	Time 0.070 (0.128)	Data 4.74e-05 (5.13e-05)	Tok/s 15226 (28159)	Loss/tok 3.4844 (4.0800)	LR 1.000e-03
0: TRAIN [1][630/1848]	Time 0.126 (0.128)	Data 4.36e-05 (5.13e-05)	Tok/s 28646 (28087)	Loss/tok 4.0569 (4.0775)	LR 1.000e-03
0: TRAIN [1][640/1848]	Time 0.125 (0.128)	Data 4.63e-05 (5.13e-05)	Tok/s 29436 (28101)	Loss/tok 3.8817 (4.0773)	LR 1.000e-03
0: TRAIN [1][650/1848]	Time 0.123 (0.128)	Data 4.46e-05 (5.12e-05)	Tok/s 29601 (28114)	Loss/tok 3.8068 (4.0757)	LR 1.000e-03
0: TRAIN [1][660/1848]	Time 0.126 (0.128)	Data 4.29e-05 (5.13e-05)	Tok/s 28829 (28135)	Loss/tok 3.8980 (4.0753)	LR 1.000e-03
0: TRAIN [1][670/1848]	Time 0.155 (0.128)	Data 4.58e-05 (5.13e-05)	Tok/s 32856 (28151)	Loss/tok 4.0581 (4.0741)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][680/1848]	Time 0.099 (0.128)	Data 4.36e-05 (5.12e-05)	Tok/s 21316 (28162)	Loss/tok 3.6842 (4.0725)	LR 1.000e-03
0: TRAIN [1][690/1848]	Time 0.155 (0.128)	Data 5.20e-05 (5.14e-05)	Tok/s 32699 (28159)	Loss/tok 4.0229 (4.0699)	LR 1.000e-03
0: TRAIN [1][700/1848]	Time 0.099 (0.128)	Data 5.05e-05 (5.14e-05)	Tok/s 22197 (28132)	Loss/tok 3.5494 (4.0671)	LR 1.000e-03
0: TRAIN [1][710/1848]	Time 0.156 (0.128)	Data 4.39e-05 (5.15e-05)	Tok/s 32431 (28151)	Loss/tok 4.1938 (4.0658)	LR 1.000e-03
0: TRAIN [1][720/1848]	Time 0.126 (0.128)	Data 4.51e-05 (5.15e-05)	Tok/s 28384 (28173)	Loss/tok 3.8825 (4.0642)	LR 1.000e-03
0: TRAIN [1][730/1848]	Time 0.093 (0.128)	Data 1.04e-04 (5.15e-05)	Tok/s 23630 (28167)	Loss/tok 3.5138 (4.0617)	LR 1.000e-03
0: TRAIN [1][740/1848]	Time 0.123 (0.128)	Data 4.34e-05 (5.15e-05)	Tok/s 29468 (28168)	Loss/tok 3.7687 (4.0589)	LR 1.000e-03
0: TRAIN [1][750/1848]	Time 0.095 (0.128)	Data 4.63e-05 (5.15e-05)	Tok/s 23370 (28149)	Loss/tok 3.6229 (4.0562)	LR 1.000e-03
0: TRAIN [1][760/1848]	Time 0.094 (0.128)	Data 4.34e-05 (5.14e-05)	Tok/s 22631 (28105)	Loss/tok 3.6023 (4.0535)	LR 1.000e-03
0: TRAIN [1][770/1848]	Time 0.194 (0.128)	Data 4.53e-05 (5.14e-05)	Tok/s 33695 (28128)	Loss/tok 4.3383 (4.0524)	LR 1.000e-03
0: TRAIN [1][780/1848]	Time 0.152 (0.128)	Data 5.84e-05 (5.14e-05)	Tok/s 33366 (28126)	Loss/tok 3.9907 (4.0495)	LR 1.000e-03
0: TRAIN [1][790/1848]	Time 0.094 (0.128)	Data 4.41e-05 (5.13e-05)	Tok/s 23026 (28134)	Loss/tok 3.5010 (4.0504)	LR 1.000e-03
0: TRAIN [1][800/1848]	Time 0.155 (0.128)	Data 4.53e-05 (5.14e-05)	Tok/s 33199 (28142)	Loss/tok 4.0261 (4.0485)	LR 1.000e-03
0: TRAIN [1][810/1848]	Time 0.155 (0.128)	Data 5.72e-05 (5.14e-05)	Tok/s 32543 (28150)	Loss/tok 4.0693 (4.0483)	LR 1.000e-03
0: TRAIN [1][820/1848]	Time 0.094 (0.128)	Data 4.41e-05 (5.13e-05)	Tok/s 22688 (28147)	Loss/tok 3.5170 (4.0458)	LR 1.000e-03
0: TRAIN [1][830/1848]	Time 0.123 (0.128)	Data 1.03e-04 (5.14e-05)	Tok/s 29740 (28124)	Loss/tok 3.7234 (4.0432)	LR 1.000e-03
0: TRAIN [1][840/1848]	Time 0.097 (0.128)	Data 4.67e-05 (5.13e-05)	Tok/s 22696 (28124)	Loss/tok 3.4690 (4.0424)	LR 1.000e-03
0: TRAIN [1][850/1848]	Time 0.122 (0.128)	Data 4.43e-05 (5.14e-05)	Tok/s 29373 (28090)	Loss/tok 3.6710 (4.0398)	LR 1.000e-03
0: TRAIN [1][860/1848]	Time 0.155 (0.128)	Data 5.05e-05 (5.14e-05)	Tok/s 32947 (28119)	Loss/tok 4.0245 (4.0381)	LR 1.000e-03
0: TRAIN [1][870/1848]	Time 0.125 (0.128)	Data 4.79e-05 (5.14e-05)	Tok/s 28823 (28154)	Loss/tok 3.8210 (4.0375)	LR 1.000e-03
0: TRAIN [1][880/1848]	Time 0.126 (0.128)	Data 4.41e-05 (5.14e-05)	Tok/s 29372 (28161)	Loss/tok 3.6463 (4.0348)	LR 1.000e-03
0: TRAIN [1][890/1848]	Time 0.123 (0.128)	Data 7.49e-05 (5.14e-05)	Tok/s 29319 (28182)	Loss/tok 3.8266 (4.0331)	LR 1.000e-03
0: TRAIN [1][900/1848]	Time 0.154 (0.128)	Data 4.51e-05 (5.15e-05)	Tok/s 33012 (28173)	Loss/tok 4.1098 (4.0306)	LR 1.000e-03
0: TRAIN [1][910/1848]	Time 0.123 (0.128)	Data 4.39e-05 (5.15e-05)	Tok/s 29312 (28188)	Loss/tok 3.6999 (4.0286)	LR 1.000e-03
0: TRAIN [1][920/1848]	Time 0.097 (0.128)	Data 5.56e-05 (5.15e-05)	Tok/s 22351 (28200)	Loss/tok 3.6103 (4.0269)	LR 5.000e-04
0: TRAIN [1][930/1848]	Time 0.121 (0.129)	Data 7.27e-05 (5.14e-05)	Tok/s 30716 (28206)	Loss/tok 3.8032 (4.0263)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][940/1848]	Time 0.191 (0.129)	Data 4.39e-05 (5.15e-05)	Tok/s 34088 (28253)	Loss/tok 4.2247 (4.0255)	LR 5.000e-04
0: TRAIN [1][950/1848]	Time 0.097 (0.129)	Data 4.72e-05 (5.15e-05)	Tok/s 23079 (28275)	Loss/tok 3.3481 (4.0239)	LR 5.000e-04
0: TRAIN [1][960/1848]	Time 0.152 (0.129)	Data 4.72e-05 (5.14e-05)	Tok/s 32936 (28258)	Loss/tok 3.7034 (4.0214)	LR 5.000e-04
0: TRAIN [1][970/1848]	Time 0.155 (0.129)	Data 4.51e-05 (5.14e-05)	Tok/s 32840 (28255)	Loss/tok 3.9358 (4.0193)	LR 5.000e-04
0: TRAIN [1][980/1848]	Time 0.188 (0.129)	Data 5.01e-05 (5.15e-05)	Tok/s 34458 (28272)	Loss/tok 4.0321 (4.0188)	LR 5.000e-04
0: TRAIN [1][990/1848]	Time 0.155 (0.129)	Data 4.67e-05 (5.14e-05)	Tok/s 32500 (28272)	Loss/tok 3.8732 (4.0162)	LR 5.000e-04
0: TRAIN [1][1000/1848]	Time 0.125 (0.129)	Data 4.34e-05 (5.14e-05)	Tok/s 28619 (28270)	Loss/tok 3.7352 (4.0139)	LR 5.000e-04
0: TRAIN [1][1010/1848]	Time 0.097 (0.129)	Data 5.53e-05 (5.14e-05)	Tok/s 21809 (28254)	Loss/tok 3.4099 (4.0128)	LR 5.000e-04
0: TRAIN [1][1020/1848]	Time 0.121 (0.129)	Data 7.03e-05 (5.15e-05)	Tok/s 28959 (28276)	Loss/tok 3.6992 (4.0112)	LR 5.000e-04
0: TRAIN [1][1030/1848]	Time 0.155 (0.129)	Data 4.53e-05 (5.15e-05)	Tok/s 32848 (28300)	Loss/tok 3.9922 (4.0099)	LR 5.000e-04
0: TRAIN [1][1040/1848]	Time 0.153 (0.129)	Data 5.13e-05 (5.15e-05)	Tok/s 32916 (28304)	Loss/tok 3.9050 (4.0085)	LR 5.000e-04
0: TRAIN [1][1050/1848]	Time 0.188 (0.130)	Data 6.96e-05 (5.14e-05)	Tok/s 34848 (28328)	Loss/tok 4.0527 (4.0079)	LR 5.000e-04
0: TRAIN [1][1060/1848]	Time 0.123 (0.130)	Data 4.48e-05 (5.14e-05)	Tok/s 29095 (28334)	Loss/tok 3.5799 (4.0060)	LR 5.000e-04
0: TRAIN [1][1070/1848]	Time 0.155 (0.130)	Data 4.82e-05 (5.14e-05)	Tok/s 32180 (28355)	Loss/tok 4.0963 (4.0049)	LR 5.000e-04
0: TRAIN [1][1080/1848]	Time 0.125 (0.130)	Data 4.41e-05 (5.13e-05)	Tok/s 28269 (28366)	Loss/tok 3.8166 (4.0033)	LR 5.000e-04
0: TRAIN [1][1090/1848]	Time 0.155 (0.130)	Data 4.39e-05 (5.13e-05)	Tok/s 32891 (28370)	Loss/tok 3.8595 (4.0014)	LR 5.000e-04
0: TRAIN [1][1100/1848]	Time 0.123 (0.130)	Data 4.86e-05 (5.14e-05)	Tok/s 29112 (28353)	Loss/tok 3.5962 (3.9986)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1110/1848]	Time 0.099 (0.130)	Data 4.51e-05 (5.13e-05)	Tok/s 21247 (28378)	Loss/tok 3.4790 (3.9968)	LR 5.000e-04
0: TRAIN [1][1120/1848]	Time 0.097 (0.130)	Data 4.51e-05 (5.14e-05)	Tok/s 22104 (28355)	Loss/tok 3.4324 (3.9945)	LR 5.000e-04
0: TRAIN [1][1130/1848]	Time 0.124 (0.129)	Data 4.58e-05 (5.13e-05)	Tok/s 29582 (28349)	Loss/tok 3.5761 (3.9921)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1140/1848]	Time 0.157 (0.129)	Data 4.39e-05 (5.13e-05)	Tok/s 32348 (28363)	Loss/tok 3.9221 (3.9900)	LR 5.000e-04
0: TRAIN [1][1150/1848]	Time 0.099 (0.129)	Data 4.86e-05 (5.14e-05)	Tok/s 22312 (28367)	Loss/tok 3.4165 (3.9887)	LR 5.000e-04
0: TRAIN [1][1160/1848]	Time 0.154 (0.129)	Data 4.48e-05 (5.13e-05)	Tok/s 32720 (28370)	Loss/tok 4.0063 (3.9872)	LR 5.000e-04
0: TRAIN [1][1170/1848]	Time 0.097 (0.130)	Data 4.34e-05 (5.13e-05)	Tok/s 22449 (28383)	Loss/tok 3.5553 (3.9863)	LR 5.000e-04
0: TRAIN [1][1180/1848]	Time 0.126 (0.130)	Data 4.63e-05 (5.13e-05)	Tok/s 28888 (28387)	Loss/tok 3.6672 (3.9845)	LR 5.000e-04
0: TRAIN [1][1190/1848]	Time 0.190 (0.130)	Data 4.46e-05 (5.13e-05)	Tok/s 34606 (28388)	Loss/tok 3.9510 (3.9824)	LR 5.000e-04
0: TRAIN [1][1200/1848]	Time 0.099 (0.129)	Data 4.43e-05 (5.13e-05)	Tok/s 22752 (28367)	Loss/tok 3.4221 (3.9801)	LR 5.000e-04
0: TRAIN [1][1210/1848]	Time 0.096 (0.129)	Data 5.75e-05 (5.13e-05)	Tok/s 22561 (28375)	Loss/tok 3.3344 (3.9778)	LR 5.000e-04
0: TRAIN [1][1220/1848]	Time 0.096 (0.129)	Data 4.43e-05 (5.13e-05)	Tok/s 21982 (28367)	Loss/tok 3.5852 (3.9760)	LR 5.000e-04
0: TRAIN [1][1230/1848]	Time 0.097 (0.129)	Data 4.55e-05 (5.13e-05)	Tok/s 22957 (28365)	Loss/tok 3.2727 (3.9740)	LR 2.500e-04
0: TRAIN [1][1240/1848]	Time 0.127 (0.129)	Data 4.39e-05 (5.13e-05)	Tok/s 28560 (28374)	Loss/tok 3.7046 (3.9729)	LR 2.500e-04
0: TRAIN [1][1250/1848]	Time 0.097 (0.130)	Data 4.74e-05 (5.13e-05)	Tok/s 22624 (28388)	Loss/tok 3.3724 (3.9721)	LR 2.500e-04
0: TRAIN [1][1260/1848]	Time 0.099 (0.130)	Data 4.46e-05 (5.12e-05)	Tok/s 22086 (28397)	Loss/tok 3.4198 (3.9715)	LR 2.500e-04
0: TRAIN [1][1270/1848]	Time 0.189 (0.130)	Data 8.20e-05 (5.12e-05)	Tok/s 34775 (28397)	Loss/tok 3.9959 (3.9702)	LR 2.500e-04
0: TRAIN [1][1280/1848]	Time 0.123 (0.130)	Data 4.72e-05 (5.12e-05)	Tok/s 28895 (28393)	Loss/tok 3.6665 (3.9683)	LR 2.500e-04
0: TRAIN [1][1290/1848]	Time 0.070 (0.130)	Data 4.43e-05 (5.12e-05)	Tok/s 15063 (28399)	Loss/tok 3.2819 (3.9668)	LR 2.500e-04
0: TRAIN [1][1300/1848]	Time 0.124 (0.130)	Data 6.06e-05 (5.12e-05)	Tok/s 28850 (28416)	Loss/tok 3.5813 (3.9653)	LR 2.500e-04
0: TRAIN [1][1310/1848]	Time 0.156 (0.130)	Data 4.67e-05 (5.12e-05)	Tok/s 32879 (28437)	Loss/tok 3.8464 (3.9635)	LR 2.500e-04
0: TRAIN [1][1320/1848]	Time 0.121 (0.130)	Data 4.51e-05 (5.12e-05)	Tok/s 28523 (28438)	Loss/tok 3.7069 (3.9616)	LR 2.500e-04
0: TRAIN [1][1330/1848]	Time 0.125 (0.130)	Data 8.15e-05 (5.11e-05)	Tok/s 28884 (28419)	Loss/tok 3.5745 (3.9600)	LR 2.500e-04
0: TRAIN [1][1340/1848]	Time 0.150 (0.130)	Data 7.15e-05 (5.12e-05)	Tok/s 33286 (28431)	Loss/tok 3.8649 (3.9588)	LR 2.500e-04
0: TRAIN [1][1350/1848]	Time 0.126 (0.130)	Data 4.65e-05 (5.12e-05)	Tok/s 28500 (28441)	Loss/tok 3.6860 (3.9574)	LR 2.500e-04
0: TRAIN [1][1360/1848]	Time 0.125 (0.130)	Data 5.51e-05 (5.12e-05)	Tok/s 28795 (28463)	Loss/tok 3.8163 (3.9567)	LR 2.500e-04
0: TRAIN [1][1370/1848]	Time 0.125 (0.130)	Data 4.34e-05 (5.11e-05)	Tok/s 29217 (28480)	Loss/tok 3.6305 (3.9552)	LR 2.500e-04
0: TRAIN [1][1380/1848]	Time 0.097 (0.130)	Data 4.51e-05 (5.11e-05)	Tok/s 22032 (28476)	Loss/tok 3.3503 (3.9538)	LR 2.500e-04
0: TRAIN [1][1390/1848]	Time 0.127 (0.130)	Data 8.20e-05 (5.11e-05)	Tok/s 27704 (28467)	Loss/tok 3.6578 (3.9516)	LR 2.500e-04
0: TRAIN [1][1400/1848]	Time 0.153 (0.130)	Data 4.46e-05 (5.11e-05)	Tok/s 32634 (28476)	Loss/tok 3.9043 (3.9498)	LR 2.500e-04
0: TRAIN [1][1410/1848]	Time 0.189 (0.130)	Data 4.36e-05 (5.12e-05)	Tok/s 34262 (28483)	Loss/tok 3.9257 (3.9485)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1420/1848]	Time 0.126 (0.130)	Data 7.80e-05 (5.12e-05)	Tok/s 28471 (28494)	Loss/tok 3.5107 (3.9467)	LR 2.500e-04
0: TRAIN [1][1430/1848]	Time 0.126 (0.130)	Data 4.74e-05 (5.12e-05)	Tok/s 28981 (28495)	Loss/tok 3.5446 (3.9447)	LR 2.500e-04
0: TRAIN [1][1440/1848]	Time 0.192 (0.130)	Data 4.55e-05 (5.12e-05)	Tok/s 34455 (28503)	Loss/tok 4.1342 (3.9434)	LR 2.500e-04
0: TRAIN [1][1450/1848]	Time 0.123 (0.130)	Data 4.27e-05 (5.11e-05)	Tok/s 29713 (28469)	Loss/tok 3.7458 (3.9416)	LR 2.500e-04
0: TRAIN [1][1460/1848]	Time 0.120 (0.130)	Data 7.37e-05 (5.11e-05)	Tok/s 30041 (28482)	Loss/tok 3.5747 (3.9411)	LR 2.500e-04
0: TRAIN [1][1470/1848]	Time 0.099 (0.130)	Data 5.13e-05 (5.12e-05)	Tok/s 21458 (28486)	Loss/tok 3.3303 (3.9399)	LR 2.500e-04
0: TRAIN [1][1480/1848]	Time 0.125 (0.130)	Data 4.32e-05 (5.12e-05)	Tok/s 29059 (28492)	Loss/tok 3.6116 (3.9385)	LR 2.500e-04
0: TRAIN [1][1490/1848]	Time 0.155 (0.130)	Data 4.41e-05 (5.11e-05)	Tok/s 33253 (28491)	Loss/tok 3.6988 (3.9370)	LR 2.500e-04
0: TRAIN [1][1500/1848]	Time 0.092 (0.130)	Data 1.04e-04 (5.12e-05)	Tok/s 24164 (28477)	Loss/tok 3.4149 (3.9357)	LR 2.500e-04
0: TRAIN [1][1510/1848]	Time 0.189 (0.130)	Data 4.48e-05 (5.11e-05)	Tok/s 34419 (28480)	Loss/tok 4.0691 (3.9346)	LR 2.500e-04
0: TRAIN [1][1520/1848]	Time 0.097 (0.130)	Data 4.51e-05 (5.11e-05)	Tok/s 22751 (28472)	Loss/tok 3.3461 (3.9327)	LR 2.500e-04
0: TRAIN [1][1530/1848]	Time 0.154 (0.130)	Data 4.48e-05 (5.11e-05)	Tok/s 32499 (28467)	Loss/tok 3.8085 (3.9312)	LR 2.500e-04
0: TRAIN [1][1540/1848]	Time 0.122 (0.130)	Data 4.36e-05 (5.11e-05)	Tok/s 29036 (28474)	Loss/tok 3.7238 (3.9300)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1550/1848]	Time 0.095 (0.130)	Data 4.72e-05 (5.11e-05)	Tok/s 22279 (28475)	Loss/tok 3.5918 (3.9286)	LR 1.250e-04
0: TRAIN [1][1560/1848]	Time 0.097 (0.130)	Data 4.84e-05 (5.11e-05)	Tok/s 22630 (28463)	Loss/tok 3.4425 (3.9274)	LR 1.250e-04
0: TRAIN [1][1570/1848]	Time 0.183 (0.130)	Data 7.01e-05 (5.11e-05)	Tok/s 35574 (28478)	Loss/tok 4.0006 (3.9268)	LR 1.250e-04
0: TRAIN [1][1580/1848]	Time 0.124 (0.130)	Data 4.79e-05 (5.11e-05)	Tok/s 29482 (28485)	Loss/tok 3.7799 (3.9255)	LR 1.250e-04
0: TRAIN [1][1590/1848]	Time 0.070 (0.130)	Data 4.36e-05 (5.11e-05)	Tok/s 15924 (28463)	Loss/tok 3.0674 (3.9241)	LR 1.250e-04
0: TRAIN [1][1600/1848]	Time 0.193 (0.130)	Data 4.91e-05 (5.11e-05)	Tok/s 34242 (28475)	Loss/tok 3.9720 (3.9232)	LR 1.250e-04
0: TRAIN [1][1610/1848]	Time 0.099 (0.130)	Data 5.39e-05 (5.11e-05)	Tok/s 21627 (28466)	Loss/tok 3.4720 (3.9215)	LR 1.250e-04
0: TRAIN [1][1620/1848]	Time 0.126 (0.130)	Data 4.55e-05 (5.11e-05)	Tok/s 27464 (28491)	Loss/tok 3.6130 (3.9213)	LR 1.250e-04
0: TRAIN [1][1630/1848]	Time 0.186 (0.130)	Data 4.36e-05 (5.11e-05)	Tok/s 35332 (28493)	Loss/tok 3.9438 (3.9202)	LR 1.250e-04
0: TRAIN [1][1640/1848]	Time 0.122 (0.130)	Data 4.79e-05 (5.11e-05)	Tok/s 28850 (28486)	Loss/tok 3.5873 (3.9188)	LR 1.250e-04
0: TRAIN [1][1650/1848]	Time 0.097 (0.130)	Data 4.36e-05 (5.10e-05)	Tok/s 22579 (28484)	Loss/tok 3.4286 (3.9173)	LR 1.250e-04
0: TRAIN [1][1660/1848]	Time 0.152 (0.130)	Data 4.46e-05 (5.10e-05)	Tok/s 32933 (28475)	Loss/tok 3.7876 (3.9162)	LR 1.250e-04
0: TRAIN [1][1670/1848]	Time 0.097 (0.130)	Data 8.03e-05 (5.10e-05)	Tok/s 22183 (28467)	Loss/tok 3.3770 (3.9146)	LR 1.250e-04
0: TRAIN [1][1680/1848]	Time 0.125 (0.130)	Data 4.63e-05 (5.10e-05)	Tok/s 28794 (28473)	Loss/tok 3.6354 (3.9143)	LR 1.250e-04
0: TRAIN [1][1690/1848]	Time 0.155 (0.130)	Data 4.58e-05 (5.10e-05)	Tok/s 32959 (28468)	Loss/tok 3.7845 (3.9131)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1700/1848]	Time 0.125 (0.130)	Data 5.56e-05 (5.10e-05)	Tok/s 39432 (28495)	Loss/tok 3.9776 (3.9125)	LR 1.250e-04
0: TRAIN [1][1710/1848]	Time 0.126 (0.130)	Data 4.51e-05 (5.10e-05)	Tok/s 28758 (28493)	Loss/tok 3.5553 (3.9112)	LR 1.250e-04
0: TRAIN [1][1720/1848]	Time 0.099 (0.131)	Data 4.41e-05 (5.10e-05)	Tok/s 21678 (28506)	Loss/tok 3.4859 (3.9111)	LR 1.250e-04
0: TRAIN [1][1730/1848]	Time 0.094 (0.131)	Data 5.13e-05 (5.10e-05)	Tok/s 22970 (28509)	Loss/tok 3.3077 (3.9098)	LR 1.250e-04
0: TRAIN [1][1740/1848]	Time 0.094 (0.131)	Data 6.89e-05 (5.10e-05)	Tok/s 22924 (28515)	Loss/tok 3.2665 (3.9087)	LR 1.250e-04
0: TRAIN [1][1750/1848]	Time 0.189 (0.131)	Data 7.15e-05 (5.10e-05)	Tok/s 34313 (28521)	Loss/tok 3.9642 (3.9077)	LR 1.250e-04
0: TRAIN [1][1760/1848]	Time 0.070 (0.131)	Data 4.77e-05 (5.10e-05)	Tok/s 14918 (28521)	Loss/tok 2.9799 (3.9068)	LR 1.250e-04
0: TRAIN [1][1770/1848]	Time 0.097 (0.131)	Data 4.39e-05 (5.10e-05)	Tok/s 22240 (28519)	Loss/tok 3.3200 (3.9054)	LR 1.250e-04
0: TRAIN [1][1780/1848]	Time 0.190 (0.131)	Data 4.82e-05 (5.10e-05)	Tok/s 34469 (28527)	Loss/tok 3.9408 (3.9043)	LR 1.250e-04
0: TRAIN [1][1790/1848]	Time 0.097 (0.131)	Data 5.34e-05 (5.10e-05)	Tok/s 22316 (28530)	Loss/tok 3.2958 (3.9038)	LR 1.250e-04
0: TRAIN [1][1800/1848]	Time 0.153 (0.131)	Data 4.48e-05 (5.10e-05)	Tok/s 33455 (28527)	Loss/tok 3.8040 (3.9028)	LR 1.250e-04
0: TRAIN [1][1810/1848]	Time 0.099 (0.131)	Data 4.55e-05 (5.10e-05)	Tok/s 22055 (28524)	Loss/tok 3.4529 (3.9020)	LR 1.250e-04
0: TRAIN [1][1820/1848]	Time 0.123 (0.131)	Data 5.56e-05 (5.10e-05)	Tok/s 30130 (28517)	Loss/tok 3.5795 (3.9009)	LR 1.250e-04
0: TRAIN [1][1830/1848]	Time 0.157 (0.131)	Data 4.43e-05 (5.10e-05)	Tok/s 32484 (28525)	Loss/tok 3.9213 (3.9000)	LR 1.250e-04
0: TRAIN [1][1840/1848]	Time 0.191 (0.131)	Data 4.67e-05 (5.10e-05)	Tok/s 34154 (28521)	Loss/tok 3.9746 (3.8987)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/213]	Time 0.069 (0.000)	Data 8.68e-04 (0.00e+00)	Tok/s 64821 (0)	Loss/tok 5.5228 (5.5228)
0: VALIDATION [1][10/213]	Time 0.035 (0.040)	Data 7.74e-04 (8.08e-04)	Tok/s 76743 (76457)	Loss/tok 5.1314 (5.2339)
0: VALIDATION [1][20/213]	Time 0.030 (0.036)	Data 7.54e-04 (7.86e-04)	Tok/s 77570 (76644)	Loss/tok 5.1819 (5.1453)
0: VALIDATION [1][30/213]	Time 0.028 (0.034)	Data 7.62e-04 (7.78e-04)	Tok/s 76571 (77035)	Loss/tok 4.6990 (5.0924)
0: VALIDATION [1][40/213]	Time 0.026 (0.032)	Data 7.47e-04 (7.72e-04)	Tok/s 74494 (76803)	Loss/tok 5.1039 (5.0636)
0: VALIDATION [1][50/213]	Time 0.023 (0.031)	Data 7.32e-04 (7.68e-04)	Tok/s 77295 (76800)	Loss/tok 5.4659 (5.0430)
0: VALIDATION [1][60/213]	Time 0.022 (0.029)	Data 7.36e-04 (7.63e-04)	Tok/s 76048 (76781)	Loss/tok 4.4565 (5.0055)
0: VALIDATION [1][70/213]	Time 0.020 (0.028)	Data 7.30e-04 (7.60e-04)	Tok/s 78455 (76694)	Loss/tok 4.8235 (4.9796)
0: VALIDATION [1][80/213]	Time 0.020 (0.027)	Data 7.37e-04 (7.61e-04)	Tok/s 73768 (76490)	Loss/tok 4.8070 (4.9594)
0: VALIDATION [1][90/213]	Time 0.019 (0.026)	Data 7.46e-04 (7.58e-04)	Tok/s 70471 (76192)	Loss/tok 4.9022 (4.9420)
0: VALIDATION [1][100/213]	Time 0.019 (0.025)	Data 7.47e-04 (7.57e-04)	Tok/s 67946 (75862)	Loss/tok 4.4001 (4.9180)
0: VALIDATION [1][110/213]	Time 0.016 (0.025)	Data 7.30e-04 (7.55e-04)	Tok/s 73424 (75609)	Loss/tok 4.5947 (4.9061)
0: VALIDATION [1][120/213]	Time 0.015 (0.024)	Data 7.45e-04 (7.53e-04)	Tok/s 72938 (75359)	Loss/tok 4.5120 (4.8914)
0: VALIDATION [1][130/213]	Time 0.015 (0.023)	Data 7.48e-04 (7.52e-04)	Tok/s 69737 (74975)	Loss/tok 4.5716 (4.8794)
0: VALIDATION [1][140/213]	Time 0.014 (0.023)	Data 7.20e-04 (7.50e-04)	Tok/s 68825 (74508)	Loss/tok 4.2926 (4.8660)
0: VALIDATION [1][150/213]	Time 0.013 (0.022)	Data 7.22e-04 (7.49e-04)	Tok/s 69286 (74185)	Loss/tok 4.7189 (4.8567)
0: VALIDATION [1][160/213]	Time 0.012 (0.021)	Data 7.25e-04 (7.48e-04)	Tok/s 66789 (73808)	Loss/tok 4.5432 (4.8453)
0: VALIDATION [1][170/213]	Time 0.012 (0.021)	Data 7.24e-04 (7.46e-04)	Tok/s 60808 (73334)	Loss/tok 4.1427 (4.8334)
0: VALIDATION [1][180/213]	Time 0.010 (0.020)	Data 7.26e-04 (7.45e-04)	Tok/s 64610 (72905)	Loss/tok 4.7772 (4.8271)
0: VALIDATION [1][190/213]	Time 0.010 (0.020)	Data 7.23e-04 (7.44e-04)	Tok/s 59317 (72326)	Loss/tok 4.7340 (4.8161)
0: VALIDATION [1][200/213]	Time 0.008 (0.019)	Data 7.18e-04 (7.43e-04)	Tok/s 58143 (71657)	Loss/tok 4.2874 (4.8031)
0: VALIDATION [1][210/213]	Time 0.007 (0.019)	Data 7.21e-04 (7.42e-04)	Tok/s 48210 (70828)	Loss/tok 4.1816 (4.7928)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/126]	Time 0.2012 (0.2844)	Decoder iters 80.0 (115.4)	Tok/s 12055 (10346)
0: TEST [1][19/126]	Time 0.1598 (0.2638)	Decoder iters 63.0 (112.3)	Tok/s 12950 (10181)
0: TEST [1][29/126]	Time 0.2948 (0.2540)	Decoder iters 149.0 (111.7)	Tok/s 6424 (9892)
0: TEST [1][39/126]	Time 0.2494 (0.2428)	Decoder iters 127.0 (108.9)	Tok/s 6828 (9817)
0: TEST [1][49/126]	Time 0.1295 (0.2296)	Decoder iters 55.0 (103.8)	Tok/s 11416 (9795)
0: TEST [1][59/126]	Time 0.1055 (0.2136)	Decoder iters 45.0 (96.5)	Tok/s 12447 (10062)
0: TEST [1][69/126]	Time 0.1036 (0.2012)	Decoder iters 45.0 (90.9)	Tok/s 11661 (10204)
0: TEST [1][79/126]	Time 0.0961 (0.1944)	Decoder iters 43.0 (88.6)	Tok/s 11336 (10177)
0: TEST [1][89/126]	Time 0.0857 (0.1827)	Decoder iters 38.0 (83.1)	Tok/s 11046 (10331)
0: TEST [1][99/126]	Time 0.0666 (0.1756)	Decoder iters 29.0 (80.3)	Tok/s 11940 (10339)
0: TEST [1][109/126]	Time 0.0655 (0.1657)	Decoder iters 28.0 (75.7)	Tok/s 11201 (10447)
0: TEST [1][119/126]	Time 0.0409 (0.1582)	Decoder iters 17.0 (72.4)	Tok/s 12746 (10456)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 3.8980	Validation Loss: 4.7916	Test BLEU: 8.64
0: Performance: Epoch: 1	Training: 28514 Tok/s	Validation: 70505 Tok/s
0: Finished epoch 1
0: Total training time 564 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                  80|                      8.64|                      28516.8|                         9.392|
DONE!
