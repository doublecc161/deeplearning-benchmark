Warning:  if --fp16 is not used, static_loss_scale will be ignored.
=> creating model '('resnet50', 'classic', 1000)'
Version: {'net': <class 'image_classification.resnet.ResNet'>, 'block': <class 'image_classification.resnet.Bottleneck'>, 'layers': [3, 4, 6, 3], 'widths': [64, 128, 256, 512], 'expansion': 4}
Config: {'conv': <class 'torch.nn.modules.conv.Conv2d'>, 'conv_init': 'fan_out', 'nonlinearity': 'relu', 'last_bn_0_init': False, 'activation': <function <lambda> at 0x7fca52fc5e18>}
Num classes: 1000
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:133: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:133: DeprecationWarning: Argument 'image_type' for operator 'CropMirrorNormalize' is now deprecated. The argument is no longer used and should be removed.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
read 1281167 files from 1000 directories
/opt/conda/lib/python3.6/site-packages/nvidia/dali/plugin/base_iterator.py:121: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:174: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:174: DeprecationWarning: Argument 'image_type' for operator 'CropMirrorNormalize' is now deprecated. The argument is no longer used and should be removed.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
read 50000 files from 1000 directories
/opt/conda/lib/python3.6/site-packages/nvidia/dali/plugin/base_iterator.py:121: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
DLL 2022-03-16 21:20:20.527186 - PARAMETER data : /data/imagenet  data_backend : dali-cpu  arch : resnet50  model_config : classic  num_classes : 1000  workers : 5  epochs : 2  run_epochs : -1  batch_size : 112  optimizer_batch_size : -1  lr : 0.1  lr_schedule : step  warmup : 0  label_smoothing : 0.0  mixup : 0.0  momentum : 0.9  weight_decay : 0.0001  bn_weight_decay : False  nesterov : False  print_freq : 1  resume : None  pretrained_weights :   fp16 : False  static_loss_scale : 256.0  dynamic_loss_scale : False  prof : 100  amp : True  seed : None  gather_checkpoints : False  raport_file : benchmark.json  evaluate : False  training_only : True  save_checkpoints : True  checkpoint_filename : checkpoint.pth.tar  workspace : ./  memory_format : nchw  distributed : False  local_rank : 0  gpu : 0  world_size : 1 
 ! Weight decay NOT applied to BN parameters 
98
63
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : 256.0
RUNNING EPOCHS FROM 0 TO 2
Traceback (most recent call last):
  File "./main.py", line 541, in <module>
    main(args)
  File "./main.py", line 526, in main
    checkpoint_filename=args.checkpoint_filename,
  File "/workspace/examples/resnet50v1.5/image_classification/training.py", line 532, in train_loop
    batch_size_multiplier=batch_size_multiplier,
  File "/workspace/examples/resnet50v1.5/image_classification/training.py", line 340, in train
    loss = step(input, target, optimizer_step=optimizer_step)
  File "/workspace/examples/resnet50v1.5/image_classification/training.py", line 245, in _step
    scaled_loss.backward()
  File "/opt/conda/lib/python3.6/site-packages/torch/tensor.py", line 214, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.77 GiB total capacity; 4.15 GiB already allocated; 3.19 MiB free; 5.40 GiB reserved in total by PyTorch)
['/opt/conda/bin/python', '-u', './main.py', '/data/imagenet', '--arch', 'resnet50', '--amp', '--static-loss-scale', '256', '--epochs', '2', '--prof', '100', '--batch-size', '112', '--raport-file', 'benchmark.json', '--print-freq', '1', '--training-only']
DONE!
