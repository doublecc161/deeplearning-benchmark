0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070
Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=92, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2140
0: Scheduler decay interval: 268
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/1607]	Time 0.170 (0.000)	Data 9.65e-02 (0.00e+00)	Tok/s 15229 (0)	Loss/tok 10.5046 (10.5046)	LR 2.047e-05
0: TRAIN [0][10/1607]	Time 0.131 (0.113)	Data 4.79e-05 (5.50e-05)	Tok/s 31111 (28964)	Loss/tok 9.7238 (10.1167)	LR 2.576e-05
0: TRAIN [0][20/1607]	Time 0.162 (0.136)	Data 4.82e-05 (5.27e-05)	Tok/s 36238 (31012)	Loss/tok 9.3747 (9.7571)	LR 3.244e-05
0: TRAIN [0][30/1607]	Time 0.099 (0.138)	Data 4.74e-05 (5.48e-05)	Tok/s 26062 (31295)	Loss/tok 8.7982 (9.5437)	LR 4.083e-05
0: TRAIN [0][40/1607]	Time 0.164 (0.133)	Data 4.74e-05 (5.62e-05)	Tok/s 35587 (30659)	Loss/tok 8.8232 (9.3825)	LR 5.141e-05
0: TRAIN [0][50/1607]	Time 0.201 (0.135)	Data 4.67e-05 (5.65e-05)	Tok/s 37231 (30887)	Loss/tok 8.6416 (9.2235)	LR 6.472e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][60/1607]	Time 0.128 (0.133)	Data 7.34e-05 (5.82e-05)	Tok/s 32406 (30545)	Loss/tok 8.5183 (9.1093)	LR 8.148e-05
0: TRAIN [0][70/1607]	Time 0.133 (0.133)	Data 5.15e-05 (5.90e-05)	Tok/s 31709 (30683)	Loss/tok 8.2192 (8.9921)	LR 1.026e-04
0: TRAIN [0][80/1607]	Time 0.162 (0.133)	Data 4.72e-05 (5.88e-05)	Tok/s 35255 (30760)	Loss/tok 8.2320 (8.8929)	LR 1.291e-04
0: TRAIN [0][90/1607]	Time 0.162 (0.133)	Data 4.53e-05 (5.90e-05)	Tok/s 35998 (30780)	Loss/tok 7.9900 (8.7965)	LR 1.626e-04
0: TRAIN [0][100/1607]	Time 0.101 (0.133)	Data 5.08e-05 (5.84e-05)	Tok/s 24593 (30792)	Loss/tok 7.8339 (8.7078)	LR 2.047e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][110/1607]	Time 0.162 (0.134)	Data 4.63e-05 (5.73e-05)	Tok/s 36188 (31092)	Loss/tok 7.8608 (8.6245)	LR 2.576e-04
0: TRAIN [0][120/1607]	Time 0.163 (0.135)	Data 4.74e-05 (5.70e-05)	Tok/s 35474 (31115)	Loss/tok 7.8754 (8.5593)	LR 3.244e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][130/1607]	Time 0.193 (0.133)	Data 1.44e-04 (5.71e-05)	Tok/s 38847 (30850)	Loss/tok 8.5032 (8.5158)	LR 4.083e-04
0: TRAIN [0][140/1607]	Time 0.161 (0.133)	Data 4.60e-05 (5.64e-05)	Tok/s 35243 (30899)	Loss/tok 7.8581 (8.4695)	LR 5.141e-04
0: TRAIN [0][150/1607]	Time 0.131 (0.133)	Data 4.63e-05 (5.60e-05)	Tok/s 31962 (30951)	Loss/tok 7.6103 (8.4184)	LR 6.472e-04
0: TRAIN [0][160/1607]	Time 0.130 (0.132)	Data 5.15e-05 (5.55e-05)	Tok/s 31977 (30750)	Loss/tok 7.8117 (8.3776)	LR 8.148e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][170/1607]	Time 0.131 (0.132)	Data 4.55e-05 (5.53e-05)	Tok/s 31451 (30830)	Loss/tok 7.6860 (8.3408)	LR 1.026e-03
0: TRAIN [0][180/1607]	Time 0.130 (0.132)	Data 4.67e-05 (5.51e-05)	Tok/s 31548 (30898)	Loss/tok 7.5985 (8.3041)	LR 1.291e-03
0: TRAIN [0][190/1607]	Time 0.162 (0.133)	Data 4.84e-05 (5.49e-05)	Tok/s 35356 (31035)	Loss/tok 7.6687 (8.2663)	LR 1.626e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][200/1607]	Time 0.197 (0.133)	Data 7.41e-05 (5.51e-05)	Tok/s 37948 (31017)	Loss/tok 8.0381 (8.2410)	LR 2.000e-03
0: TRAIN [0][210/1607]	Time 0.131 (0.133)	Data 4.77e-05 (5.49e-05)	Tok/s 31537 (31116)	Loss/tok 7.4128 (8.2087)	LR 2.000e-03
0: TRAIN [0][220/1607]	Time 0.133 (0.134)	Data 7.80e-05 (5.49e-05)	Tok/s 31267 (31140)	Loss/tok 7.4096 (8.1742)	LR 2.000e-03
0: TRAIN [0][230/1607]	Time 0.162 (0.134)	Data 4.74e-05 (5.47e-05)	Tok/s 35843 (31096)	Loss/tok 7.5170 (8.1472)	LR 2.000e-03
0: TRAIN [0][240/1607]	Time 0.130 (0.133)	Data 4.74e-05 (5.46e-05)	Tok/s 31124 (31062)	Loss/tok 7.2672 (8.1149)	LR 2.000e-03
0: TRAIN [0][250/1607]	Time 0.196 (0.133)	Data 5.08e-05 (5.44e-05)	Tok/s 38212 (30981)	Loss/tok 7.6912 (8.0868)	LR 2.000e-03
0: TRAIN [0][260/1607]	Time 0.102 (0.133)	Data 4.77e-05 (5.45e-05)	Tok/s 24112 (30868)	Loss/tok 6.9914 (8.0619)	LR 2.000e-03
0: TRAIN [0][270/1607]	Time 0.131 (0.134)	Data 4.84e-05 (5.44e-05)	Tok/s 31405 (30982)	Loss/tok 6.9618 (8.0259)	LR 2.000e-03
0: TRAIN [0][280/1607]	Time 0.129 (0.134)	Data 1.10e-04 (5.47e-05)	Tok/s 32633 (31052)	Loss/tok 6.9929 (7.9924)	LR 2.000e-03
0: TRAIN [0][290/1607]	Time 0.130 (0.134)	Data 4.72e-05 (5.47e-05)	Tok/s 32275 (31089)	Loss/tok 7.0084 (7.9592)	LR 2.000e-03
0: TRAIN [0][300/1607]	Time 0.200 (0.135)	Data 4.82e-05 (5.47e-05)	Tok/s 37936 (31228)	Loss/tok 7.2389 (7.9219)	LR 2.000e-03
0: TRAIN [0][310/1607]	Time 0.164 (0.135)	Data 4.86e-05 (5.47e-05)	Tok/s 35361 (31209)	Loss/tok 6.9889 (7.8897)	LR 2.000e-03
0: TRAIN [0][320/1607]	Time 0.163 (0.135)	Data 4.74e-05 (5.48e-05)	Tok/s 34665 (31237)	Loss/tok 6.8893 (7.8560)	LR 2.000e-03
0: TRAIN [0][330/1607]	Time 0.162 (0.136)	Data 4.72e-05 (5.46e-05)	Tok/s 35939 (31289)	Loss/tok 6.8793 (7.8206)	LR 2.000e-03
0: TRAIN [0][340/1607]	Time 0.131 (0.136)	Data 4.84e-05 (5.46e-05)	Tok/s 32084 (31336)	Loss/tok 6.7268 (7.7871)	LR 2.000e-03
0: TRAIN [0][350/1607]	Time 0.099 (0.136)	Data 7.49e-05 (5.46e-05)	Tok/s 24965 (31293)	Loss/tok 6.3805 (7.7577)	LR 2.000e-03
0: TRAIN [0][360/1607]	Time 0.097 (0.136)	Data 4.65e-05 (5.45e-05)	Tok/s 26083 (31233)	Loss/tok 6.4032 (7.7302)	LR 2.000e-03
0: TRAIN [0][370/1607]	Time 0.163 (0.136)	Data 4.79e-05 (5.46e-05)	Tok/s 35485 (31271)	Loss/tok 6.7410 (7.6998)	LR 2.000e-03
0: TRAIN [0][380/1607]	Time 0.131 (0.136)	Data 8.32e-05 (5.45e-05)	Tok/s 31906 (31264)	Loss/tok 6.4127 (7.6703)	LR 2.000e-03
0: TRAIN [0][390/1607]	Time 0.100 (0.136)	Data 4.70e-05 (5.44e-05)	Tok/s 25338 (31209)	Loss/tok 6.1929 (7.6437)	LR 2.000e-03
0: TRAIN [0][400/1607]	Time 0.161 (0.136)	Data 7.22e-05 (5.44e-05)	Tok/s 35249 (31248)	Loss/tok 6.7312 (7.6154)	LR 2.000e-03
0: TRAIN [0][410/1607]	Time 0.132 (0.136)	Data 4.67e-05 (5.43e-05)	Tok/s 31655 (31187)	Loss/tok 6.2439 (7.5906)	LR 2.000e-03
0: TRAIN [0][420/1607]	Time 0.101 (0.136)	Data 4.79e-05 (5.43e-05)	Tok/s 25449 (31165)	Loss/tok 6.0698 (7.5644)	LR 2.000e-03
0: TRAIN [0][430/1607]	Time 0.195 (0.136)	Data 7.41e-05 (5.44e-05)	Tok/s 38520 (31163)	Loss/tok 6.6364 (7.5370)	LR 2.000e-03
0: TRAIN [0][440/1607]	Time 0.160 (0.136)	Data 4.82e-05 (5.42e-05)	Tok/s 36291 (31195)	Loss/tok 6.4178 (7.5079)	LR 2.000e-03
0: TRAIN [0][450/1607]	Time 0.200 (0.136)	Data 4.67e-05 (5.44e-05)	Tok/s 37389 (31204)	Loss/tok 6.7191 (7.4811)	LR 2.000e-03
0: TRAIN [0][460/1607]	Time 0.131 (0.136)	Data 4.55e-05 (5.43e-05)	Tok/s 31814 (31213)	Loss/tok 6.1864 (7.4556)	LR 2.000e-03
0: TRAIN [0][470/1607]	Time 0.160 (0.136)	Data 8.18e-05 (5.43e-05)	Tok/s 35753 (31252)	Loss/tok 6.2627 (7.4287)	LR 2.000e-03
0: TRAIN [0][480/1607]	Time 0.129 (0.136)	Data 5.20e-05 (5.42e-05)	Tok/s 31701 (31226)	Loss/tok 6.0618 (7.4048)	LR 2.000e-03
0: TRAIN [0][490/1607]	Time 0.100 (0.136)	Data 4.67e-05 (5.42e-05)	Tok/s 25426 (31201)	Loss/tok 5.8048 (7.3817)	LR 2.000e-03
0: TRAIN [0][500/1607]	Time 0.100 (0.136)	Data 4.67e-05 (5.41e-05)	Tok/s 25357 (31159)	Loss/tok 5.7557 (7.3597)	LR 2.000e-03
0: TRAIN [0][510/1607]	Time 0.161 (0.136)	Data 5.17e-05 (5.41e-05)	Tok/s 36055 (31223)	Loss/tok 6.2124 (7.3319)	LR 2.000e-03
0: TRAIN [0][520/1607]	Time 0.201 (0.136)	Data 4.72e-05 (5.40e-05)	Tok/s 37860 (31243)	Loss/tok 6.4693 (7.3065)	LR 2.000e-03
0: TRAIN [0][530/1607]	Time 0.131 (0.137)	Data 5.03e-05 (5.40e-05)	Tok/s 31013 (31253)	Loss/tok 5.8311 (7.2815)	LR 2.000e-03
0: TRAIN [0][540/1607]	Time 0.199 (0.137)	Data 4.86e-05 (5.40e-05)	Tok/s 37625 (31260)	Loss/tok 6.3378 (7.2578)	LR 2.000e-03
0: TRAIN [0][550/1607]	Time 0.129 (0.137)	Data 4.82e-05 (5.39e-05)	Tok/s 32003 (31300)	Loss/tok 5.8473 (7.2321)	LR 2.000e-03
0: TRAIN [0][560/1607]	Time 0.129 (0.137)	Data 4.72e-05 (5.39e-05)	Tok/s 31457 (31290)	Loss/tok 5.9672 (7.2106)	LR 2.000e-03
0: TRAIN [0][570/1607]	Time 0.165 (0.137)	Data 4.84e-05 (5.38e-05)	Tok/s 34603 (31274)	Loss/tok 5.8404 (7.1887)	LR 2.000e-03
0: TRAIN [0][580/1607]	Time 0.163 (0.137)	Data 4.72e-05 (5.38e-05)	Tok/s 34924 (31240)	Loss/tok 5.9883 (7.1682)	LR 2.000e-03
0: TRAIN [0][590/1607]	Time 0.131 (0.137)	Data 4.77e-05 (5.39e-05)	Tok/s 32209 (31290)	Loss/tok 5.7305 (7.1428)	LR 2.000e-03
0: TRAIN [0][600/1607]	Time 0.163 (0.137)	Data 4.72e-05 (5.38e-05)	Tok/s 35844 (31317)	Loss/tok 5.9234 (7.1194)	LR 2.000e-03
0: TRAIN [0][610/1607]	Time 0.096 (0.137)	Data 4.67e-05 (5.40e-05)	Tok/s 25744 (31256)	Loss/tok 5.4423 (7.1011)	LR 2.000e-03
0: TRAIN [0][620/1607]	Time 0.074 (0.137)	Data 4.79e-05 (5.40e-05)	Tok/s 16040 (31287)	Loss/tok 5.1991 (7.0769)	LR 2.000e-03
0: TRAIN [0][630/1607]	Time 0.096 (0.137)	Data 7.41e-05 (5.40e-05)	Tok/s 26733 (31215)	Loss/tok 5.2070 (7.0598)	LR 2.000e-03
0: TRAIN [0][640/1607]	Time 0.098 (0.136)	Data 4.70e-05 (5.40e-05)	Tok/s 25177 (31212)	Loss/tok 5.2232 (7.0387)	LR 2.000e-03
0: TRAIN [0][650/1607]	Time 0.132 (0.137)	Data 4.79e-05 (5.40e-05)	Tok/s 31462 (31219)	Loss/tok 5.5372 (7.0172)	LR 2.000e-03
0: TRAIN [0][660/1607]	Time 0.134 (0.137)	Data 4.82e-05 (5.39e-05)	Tok/s 30697 (31232)	Loss/tok 5.4370 (6.9962)	LR 2.000e-03
0: TRAIN [0][670/1607]	Time 0.129 (0.137)	Data 5.15e-05 (5.39e-05)	Tok/s 32145 (31231)	Loss/tok 5.4127 (6.9755)	LR 2.000e-03
0: TRAIN [0][680/1607]	Time 0.164 (0.137)	Data 4.60e-05 (5.40e-05)	Tok/s 35484 (31279)	Loss/tok 5.5514 (6.9511)	LR 2.000e-03
0: TRAIN [0][690/1607]	Time 0.165 (0.137)	Data 8.23e-05 (5.41e-05)	Tok/s 34451 (31272)	Loss/tok 5.6993 (6.9318)	LR 2.000e-03
0: TRAIN [0][700/1607]	Time 0.132 (0.137)	Data 4.82e-05 (5.41e-05)	Tok/s 32052 (31296)	Loss/tok 5.4201 (6.9088)	LR 2.000e-03
0: TRAIN [0][710/1607]	Time 0.164 (0.137)	Data 4.70e-05 (5.41e-05)	Tok/s 35473 (31302)	Loss/tok 5.6119 (6.8879)	LR 2.000e-03
0: TRAIN [0][720/1607]	Time 0.202 (0.137)	Data 5.27e-05 (5.41e-05)	Tok/s 37041 (31292)	Loss/tok 5.8433 (6.8683)	LR 2.000e-03
0: TRAIN [0][730/1607]	Time 0.133 (0.137)	Data 4.70e-05 (5.41e-05)	Tok/s 31621 (31274)	Loss/tok 5.3766 (6.8506)	LR 2.000e-03
0: TRAIN [0][740/1607]	Time 0.099 (0.137)	Data 5.08e-05 (5.40e-05)	Tok/s 24999 (31272)	Loss/tok 4.9614 (6.8314)	LR 2.000e-03
0: TRAIN [0][750/1607]	Time 0.202 (0.137)	Data 5.20e-05 (5.40e-05)	Tok/s 37458 (31287)	Loss/tok 5.6462 (6.8113)	LR 2.000e-03
0: TRAIN [0][760/1607]	Time 0.101 (0.137)	Data 4.65e-05 (5.40e-05)	Tok/s 24778 (31279)	Loss/tok 4.8470 (6.7934)	LR 2.000e-03
0: TRAIN [0][770/1607]	Time 0.132 (0.138)	Data 4.74e-05 (5.39e-05)	Tok/s 31249 (31310)	Loss/tok 5.1394 (6.7721)	LR 2.000e-03
0: TRAIN [0][780/1607]	Time 0.098 (0.137)	Data 4.77e-05 (5.39e-05)	Tok/s 26085 (31296)	Loss/tok 4.9040 (6.7551)	LR 2.000e-03
0: TRAIN [0][790/1607]	Time 0.100 (0.137)	Data 4.84e-05 (5.38e-05)	Tok/s 26251 (31277)	Loss/tok 5.0485 (6.7386)	LR 2.000e-03
0: TRAIN [0][800/1607]	Time 0.100 (0.137)	Data 5.03e-05 (5.38e-05)	Tok/s 24787 (31236)	Loss/tok 4.8472 (6.7236)	LR 2.000e-03
0: TRAIN [0][810/1607]	Time 0.096 (0.137)	Data 5.96e-05 (5.38e-05)	Tok/s 26128 (31207)	Loss/tok 4.8984 (6.7075)	LR 2.000e-03
0: TRAIN [0][820/1607]	Time 0.127 (0.137)	Data 7.53e-05 (5.38e-05)	Tok/s 32396 (31185)	Loss/tok 5.2158 (6.6920)	LR 2.000e-03
0: TRAIN [0][830/1607]	Time 0.201 (0.137)	Data 4.86e-05 (5.38e-05)	Tok/s 37666 (31245)	Loss/tok 5.4779 (6.6698)	LR 2.000e-03
0: TRAIN [0][840/1607]	Time 0.071 (0.137)	Data 4.79e-05 (5.38e-05)	Tok/s 16821 (31232)	Loss/tok 4.4268 (6.6533)	LR 2.000e-03
0: TRAIN [0][850/1607]	Time 0.099 (0.137)	Data 7.32e-05 (5.38e-05)	Tok/s 25134 (31243)	Loss/tok 4.7312 (6.6350)	LR 2.000e-03
0: TRAIN [0][860/1607]	Time 0.101 (0.137)	Data 4.58e-05 (5.37e-05)	Tok/s 23717 (31250)	Loss/tok 4.6744 (6.6182)	LR 2.000e-03
0: TRAIN [0][870/1607]	Time 0.135 (0.137)	Data 4.77e-05 (5.37e-05)	Tok/s 31059 (31240)	Loss/tok 5.1813 (6.6027)	LR 2.000e-03
0: TRAIN [0][880/1607]	Time 0.164 (0.138)	Data 6.27e-05 (5.37e-05)	Tok/s 35271 (31275)	Loss/tok 5.3821 (6.5838)	LR 2.000e-03
0: TRAIN [0][890/1607]	Time 0.198 (0.138)	Data 7.58e-05 (5.37e-05)	Tok/s 37791 (31300)	Loss/tok 5.5976 (6.5660)	LR 2.000e-03
0: TRAIN [0][900/1607]	Time 0.128 (0.138)	Data 7.72e-05 (5.37e-05)	Tok/s 32031 (31245)	Loss/tok 5.0383 (6.5540)	LR 2.000e-03
0: TRAIN [0][910/1607]	Time 0.101 (0.137)	Data 7.87e-05 (5.37e-05)	Tok/s 24655 (31227)	Loss/tok 4.7657 (6.5394)	LR 2.000e-03
0: TRAIN [0][920/1607]	Time 0.074 (0.137)	Data 4.74e-05 (5.37e-05)	Tok/s 15682 (31229)	Loss/tok 4.0732 (6.5231)	LR 2.000e-03
0: TRAIN [0][930/1607]	Time 0.101 (0.137)	Data 5.05e-05 (5.37e-05)	Tok/s 26233 (31232)	Loss/tok 4.5623 (6.5070)	LR 2.000e-03
0: TRAIN [0][940/1607]	Time 0.161 (0.138)	Data 7.61e-05 (5.37e-05)	Tok/s 36367 (31259)	Loss/tok 5.1810 (6.4898)	LR 2.000e-03
0: TRAIN [0][950/1607]	Time 0.163 (0.138)	Data 4.77e-05 (5.36e-05)	Tok/s 35429 (31271)	Loss/tok 5.1304 (6.4734)	LR 2.000e-03
0: TRAIN [0][960/1607]	Time 0.098 (0.138)	Data 4.77e-05 (5.36e-05)	Tok/s 25486 (31261)	Loss/tok 4.4603 (6.4588)	LR 2.000e-03
0: TRAIN [0][970/1607]	Time 0.132 (0.138)	Data 4.79e-05 (5.36e-05)	Tok/s 31125 (31260)	Loss/tok 4.9752 (6.4439)	LR 2.000e-03
0: TRAIN [0][980/1607]	Time 0.073 (0.138)	Data 7.77e-05 (5.35e-05)	Tok/s 16638 (31265)	Loss/tok 4.4217 (6.4278)	LR 2.000e-03
0: TRAIN [0][990/1607]	Time 0.202 (0.138)	Data 4.74e-05 (5.35e-05)	Tok/s 36822 (31280)	Loss/tok 5.2478 (6.4119)	LR 2.000e-03
0: TRAIN [0][1000/1607]	Time 0.133 (0.138)	Data 4.82e-05 (5.35e-05)	Tok/s 30984 (31283)	Loss/tok 4.8484 (6.3972)	LR 2.000e-03
0: TRAIN [0][1010/1607]	Time 0.162 (0.138)	Data 1.05e-04 (5.35e-05)	Tok/s 35841 (31270)	Loss/tok 5.0202 (6.3837)	LR 2.000e-03
0: TRAIN [0][1020/1607]	Time 0.098 (0.138)	Data 7.44e-05 (5.35e-05)	Tok/s 26076 (31285)	Loss/tok 4.5573 (6.3686)	LR 2.000e-03
0: TRAIN [0][1030/1607]	Time 0.201 (0.138)	Data 4.79e-05 (5.35e-05)	Tok/s 37407 (31292)	Loss/tok 5.1633 (6.3541)	LR 2.000e-03
0: TRAIN [0][1040/1607]	Time 0.164 (0.138)	Data 6.03e-05 (5.36e-05)	Tok/s 35221 (31274)	Loss/tok 4.8618 (6.3416)	LR 2.000e-03
0: TRAIN [0][1050/1607]	Time 0.072 (0.138)	Data 4.72e-05 (5.35e-05)	Tok/s 17166 (31272)	Loss/tok 4.2000 (6.3278)	LR 2.000e-03
0: TRAIN [0][1060/1607]	Time 0.165 (0.138)	Data 4.63e-05 (5.35e-05)	Tok/s 34836 (31303)	Loss/tok 5.0324 (6.3113)	LR 2.000e-03
0: TRAIN [0][1070/1607]	Time 0.101 (0.138)	Data 5.01e-05 (5.35e-05)	Tok/s 24229 (31298)	Loss/tok 4.5267 (6.2978)	LR 2.000e-03
0: TRAIN [0][1080/1607]	Time 0.165 (0.138)	Data 4.72e-05 (5.35e-05)	Tok/s 34887 (31293)	Loss/tok 4.8938 (6.2845)	LR 2.000e-03
0: TRAIN [0][1090/1607]	Time 0.131 (0.138)	Data 4.77e-05 (5.35e-05)	Tok/s 31209 (31316)	Loss/tok 4.8379 (6.2693)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1100/1607]	Time 0.199 (0.138)	Data 4.84e-05 (5.35e-05)	Tok/s 38288 (31327)	Loss/tok 5.0283 (6.2553)	LR 2.000e-03
0: TRAIN [0][1110/1607]	Time 0.193 (0.138)	Data 1.00e-04 (5.36e-05)	Tok/s 38983 (31294)	Loss/tok 5.0596 (6.2445)	LR 2.000e-03
0: TRAIN [0][1120/1607]	Time 0.098 (0.138)	Data 4.74e-05 (5.36e-05)	Tok/s 24154 (31305)	Loss/tok 4.3185 (6.2305)	LR 2.000e-03
0: TRAIN [0][1130/1607]	Time 0.127 (0.138)	Data 4.74e-05 (5.36e-05)	Tok/s 32246 (31317)	Loss/tok 4.8565 (6.2169)	LR 2.000e-03
0: TRAIN [0][1140/1607]	Time 0.098 (0.138)	Data 7.75e-05 (5.35e-05)	Tok/s 26405 (31298)	Loss/tok 4.2777 (6.2059)	LR 2.000e-03
0: TRAIN [0][1150/1607]	Time 0.132 (0.138)	Data 4.72e-05 (5.35e-05)	Tok/s 31322 (31307)	Loss/tok 4.5246 (6.1923)	LR 2.000e-03
0: TRAIN [0][1160/1607]	Time 0.129 (0.138)	Data 4.77e-05 (5.35e-05)	Tok/s 32761 (31290)	Loss/tok 4.6582 (6.1808)	LR 2.000e-03
0: TRAIN [0][1170/1607]	Time 0.130 (0.138)	Data 4.60e-05 (5.35e-05)	Tok/s 31782 (31271)	Loss/tok 4.5761 (6.1697)	LR 2.000e-03
0: TRAIN [0][1180/1607]	Time 0.164 (0.138)	Data 4.74e-05 (5.34e-05)	Tok/s 34840 (31280)	Loss/tok 4.8348 (6.1569)	LR 2.000e-03
0: TRAIN [0][1190/1607]	Time 0.159 (0.138)	Data 5.13e-05 (5.34e-05)	Tok/s 36855 (31260)	Loss/tok 4.7341 (6.1462)	LR 2.000e-03
0: TRAIN [0][1200/1607]	Time 0.132 (0.138)	Data 4.79e-05 (5.34e-05)	Tok/s 31704 (31250)	Loss/tok 4.5978 (6.1352)	LR 2.000e-03
0: TRAIN [0][1210/1607]	Time 0.201 (0.138)	Data 4.65e-05 (5.34e-05)	Tok/s 37400 (31243)	Loss/tok 4.9014 (6.1236)	LR 2.000e-03
0: TRAIN [0][1220/1607]	Time 0.098 (0.138)	Data 4.72e-05 (5.34e-05)	Tok/s 25705 (31227)	Loss/tok 4.2545 (6.1129)	LR 2.000e-03
0: TRAIN [0][1230/1607]	Time 0.197 (0.138)	Data 7.44e-05 (5.34e-05)	Tok/s 37548 (31199)	Loss/tok 5.0022 (6.1034)	LR 2.000e-03
0: TRAIN [0][1240/1607]	Time 0.100 (0.137)	Data 4.82e-05 (5.34e-05)	Tok/s 25482 (31188)	Loss/tok 4.3400 (6.0930)	LR 2.000e-03
0: TRAIN [0][1250/1607]	Time 0.101 (0.137)	Data 8.13e-05 (5.34e-05)	Tok/s 24922 (31175)	Loss/tok 4.0847 (6.0824)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1260/1607]	Time 0.200 (0.138)	Data 4.72e-05 (5.34e-05)	Tok/s 37394 (31213)	Loss/tok 4.8504 (6.0687)	LR 2.000e-03
0: TRAIN [0][1270/1607]	Time 0.101 (0.138)	Data 4.74e-05 (5.34e-05)	Tok/s 23722 (31214)	Loss/tok 4.3448 (6.0573)	LR 2.000e-03
0: TRAIN [0][1280/1607]	Time 0.130 (0.138)	Data 4.98e-05 (5.34e-05)	Tok/s 31214 (31213)	Loss/tok 4.2170 (6.0458)	LR 2.000e-03
0: TRAIN [0][1290/1607]	Time 0.131 (0.138)	Data 4.74e-05 (5.34e-05)	Tok/s 31551 (31218)	Loss/tok 4.3636 (6.0343)	LR 2.000e-03
0: TRAIN [0][1300/1607]	Time 0.100 (0.138)	Data 4.63e-05 (5.34e-05)	Tok/s 24980 (31209)	Loss/tok 4.2171 (6.0239)	LR 2.000e-03
0: TRAIN [0][1310/1607]	Time 0.133 (0.137)	Data 5.05e-05 (5.34e-05)	Tok/s 31039 (31188)	Loss/tok 4.4916 (6.0150)	LR 2.000e-03
0: TRAIN [0][1320/1607]	Time 0.159 (0.138)	Data 7.41e-05 (5.34e-05)	Tok/s 36708 (31196)	Loss/tok 4.6920 (6.0038)	LR 2.000e-03
Traceback (most recent call last):
  File "train.py", line 636, in <module>
    main()
  File "train.py", line 561, in main
    train_loss, train_perf = trainer.optimize(train_loader)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 352, in optimize
    output = self.feed_data(data_loader, training=True)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 236, in feed_data
    stats = self.iterate(src, tgt, update, training=training)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 192, in iterate
    update)
  File "/workspace/examples/gnmt/seq2seq/train/fp_optimizers.py", line 231, in step
    scaled_loss.backward()
  File "/opt/conda/lib/python3.6/site-packages/torch/tensor.py", line 214, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 548.00 MiB (GPU 0; 7.77 GiB total capacity; 5.02 GiB already allocated; 149.19 MiB free; 6.41 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 257, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python3', '-u', 'train.py', '--local_rank=0', '--dataset-dir', '/data/gnmt/wmt16_de_en', '--train-batch-size', '92', '--val-batch-size', '32', '--test-batch-size', '32', '--math', 'fp16', '--epochs', '2', '--seed', '2']' returned non-zero exit status 1.
DONE!
