0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070
Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=92, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2140
0: Scheduler decay interval: 268
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/1607]	Time 0.157 (0.000)	Data 8.50e-02 (0.00e+00)	Tok/s 16511 (0)	Loss/tok 10.5046 (10.5046)	LR 2.047e-05
0: TRAIN [0][10/1607]	Time 0.130 (0.113)	Data 4.63e-05 (5.13e-05)	Tok/s 31198 (29210)	Loss/tok 9.7238 (10.1167)	LR 2.576e-05
0: TRAIN [0][20/1607]	Time 0.162 (0.135)	Data 4.60e-05 (5.05e-05)	Tok/s 36119 (31149)	Loss/tok 9.3747 (9.7571)	LR 3.244e-05
0: TRAIN [0][30/1607]	Time 0.099 (0.137)	Data 4.58e-05 (4.91e-05)	Tok/s 25993 (31395)	Loss/tok 8.7982 (9.5437)	LR 4.083e-05
0: TRAIN [0][40/1607]	Time 0.162 (0.133)	Data 4.63e-05 (4.93e-05)	Tok/s 35961 (30742)	Loss/tok 8.8232 (9.3825)	LR 5.141e-05
0: TRAIN [0][50/1607]	Time 0.197 (0.134)	Data 4.89e-05 (4.89e-05)	Tok/s 38075 (30990)	Loss/tok 8.6416 (9.2235)	LR 6.472e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][60/1607]	Time 0.128 (0.133)	Data 7.96e-05 (4.94e-05)	Tok/s 32283 (30649)	Loss/tok 8.5183 (9.1093)	LR 8.148e-05
0: TRAIN [0][70/1607]	Time 0.130 (0.133)	Data 4.63e-05 (4.94e-05)	Tok/s 32268 (30792)	Loss/tok 8.2192 (8.9921)	LR 1.026e-04
0: TRAIN [0][80/1607]	Time 0.163 (0.133)	Data 4.84e-05 (5.00e-05)	Tok/s 34958 (30847)	Loss/tok 8.2320 (8.8929)	LR 1.291e-04
0: TRAIN [0][90/1607]	Time 0.163 (0.133)	Data 4.70e-05 (4.96e-05)	Tok/s 35886 (30857)	Loss/tok 7.9900 (8.7965)	LR 1.626e-04
0: TRAIN [0][100/1607]	Time 0.101 (0.133)	Data 4.58e-05 (4.99e-05)	Tok/s 24619 (30861)	Loss/tok 7.8339 (8.7078)	LR 2.047e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][110/1607]	Time 0.162 (0.134)	Data 4.84e-05 (5.04e-05)	Tok/s 36213 (31154)	Loss/tok 7.8608 (8.6245)	LR 2.576e-04
0: TRAIN [0][120/1607]	Time 0.165 (0.134)	Data 8.34e-05 (5.04e-05)	Tok/s 35192 (31166)	Loss/tok 7.8754 (8.5593)	LR 3.244e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][130/1607]	Time 0.196 (0.133)	Data 7.20e-05 (5.10e-05)	Tok/s 38188 (30895)	Loss/tok 8.5032 (8.5158)	LR 4.083e-04
0: TRAIN [0][140/1607]	Time 0.162 (0.133)	Data 4.96e-05 (5.14e-05)	Tok/s 35124 (30923)	Loss/tok 7.8581 (8.4695)	LR 5.141e-04
0: TRAIN [0][150/1607]	Time 0.131 (0.133)	Data 4.58e-05 (5.13e-05)	Tok/s 31818 (30965)	Loss/tok 7.6103 (8.4184)	LR 6.472e-04
0: TRAIN [0][160/1607]	Time 0.131 (0.132)	Data 4.65e-05 (5.16e-05)	Tok/s 31909 (30755)	Loss/tok 7.8117 (8.3776)	LR 8.148e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][170/1607]	Time 0.130 (0.132)	Data 5.41e-05 (5.14e-05)	Tok/s 31502 (30835)	Loss/tok 7.6860 (8.3408)	LR 1.026e-03
0: TRAIN [0][180/1607]	Time 0.128 (0.132)	Data 7.30e-05 (5.15e-05)	Tok/s 32026 (30903)	Loss/tok 7.5985 (8.3041)	LR 1.291e-03
0: TRAIN [0][190/1607]	Time 0.162 (0.133)	Data 4.55e-05 (5.17e-05)	Tok/s 35313 (31036)	Loss/tok 7.6687 (8.2663)	LR 1.626e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][200/1607]	Time 0.197 (0.133)	Data 5.41e-05 (5.19e-05)	Tok/s 37914 (31015)	Loss/tok 8.0381 (8.2410)	LR 2.000e-03
0: TRAIN [0][210/1607]	Time 0.131 (0.133)	Data 4.55e-05 (5.17e-05)	Tok/s 31537 (31110)	Loss/tok 7.4128 (8.2087)	LR 2.000e-03
0: TRAIN [0][220/1607]	Time 0.131 (0.134)	Data 4.60e-05 (5.19e-05)	Tok/s 31876 (31135)	Loss/tok 7.4096 (8.1742)	LR 2.000e-03
0: TRAIN [0][230/1607]	Time 0.162 (0.134)	Data 5.15e-05 (5.20e-05)	Tok/s 35947 (31088)	Loss/tok 7.5170 (8.1472)	LR 2.000e-03
0: TRAIN [0][240/1607]	Time 0.130 (0.134)	Data 4.74e-05 (5.18e-05)	Tok/s 31159 (31056)	Loss/tok 7.2672 (8.1149)	LR 2.000e-03
0: TRAIN [0][250/1607]	Time 0.197 (0.133)	Data 4.58e-05 (5.20e-05)	Tok/s 38099 (30973)	Loss/tok 7.6912 (8.0868)	LR 2.000e-03
0: TRAIN [0][260/1607]	Time 0.100 (0.133)	Data 6.13e-05 (5.21e-05)	Tok/s 24523 (30863)	Loss/tok 6.9914 (8.0619)	LR 2.000e-03
0: TRAIN [0][270/1607]	Time 0.131 (0.134)	Data 4.74e-05 (5.19e-05)	Tok/s 31428 (30975)	Loss/tok 6.9618 (8.0259)	LR 2.000e-03
0: TRAIN [0][280/1607]	Time 0.130 (0.134)	Data 4.58e-05 (5.19e-05)	Tok/s 32225 (31042)	Loss/tok 6.9929 (7.9924)	LR 2.000e-03
0: TRAIN [0][290/1607]	Time 0.131 (0.134)	Data 5.15e-05 (5.20e-05)	Tok/s 32124 (31077)	Loss/tok 7.0084 (7.9592)	LR 2.000e-03
0: TRAIN [0][300/1607]	Time 0.200 (0.135)	Data 7.41e-05 (5.19e-05)	Tok/s 37964 (31217)	Loss/tok 7.2389 (7.9219)	LR 2.000e-03
0: TRAIN [0][310/1607]	Time 0.162 (0.135)	Data 4.72e-05 (5.19e-05)	Tok/s 35690 (31199)	Loss/tok 6.9889 (7.8897)	LR 2.000e-03
0: TRAIN [0][320/1607]	Time 0.164 (0.136)	Data 4.70e-05 (5.21e-05)	Tok/s 34586 (31224)	Loss/tok 6.8893 (7.8560)	LR 2.000e-03
0: TRAIN [0][330/1607]	Time 0.162 (0.136)	Data 8.06e-05 (5.23e-05)	Tok/s 35822 (31274)	Loss/tok 6.8793 (7.8206)	LR 2.000e-03
0: TRAIN [0][340/1607]	Time 0.130 (0.136)	Data 4.58e-05 (5.26e-05)	Tok/s 32196 (31320)	Loss/tok 6.7268 (7.7871)	LR 2.000e-03
0: TRAIN [0][350/1607]	Time 0.101 (0.136)	Data 4.70e-05 (5.27e-05)	Tok/s 24656 (31278)	Loss/tok 6.3805 (7.7577)	LR 2.000e-03
0: TRAIN [0][360/1607]	Time 0.098 (0.136)	Data 5.10e-05 (5.27e-05)	Tok/s 25871 (31216)	Loss/tok 6.4032 (7.7302)	LR 2.000e-03
0: TRAIN [0][370/1607]	Time 0.161 (0.136)	Data 4.77e-05 (5.26e-05)	Tok/s 35727 (31256)	Loss/tok 6.7410 (7.6998)	LR 2.000e-03
0: TRAIN [0][380/1607]	Time 0.130 (0.136)	Data 4.72e-05 (5.27e-05)	Tok/s 32110 (31251)	Loss/tok 6.4127 (7.6703)	LR 2.000e-03
0: TRAIN [0][390/1607]	Time 0.098 (0.136)	Data 7.51e-05 (5.28e-05)	Tok/s 25893 (31198)	Loss/tok 6.1929 (7.6437)	LR 2.000e-03
0: TRAIN [0][400/1607]	Time 0.163 (0.136)	Data 4.67e-05 (5.26e-05)	Tok/s 34845 (31240)	Loss/tok 6.7312 (7.6154)	LR 2.000e-03
0: TRAIN [0][410/1607]	Time 0.131 (0.136)	Data 4.67e-05 (5.27e-05)	Tok/s 31892 (31182)	Loss/tok 6.2439 (7.5906)	LR 2.000e-03
0: TRAIN [0][420/1607]	Time 0.100 (0.136)	Data 5.27e-05 (5.26e-05)	Tok/s 25532 (31162)	Loss/tok 6.0698 (7.5644)	LR 2.000e-03
0: TRAIN [0][430/1607]	Time 0.195 (0.136)	Data 7.30e-05 (5.28e-05)	Tok/s 38513 (31162)	Loss/tok 6.6364 (7.5370)	LR 2.000e-03
0: TRAIN [0][440/1607]	Time 0.160 (0.136)	Data 4.72e-05 (5.28e-05)	Tok/s 36446 (31193)	Loss/tok 6.4178 (7.5079)	LR 2.000e-03
0: TRAIN [0][450/1607]	Time 0.199 (0.136)	Data 4.79e-05 (5.28e-05)	Tok/s 37480 (31201)	Loss/tok 6.7191 (7.4811)	LR 2.000e-03
0: TRAIN [0][460/1607]	Time 0.131 (0.136)	Data 4.60e-05 (5.26e-05)	Tok/s 31708 (31211)	Loss/tok 6.1864 (7.4556)	LR 2.000e-03
0: TRAIN [0][470/1607]	Time 0.164 (0.136)	Data 4.60e-05 (5.27e-05)	Tok/s 34906 (31250)	Loss/tok 6.2627 (7.4287)	LR 2.000e-03
0: TRAIN [0][480/1607]	Time 0.128 (0.136)	Data 4.72e-05 (5.28e-05)	Tok/s 31721 (31226)	Loss/tok 6.0618 (7.4048)	LR 2.000e-03
0: TRAIN [0][490/1607]	Time 0.102 (0.136)	Data 4.74e-05 (5.28e-05)	Tok/s 24931 (31201)	Loss/tok 5.8048 (7.3817)	LR 2.000e-03
0: TRAIN [0][500/1607]	Time 0.102 (0.136)	Data 4.70e-05 (5.28e-05)	Tok/s 24863 (31160)	Loss/tok 5.7557 (7.3597)	LR 2.000e-03
0: TRAIN [0][510/1607]	Time 0.161 (0.136)	Data 5.44e-05 (5.28e-05)	Tok/s 36015 (31224)	Loss/tok 6.2124 (7.3319)	LR 2.000e-03
0: TRAIN [0][520/1607]	Time 0.201 (0.136)	Data 4.91e-05 (5.27e-05)	Tok/s 37810 (31244)	Loss/tok 6.4693 (7.3065)	LR 2.000e-03
0: TRAIN [0][530/1607]	Time 0.132 (0.137)	Data 4.63e-05 (5.27e-05)	Tok/s 30928 (31255)	Loss/tok 5.8311 (7.2815)	LR 2.000e-03
0: TRAIN [0][540/1607]	Time 0.200 (0.137)	Data 4.86e-05 (5.27e-05)	Tok/s 37576 (31261)	Loss/tok 6.3378 (7.2578)	LR 2.000e-03
0: TRAIN [0][550/1607]	Time 0.129 (0.137)	Data 5.41e-05 (5.27e-05)	Tok/s 32063 (31301)	Loss/tok 5.8473 (7.2321)	LR 2.000e-03
0: TRAIN [0][560/1607]	Time 0.129 (0.137)	Data 4.53e-05 (5.26e-05)	Tok/s 31514 (31292)	Loss/tok 5.9672 (7.2106)	LR 2.000e-03
0: TRAIN [0][570/1607]	Time 0.164 (0.137)	Data 4.82e-05 (5.27e-05)	Tok/s 34792 (31276)	Loss/tok 5.8404 (7.1887)	LR 2.000e-03
0: TRAIN [0][580/1607]	Time 0.161 (0.137)	Data 7.61e-05 (5.27e-05)	Tok/s 35430 (31243)	Loss/tok 5.9883 (7.1682)	LR 2.000e-03
0: TRAIN [0][590/1607]	Time 0.131 (0.137)	Data 4.58e-05 (5.27e-05)	Tok/s 32263 (31294)	Loss/tok 5.7305 (7.1428)	LR 2.000e-03
0: TRAIN [0][600/1607]	Time 0.164 (0.137)	Data 4.65e-05 (5.27e-05)	Tok/s 35788 (31321)	Loss/tok 5.9234 (7.1194)	LR 2.000e-03
0: TRAIN [0][610/1607]	Time 0.098 (0.137)	Data 5.67e-05 (5.26e-05)	Tok/s 25189 (31260)	Loss/tok 5.4423 (7.1011)	LR 2.000e-03
0: TRAIN [0][620/1607]	Time 0.072 (0.137)	Data 4.86e-05 (5.26e-05)	Tok/s 16561 (31291)	Loss/tok 5.1991 (7.0769)	LR 2.000e-03
0: TRAIN [0][630/1607]	Time 0.097 (0.137)	Data 7.32e-05 (5.27e-05)	Tok/s 26653 (31218)	Loss/tok 5.2070 (7.0598)	LR 2.000e-03
0: TRAIN [0][640/1607]	Time 0.096 (0.136)	Data 8.25e-05 (5.27e-05)	Tok/s 25685 (31214)	Loss/tok 5.2232 (7.0387)	LR 2.000e-03
0: TRAIN [0][650/1607]	Time 0.132 (0.137)	Data 4.55e-05 (5.26e-05)	Tok/s 31304 (31221)	Loss/tok 5.5372 (7.0172)	LR 2.000e-03
0: TRAIN [0][660/1607]	Time 0.131 (0.137)	Data 4.67e-05 (5.27e-05)	Tok/s 31260 (31234)	Loss/tok 5.4370 (6.9962)	LR 2.000e-03
0: TRAIN [0][670/1607]	Time 0.131 (0.137)	Data 5.34e-05 (5.28e-05)	Tok/s 31671 (31232)	Loss/tok 5.4127 (6.9755)	LR 2.000e-03
0: TRAIN [0][680/1607]	Time 0.162 (0.137)	Data 7.44e-05 (5.27e-05)	Tok/s 35803 (31280)	Loss/tok 5.5514 (6.9511)	LR 2.000e-03
0: TRAIN [0][690/1607]	Time 0.165 (0.137)	Data 4.74e-05 (5.27e-05)	Tok/s 34502 (31273)	Loss/tok 5.6993 (6.9318)	LR 2.000e-03
0: TRAIN [0][700/1607]	Time 0.134 (0.137)	Data 4.84e-05 (5.28e-05)	Tok/s 31483 (31295)	Loss/tok 5.4201 (6.9088)	LR 2.000e-03
0: TRAIN [0][710/1607]	Time 0.166 (0.137)	Data 7.56e-05 (5.29e-05)	Tok/s 35078 (31300)	Loss/tok 5.6119 (6.8879)	LR 2.000e-03
0: TRAIN [0][720/1607]	Time 0.203 (0.137)	Data 4.58e-05 (5.29e-05)	Tok/s 36908 (31290)	Loss/tok 5.8433 (6.8683)	LR 2.000e-03
0: TRAIN [0][730/1607]	Time 0.133 (0.137)	Data 4.55e-05 (5.29e-05)	Tok/s 31484 (31270)	Loss/tok 5.3766 (6.8506)	LR 2.000e-03
0: TRAIN [0][740/1607]	Time 0.102 (0.137)	Data 6.79e-05 (5.29e-05)	Tok/s 24169 (31268)	Loss/tok 4.9614 (6.8314)	LR 2.000e-03
0: TRAIN [0][750/1607]	Time 0.202 (0.137)	Data 4.63e-05 (5.28e-05)	Tok/s 37379 (31283)	Loss/tok 5.6462 (6.8113)	LR 2.000e-03
0: TRAIN [0][760/1607]	Time 0.101 (0.137)	Data 4.65e-05 (5.28e-05)	Tok/s 24806 (31274)	Loss/tok 4.8470 (6.7934)	LR 2.000e-03
0: TRAIN [0][770/1607]	Time 0.134 (0.138)	Data 5.20e-05 (5.28e-05)	Tok/s 30832 (31304)	Loss/tok 5.1394 (6.7721)	LR 2.000e-03
0: TRAIN [0][780/1607]	Time 0.098 (0.137)	Data 4.74e-05 (5.27e-05)	Tok/s 26184 (31290)	Loss/tok 4.9040 (6.7551)	LR 2.000e-03
0: TRAIN [0][790/1607]	Time 0.101 (0.137)	Data 4.74e-05 (5.28e-05)	Tok/s 26072 (31270)	Loss/tok 5.0485 (6.7386)	LR 2.000e-03
0: TRAIN [0][800/1607]	Time 0.101 (0.137)	Data 5.79e-05 (5.28e-05)	Tok/s 24630 (31229)	Loss/tok 4.8472 (6.7236)	LR 2.000e-03
0: TRAIN [0][810/1607]	Time 0.096 (0.137)	Data 7.15e-05 (5.28e-05)	Tok/s 26182 (31200)	Loss/tok 4.8984 (6.7075)	LR 2.000e-03
0: TRAIN [0][820/1607]	Time 0.129 (0.137)	Data 4.77e-05 (5.28e-05)	Tok/s 31854 (31177)	Loss/tok 5.2158 (6.6920)	LR 2.000e-03
0: TRAIN [0][830/1607]	Time 0.202 (0.137)	Data 5.89e-05 (5.29e-05)	Tok/s 37420 (31237)	Loss/tok 5.4779 (6.6698)	LR 2.000e-03
0: TRAIN [0][840/1607]	Time 0.073 (0.137)	Data 4.86e-05 (5.29e-05)	Tok/s 16403 (31223)	Loss/tok 4.4268 (6.6533)	LR 2.000e-03
0: TRAIN [0][850/1607]	Time 0.101 (0.137)	Data 4.63e-05 (5.29e-05)	Tok/s 24693 (31234)	Loss/tok 4.7312 (6.6350)	LR 2.000e-03
0: TRAIN [0][860/1607]	Time 0.101 (0.137)	Data 5.05e-05 (5.29e-05)	Tok/s 23610 (31241)	Loss/tok 4.6744 (6.6182)	LR 2.000e-03
0: TRAIN [0][870/1607]	Time 0.132 (0.137)	Data 4.65e-05 (5.28e-05)	Tok/s 31613 (31232)	Loss/tok 5.1813 (6.6027)	LR 2.000e-03
0: TRAIN [0][880/1607]	Time 0.167 (0.138)	Data 4.82e-05 (5.28e-05)	Tok/s 34689 (31266)	Loss/tok 5.3821 (6.5838)	LR 2.000e-03
0: TRAIN [0][890/1607]	Time 0.200 (0.138)	Data 4.53e-05 (5.28e-05)	Tok/s 37394 (31292)	Loss/tok 5.5976 (6.5660)	LR 2.000e-03
0: TRAIN [0][900/1607]	Time 0.128 (0.138)	Data 8.39e-05 (5.29e-05)	Tok/s 32015 (31236)	Loss/tok 5.0383 (6.5540)	LR 2.000e-03
0: TRAIN [0][910/1607]	Time 0.102 (0.137)	Data 5.05e-05 (5.29e-05)	Tok/s 24314 (31218)	Loss/tok 4.7657 (6.5394)	LR 2.000e-03
0: TRAIN [0][920/1607]	Time 0.074 (0.138)	Data 4.79e-05 (5.29e-05)	Tok/s 15722 (31221)	Loss/tok 4.0732 (6.5231)	LR 2.000e-03
0: TRAIN [0][930/1607]	Time 0.101 (0.138)	Data 7.51e-05 (5.30e-05)	Tok/s 26314 (31225)	Loss/tok 4.5623 (6.5070)	LR 2.000e-03
0: TRAIN [0][940/1607]	Time 0.163 (0.138)	Data 4.72e-05 (5.29e-05)	Tok/s 35921 (31252)	Loss/tok 5.1810 (6.4898)	LR 2.000e-03
0: TRAIN [0][950/1607]	Time 0.166 (0.138)	Data 4.70e-05 (5.29e-05)	Tok/s 34713 (31263)	Loss/tok 5.1304 (6.4734)	LR 2.000e-03
0: TRAIN [0][960/1607]	Time 0.098 (0.138)	Data 4.72e-05 (5.29e-05)	Tok/s 25508 (31254)	Loss/tok 4.4603 (6.4588)	LR 2.000e-03
0: TRAIN [0][970/1607]	Time 0.132 (0.138)	Data 4.58e-05 (5.29e-05)	Tok/s 31180 (31252)	Loss/tok 4.9752 (6.4439)	LR 2.000e-03
0: TRAIN [0][980/1607]	Time 0.072 (0.138)	Data 4.94e-05 (5.29e-05)	Tok/s 16815 (31258)	Loss/tok 4.4217 (6.4278)	LR 2.000e-03
0: TRAIN [0][990/1607]	Time 0.201 (0.138)	Data 4.63e-05 (5.30e-05)	Tok/s 36847 (31273)	Loss/tok 5.2478 (6.4119)	LR 2.000e-03
0: TRAIN [0][1000/1607]	Time 0.132 (0.138)	Data 7.63e-05 (5.30e-05)	Tok/s 31043 (31276)	Loss/tok 4.8484 (6.3972)	LR 2.000e-03
0: TRAIN [0][1010/1607]	Time 0.165 (0.138)	Data 4.89e-05 (5.29e-05)	Tok/s 35307 (31263)	Loss/tok 5.0202 (6.3837)	LR 2.000e-03
0: TRAIN [0][1020/1607]	Time 0.101 (0.138)	Data 4.82e-05 (5.30e-05)	Tok/s 25416 (31278)	Loss/tok 4.5573 (6.3686)	LR 2.000e-03
0: TRAIN [0][1030/1607]	Time 0.201 (0.138)	Data 7.30e-05 (5.29e-05)	Tok/s 37428 (31285)	Loss/tok 5.1633 (6.3541)	LR 2.000e-03
0: TRAIN [0][1040/1607]	Time 0.165 (0.138)	Data 4.74e-05 (5.29e-05)	Tok/s 35128 (31267)	Loss/tok 4.8618 (6.3416)	LR 2.000e-03
0: TRAIN [0][1050/1607]	Time 0.074 (0.138)	Data 4.82e-05 (5.29e-05)	Tok/s 16731 (31265)	Loss/tok 4.2000 (6.3278)	LR 2.000e-03
0: TRAIN [0][1060/1607]	Time 0.163 (0.138)	Data 5.27e-05 (5.29e-05)	Tok/s 35230 (31296)	Loss/tok 5.0324 (6.3113)	LR 2.000e-03
0: TRAIN [0][1070/1607]	Time 0.101 (0.138)	Data 4.70e-05 (5.29e-05)	Tok/s 24126 (31291)	Loss/tok 4.5267 (6.2978)	LR 2.000e-03
0: TRAIN [0][1080/1607]	Time 0.163 (0.138)	Data 4.84e-05 (5.29e-05)	Tok/s 35344 (31286)	Loss/tok 4.8938 (6.2845)	LR 2.000e-03
0: TRAIN [0][1090/1607]	Time 0.131 (0.138)	Data 4.82e-05 (5.30e-05)	Tok/s 31199 (31308)	Loss/tok 4.8379 (6.2693)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1100/1607]	Time 0.199 (0.138)	Data 7.92e-05 (5.30e-05)	Tok/s 38382 (31319)	Loss/tok 5.0283 (6.2553)	LR 2.000e-03
0: TRAIN [0][1110/1607]	Time 0.196 (0.138)	Data 4.96e-05 (5.29e-05)	Tok/s 38476 (31287)	Loss/tok 5.0596 (6.2445)	LR 2.000e-03
0: TRAIN [0][1120/1607]	Time 0.100 (0.138)	Data 4.79e-05 (5.29e-05)	Tok/s 23732 (31297)	Loss/tok 4.3185 (6.2305)	LR 2.000e-03
0: TRAIN [0][1130/1607]	Time 0.127 (0.138)	Data 5.75e-05 (5.29e-05)	Tok/s 32373 (31310)	Loss/tok 4.8565 (6.2169)	LR 2.000e-03
0: TRAIN [0][1140/1607]	Time 0.100 (0.138)	Data 4.63e-05 (5.29e-05)	Tok/s 26010 (31290)	Loss/tok 4.2777 (6.2059)	LR 2.000e-03
0: TRAIN [0][1150/1607]	Time 0.132 (0.138)	Data 4.70e-05 (5.29e-05)	Tok/s 31324 (31300)	Loss/tok 4.5246 (6.1923)	LR 2.000e-03
0: TRAIN [0][1160/1607]	Time 0.130 (0.138)	Data 4.58e-05 (5.29e-05)	Tok/s 32699 (31283)	Loss/tok 4.6582 (6.1808)	LR 2.000e-03
0: TRAIN [0][1170/1607]	Time 0.129 (0.138)	Data 4.65e-05 (5.29e-05)	Tok/s 31992 (31264)	Loss/tok 4.5761 (6.1697)	LR 2.000e-03
0: TRAIN [0][1180/1607]	Time 0.166 (0.138)	Data 4.82e-05 (5.29e-05)	Tok/s 34425 (31272)	Loss/tok 4.8348 (6.1569)	LR 2.000e-03
0: TRAIN [0][1190/1607]	Time 0.159 (0.138)	Data 4.55e-05 (5.29e-05)	Tok/s 36796 (31252)	Loss/tok 4.7341 (6.1462)	LR 2.000e-03
0: TRAIN [0][1200/1607]	Time 0.133 (0.138)	Data 7.46e-05 (5.29e-05)	Tok/s 31602 (31241)	Loss/tok 4.5978 (6.1352)	LR 2.000e-03
0: TRAIN [0][1210/1607]	Time 0.197 (0.138)	Data 8.01e-05 (5.30e-05)	Tok/s 38112 (31236)	Loss/tok 4.9014 (6.1236)	LR 2.000e-03
0: TRAIN [0][1220/1607]	Time 0.098 (0.138)	Data 4.70e-05 (5.30e-05)	Tok/s 25774 (31218)	Loss/tok 4.2545 (6.1129)	LR 2.000e-03
0: TRAIN [0][1230/1607]	Time 0.198 (0.138)	Data 7.68e-05 (5.31e-05)	Tok/s 37333 (31190)	Loss/tok 5.0022 (6.1034)	LR 2.000e-03
0: TRAIN [0][1240/1607]	Time 0.101 (0.137)	Data 7.99e-05 (5.31e-05)	Tok/s 25348 (31180)	Loss/tok 4.3400 (6.0930)	LR 2.000e-03
0: TRAIN [0][1250/1607]	Time 0.101 (0.137)	Data 7.34e-05 (5.31e-05)	Tok/s 24874 (31167)	Loss/tok 4.0847 (6.0824)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1260/1607]	Time 0.201 (0.138)	Data 4.79e-05 (5.31e-05)	Tok/s 37307 (31206)	Loss/tok 4.8504 (6.0687)	LR 2.000e-03
0: TRAIN [0][1270/1607]	Time 0.100 (0.138)	Data 5.58e-05 (5.32e-05)	Tok/s 23847 (31206)	Loss/tok 4.3448 (6.0573)	LR 2.000e-03
0: TRAIN [0][1280/1607]	Time 0.134 (0.138)	Data 4.53e-05 (5.32e-05)	Tok/s 30386 (31206)	Loss/tok 4.2170 (6.0458)	LR 2.000e-03
0: TRAIN [0][1290/1607]	Time 0.131 (0.138)	Data 7.80e-05 (5.33e-05)	Tok/s 31504 (31211)	Loss/tok 4.3636 (6.0343)	LR 2.000e-03
0: TRAIN [0][1300/1607]	Time 0.097 (0.138)	Data 5.32e-05 (5.34e-05)	Tok/s 25768 (31203)	Loss/tok 4.2171 (6.0239)	LR 2.000e-03
0: TRAIN [0][1310/1607]	Time 0.133 (0.137)	Data 4.72e-05 (5.34e-05)	Tok/s 30971 (31182)	Loss/tok 4.4916 (6.0150)	LR 2.000e-03
0: TRAIN [0][1320/1607]	Time 0.160 (0.138)	Data 7.22e-05 (5.34e-05)	Tok/s 36557 (31190)	Loss/tok 4.6920 (6.0038)	LR 2.000e-03
0: TRAIN [0][1330/1607]	Time 0.134 (0.138)	Data 5.44e-05 (5.35e-05)	Tok/s 31601 (31200)	Loss/tok 4.5317 (5.9926)	LR 2.000e-03
0: TRAIN [0][1340/1607]	Time 0.164 (0.138)	Data 4.58e-05 (5.34e-05)	Tok/s 34672 (31202)	Loss/tok 4.7086 (5.9816)	LR 2.000e-03
0: TRAIN [0][1350/1607]	Time 0.130 (0.138)	Data 7.27e-05 (5.34e-05)	Tok/s 31716 (31209)	Loss/tok 4.4596 (5.9705)	LR 2.000e-03
0: TRAIN [0][1360/1607]	Time 0.100 (0.138)	Data 4.53e-05 (5.34e-05)	Tok/s 25372 (31219)	Loss/tok 4.1570 (5.9594)	LR 2.000e-03
0: TRAIN [0][1370/1607]	Time 0.162 (0.138)	Data 7.84e-05 (5.34e-05)	Tok/s 35550 (31217)	Loss/tok 4.6153 (5.9495)	LR 2.000e-03
0: TRAIN [0][1380/1607]	Time 0.130 (0.138)	Data 4.63e-05 (5.35e-05)	Tok/s 31980 (31220)	Loss/tok 4.4298 (5.9390)	LR 2.000e-03
0: TRAIN [0][1390/1607]	Time 0.131 (0.138)	Data 4.96e-05 (5.35e-05)	Tok/s 31663 (31224)	Loss/tok 4.4521 (5.9284)	LR 2.000e-03
0: TRAIN [0][1400/1607]	Time 0.202 (0.138)	Data 5.72e-05 (5.35e-05)	Tok/s 36573 (31235)	Loss/tok 4.8128 (5.9176)	LR 2.000e-03
0: TRAIN [0][1410/1607]	Time 0.101 (0.138)	Data 4.72e-05 (5.35e-05)	Tok/s 24760 (31238)	Loss/tok 4.1916 (5.9074)	LR 2.000e-03
0: TRAIN [0][1420/1607]	Time 0.129 (0.138)	Data 4.58e-05 (5.36e-05)	Tok/s 32385 (31232)	Loss/tok 4.4242 (5.8980)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1430/1607]	Time 0.132 (0.138)	Data 9.78e-05 (5.36e-05)	Tok/s 31296 (31237)	Loss/tok 4.2455 (5.8885)	LR 2.000e-03
0: TRAIN [0][1440/1607]	Time 0.164 (0.138)	Data 4.53e-05 (5.37e-05)	Tok/s 35345 (31226)	Loss/tok 4.6385 (5.8794)	LR 2.000e-03
0: TRAIN [0][1450/1607]	Time 0.074 (0.138)	Data 4.70e-05 (5.37e-05)	Tok/s 16575 (31225)	Loss/tok 3.7750 (5.8698)	LR 2.000e-03
0: TRAIN [0][1460/1607]	Time 0.130 (0.138)	Data 7.70e-05 (5.38e-05)	Tok/s 32136 (31232)	Loss/tok 4.4607 (5.8598)	LR 2.000e-03
0: TRAIN [0][1470/1607]	Time 0.131 (0.138)	Data 4.55e-05 (5.37e-05)	Tok/s 31240 (31226)	Loss/tok 4.2369 (5.8508)	LR 2.000e-03
0: TRAIN [0][1480/1607]	Time 0.072 (0.138)	Data 4.67e-05 (5.38e-05)	Tok/s 16966 (31220)	Loss/tok 3.8570 (5.8423)	LR 2.000e-03
0: TRAIN [0][1490/1607]	Time 0.133 (0.138)	Data 5.17e-05 (5.37e-05)	Tok/s 30966 (31203)	Loss/tok 4.1664 (5.8344)	LR 2.000e-03
0: TRAIN [0][1500/1607]	Time 0.101 (0.138)	Data 4.65e-05 (5.37e-05)	Tok/s 24712 (31201)	Loss/tok 3.9611 (5.8252)	LR 2.000e-03
0: TRAIN [0][1510/1607]	Time 0.129 (0.138)	Data 9.47e-05 (5.38e-05)	Tok/s 32220 (31193)	Loss/tok 4.4438 (5.8169)	LR 2.000e-03
0: TRAIN [0][1520/1607]	Time 0.073 (0.138)	Data 7.77e-05 (5.38e-05)	Tok/s 16763 (31176)	Loss/tok 3.8823 (5.8092)	LR 2.000e-03
0: TRAIN [0][1530/1607]	Time 0.128 (0.138)	Data 7.61e-05 (5.39e-05)	Tok/s 32627 (31180)	Loss/tok 4.4001 (5.8000)	LR 2.000e-03
0: TRAIN [0][1540/1607]	Time 0.129 (0.138)	Data 4.70e-05 (5.39e-05)	Tok/s 32137 (31160)	Loss/tok 4.1391 (5.7923)	LR 2.000e-03
0: TRAIN [0][1550/1607]	Time 0.162 (0.138)	Data 5.72e-05 (5.39e-05)	Tok/s 35956 (31165)	Loss/tok 4.7546 (5.7831)	LR 2.000e-03
0: TRAIN [0][1560/1607]	Time 0.127 (0.138)	Data 9.82e-05 (5.39e-05)	Tok/s 32834 (31160)	Loss/tok 4.3928 (5.7750)	LR 2.000e-03
0: TRAIN [0][1570/1607]	Time 0.134 (0.138)	Data 4.98e-05 (5.40e-05)	Tok/s 30970 (31143)	Loss/tok 4.2490 (5.7674)	LR 2.000e-03
0: TRAIN [0][1580/1607]	Time 0.201 (0.137)	Data 4.70e-05 (5.39e-05)	Tok/s 37306 (31134)	Loss/tok 4.7648 (5.7594)	LR 2.000e-03
0: TRAIN [0][1590/1607]	Time 0.165 (0.138)	Data 4.91e-05 (5.40e-05)	Tok/s 34904 (31137)	Loss/tok 4.4666 (5.7503)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1600/1607]	Time 0.136 (0.138)	Data 4.70e-05 (5.40e-05)	Tok/s 31357 (31157)	Loss/tok 4.2749 (5.7407)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
Traceback (most recent call last):
  File "train.py", line 636, in <module>
    main()
  File "train.py", line 567, in main
    val_loss, val_perf = trainer.evaluate(val_loader)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 367, in evaluate
    training=False)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 336, in preallocate
    self.iterate(src, tgt, update=False, training=training)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 180, in iterate
    output = self.model(src, src_length, tgt[:-1])
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/workspace/examples/gnmt/seq2seq/models/gnmt.py", line 68, in forward
    context = self.encode(input_encoder, input_enc_len)
  File "/workspace/examples/gnmt/seq2seq/models/seq2seq_base.py", line 52, in encode
    return self.encoder(inputs, lengths)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/gnmt/seq2seq/models/encoder.py", line 107, in forward
    x, _ = self.rnn_layers[1](x)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 257, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python3', '-u', 'train.py', '--local_rank=0', '--dataset-dir', '/data/gnmt/wmt16_de_en', '--train-batch-size', '92', '--val-batch-size', '32', '--test-batch-size', '32', '--math', 'fp16', '--epochs', '2', '--seed', '2']' returned non-zero exit status 1.
DONE!
