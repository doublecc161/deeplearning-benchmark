DLL 2022-03-16 21:24:28.199587 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 32  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 4040
loading annotations into memory...
Done (t=0.27s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "main.py", line 287, in <module>
    train(train_loop_func, logger, args)
  File "main.py", line 206, in train
    logger, args, mean, std)
  File "/workspace/examples/ssd/src/train.py", line 136, in benchmark_train_loop
    with amp.scale_loss(loss, optim) as scale_loss:
  File "/opt/conda/lib/python3.6/contextlib.py", line 81, in __enter__
    return next(self.gen)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/handle.py", line 111, in scale_loss
    optimizer._prepare_amp_backward()
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 145, in prepare_backward_with_master_weights
    self._amp_lazy_init()
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 317, in _amp_lazy_init
    self._lazy_init_maybe_master_weights()
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_process_optimizer.py", line 44, in lazy_init_with_master_weights
    master_param = param.detach().clone().float()
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.77 GiB total capacity; 3.84 GiB already allocated; 3.19 MiB free; 6.41 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 257, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'main.py', '--local_rank=0', '--mode', 'benchmark-training', '--data', '/data/object_detection', '--batch-size', '32', '--benchmark-warmup', '50', '--benchmark-iterations', '200', '--amp', '--learning-rate', '0']' returned non-zero exit status 1.
DONE!
