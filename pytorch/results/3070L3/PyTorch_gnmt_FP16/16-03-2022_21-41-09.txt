0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070
Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2461
0: Scheduler decay interval: 308
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/1848]	Time 0.194 (0.000)	Data 1.02e-01 (0.00e+00)	Tok/s 18713 (0)	Loss/tok 10.5912 (10.5912)	LR 2.047e-05
0: TRAIN [0][10/1848]	Time 0.122 (0.122)	Data 4.39e-05 (4.76e-05)	Tok/s 29261 (28996)	Loss/tok 9.7189 (10.1349)	LR 2.576e-05
0: TRAIN [0][20/1848]	Time 0.097 (0.123)	Data 4.55e-05 (4.96e-05)	Tok/s 22082 (27475)	Loss/tok 9.0378 (9.8274)	LR 3.244e-05
0: TRAIN [0][30/1848]	Time 0.153 (0.125)	Data 4.46e-05 (4.87e-05)	Tok/s 32261 (27969)	Loss/tok 9.0055 (9.5785)	LR 4.083e-05
0: TRAIN [0][40/1848]	Time 0.126 (0.122)	Data 4.72e-05 (4.94e-05)	Tok/s 28225 (27295)	Loss/tok 8.7691 (9.4263)	LR 5.141e-05
0: TRAIN [0][50/1848]	Time 0.153 (0.120)	Data 5.63e-05 (5.01e-05)	Tok/s 32449 (27192)	Loss/tok 8.9142 (9.2936)	LR 6.472e-05
0: TRAIN [0][60/1848]	Time 0.125 (0.123)	Data 4.32e-05 (4.95e-05)	Tok/s 29257 (27578)	Loss/tok 8.6687 (9.1615)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][70/1848]	Time 0.152 (0.125)	Data 7.03e-05 (5.06e-05)	Tok/s 32499 (28259)	Loss/tok 8.3456 (9.0707)	LR 1.026e-04
0: TRAIN [0][80/1848]	Time 0.125 (0.126)	Data 6.56e-05 (5.06e-05)	Tok/s 28828 (28246)	Loss/tok 8.0534 (8.9627)	LR 1.291e-04
0: TRAIN [0][90/1848]	Time 0.096 (0.124)	Data 4.43e-05 (5.00e-05)	Tok/s 23219 (27954)	Loss/tok 7.9235 (8.8701)	LR 1.626e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][100/1848]	Time 0.153 (0.123)	Data 4.65e-05 (5.05e-05)	Tok/s 33037 (27996)	Loss/tok 7.9150 (8.7798)	LR 2.047e-04
0: TRAIN [0][110/1848]	Time 0.125 (0.125)	Data 4.24e-05 (5.02e-05)	Tok/s 28244 (28260)	Loss/tok 7.7304 (8.6859)	LR 2.576e-04
0: TRAIN [0][120/1848]	Time 0.096 (0.124)	Data 4.94e-05 (5.04e-05)	Tok/s 22725 (28027)	Loss/tok 7.4818 (8.6246)	LR 3.244e-04
0: TRAIN [0][130/1848]	Time 0.150 (0.124)	Data 1.02e-04 (5.04e-05)	Tok/s 33137 (27911)	Loss/tok 7.8260 (8.5623)	LR 4.083e-04
0: TRAIN [0][140/1848]	Time 0.068 (0.124)	Data 4.17e-05 (5.02e-05)	Tok/s 16055 (27882)	Loss/tok 7.3713 (8.5050)	LR 5.141e-04
0: TRAIN [0][150/1848]	Time 0.124 (0.123)	Data 5.15e-05 (5.13e-05)	Tok/s 29341 (27773)	Loss/tok 7.8309 (8.4559)	LR 6.472e-04
0: TRAIN [0][160/1848]	Time 0.125 (0.124)	Data 6.72e-05 (5.15e-05)	Tok/s 28976 (27813)	Loss/tok 7.6131 (8.4078)	LR 8.148e-04
0: TRAIN [0][170/1848]	Time 0.124 (0.124)	Data 9.39e-05 (5.22e-05)	Tok/s 28881 (27851)	Loss/tok 7.8473 (8.3650)	LR 1.026e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][180/1848]	Time 0.066 (0.124)	Data 7.49e-05 (5.28e-05)	Tok/s 32708 (27924)	Loss/tok 7.3849 (8.3314)	LR 1.291e-03
0: TRAIN [0][190/1848]	Time 0.124 (0.124)	Data 5.20e-05 (5.25e-05)	Tok/s 28994 (28011)	Loss/tok 7.5243 (8.2955)	LR 1.626e-03
0: TRAIN [0][200/1848]	Time 0.097 (0.125)	Data 4.77e-05 (5.25e-05)	Tok/s 23413 (27998)	Loss/tok 7.4214 (8.2686)	LR 2.000e-03
0: TRAIN [0][210/1848]	Time 0.126 (0.125)	Data 4.46e-05 (5.21e-05)	Tok/s 27601 (27929)	Loss/tok 7.5716 (8.2378)	LR 2.000e-03
0: TRAIN [0][220/1848]	Time 0.125 (0.125)	Data 4.34e-05 (5.21e-05)	Tok/s 28635 (28007)	Loss/tok 7.4339 (8.2012)	LR 2.000e-03
0: TRAIN [0][230/1848]	Time 0.151 (0.126)	Data 8.13e-05 (5.23e-05)	Tok/s 33030 (28035)	Loss/tok 7.6559 (8.1664)	LR 2.000e-03
0: TRAIN [0][240/1848]	Time 0.097 (0.126)	Data 7.92e-05 (5.21e-05)	Tok/s 22652 (28059)	Loss/tok 6.9944 (8.1337)	LR 2.000e-03
0: TRAIN [0][250/1848]	Time 0.154 (0.127)	Data 5.10e-05 (5.20e-05)	Tok/s 32289 (28148)	Loss/tok 7.2919 (8.0956)	LR 2.000e-03
0: TRAIN [0][260/1848]	Time 0.125 (0.127)	Data 4.41e-05 (5.21e-05)	Tok/s 29248 (28125)	Loss/tok 7.1171 (8.0606)	LR 2.000e-03
0: TRAIN [0][270/1848]	Time 0.123 (0.126)	Data 4.32e-05 (5.19e-05)	Tok/s 28755 (28000)	Loss/tok 7.1266 (8.0318)	LR 2.000e-03
0: TRAIN [0][280/1848]	Time 0.154 (0.126)	Data 4.55e-05 (5.19e-05)	Tok/s 33087 (28053)	Loss/tok 7.2064 (7.9966)	LR 2.000e-03
0: TRAIN [0][290/1848]	Time 0.154 (0.127)	Data 5.05e-05 (5.20e-05)	Tok/s 32339 (28063)	Loss/tok 7.2112 (7.9651)	LR 2.000e-03
0: TRAIN [0][300/1848]	Time 0.153 (0.127)	Data 5.08e-05 (5.20e-05)	Tok/s 33774 (28079)	Loss/tok 7.0520 (7.9311)	LR 2.000e-03
0: TRAIN [0][310/1848]	Time 0.154 (0.127)	Data 4.63e-05 (5.21e-05)	Tok/s 33125 (28113)	Loss/tok 7.1268 (7.9002)	LR 2.000e-03
0: TRAIN [0][320/1848]	Time 0.189 (0.128)	Data 5.08e-05 (5.20e-05)	Tok/s 34185 (28179)	Loss/tok 7.0834 (7.8658)	LR 2.000e-03
0: TRAIN [0][330/1848]	Time 0.100 (0.128)	Data 4.34e-05 (5.18e-05)	Tok/s 22373 (28268)	Loss/tok 6.4515 (7.8298)	LR 2.000e-03
0: TRAIN [0][340/1848]	Time 0.192 (0.129)	Data 5.25e-05 (5.19e-05)	Tok/s 34414 (28339)	Loss/tok 7.1153 (7.7989)	LR 2.000e-03
0: TRAIN [0][350/1848]	Time 0.097 (0.129)	Data 4.36e-05 (5.19e-05)	Tok/s 22278 (28346)	Loss/tok 6.5800 (7.7709)	LR 2.000e-03
0: TRAIN [0][360/1848]	Time 0.100 (0.129)	Data 4.32e-05 (5.18e-05)	Tok/s 21720 (28335)	Loss/tok 6.3589 (7.7435)	LR 2.000e-03
0: TRAIN [0][370/1848]	Time 0.125 (0.129)	Data 4.48e-05 (5.20e-05)	Tok/s 28592 (28364)	Loss/tok 6.4887 (7.7135)	LR 2.000e-03
0: TRAIN [0][380/1848]	Time 0.097 (0.129)	Data 5.03e-05 (5.20e-05)	Tok/s 22774 (28388)	Loss/tok 6.3129 (7.6840)	LR 2.000e-03
0: TRAIN [0][390/1848]	Time 0.125 (0.129)	Data 4.53e-05 (5.19e-05)	Tok/s 28523 (28399)	Loss/tok 6.4967 (7.6562)	LR 2.000e-03
0: TRAIN [0][400/1848]	Time 0.099 (0.129)	Data 4.43e-05 (5.19e-05)	Tok/s 21896 (28346)	Loss/tok 6.2939 (7.6318)	LR 2.000e-03
0: TRAIN [0][410/1848]	Time 0.189 (0.129)	Data 5.63e-05 (5.17e-05)	Tok/s 34536 (28362)	Loss/tok 6.7111 (7.6047)	LR 2.000e-03
0: TRAIN [0][420/1848]	Time 0.125 (0.129)	Data 4.48e-05 (5.17e-05)	Tok/s 28970 (28371)	Loss/tok 6.5727 (7.5789)	LR 2.000e-03
0: TRAIN [0][430/1848]	Time 0.126 (0.129)	Data 4.46e-05 (5.16e-05)	Tok/s 28575 (28386)	Loss/tok 6.3869 (7.5537)	LR 2.000e-03
0: TRAIN [0][440/1848]	Time 0.126 (0.129)	Data 7.77e-05 (5.15e-05)	Tok/s 28594 (28355)	Loss/tok 6.3602 (7.5306)	LR 2.000e-03
0: TRAIN [0][450/1848]	Time 0.186 (0.129)	Data 4.27e-05 (5.14e-05)	Tok/s 35437 (28336)	Loss/tok 6.6693 (7.5075)	LR 2.000e-03
0: TRAIN [0][460/1848]	Time 0.126 (0.129)	Data 4.74e-05 (5.15e-05)	Tok/s 28555 (28303)	Loss/tok 6.4741 (7.4851)	LR 2.000e-03
0: TRAIN [0][470/1848]	Time 0.192 (0.129)	Data 4.29e-05 (5.13e-05)	Tok/s 34155 (28323)	Loss/tok 6.6504 (7.4606)	LR 2.000e-03
0: TRAIN [0][480/1848]	Time 0.126 (0.129)	Data 4.32e-05 (5.13e-05)	Tok/s 28048 (28316)	Loss/tok 6.1768 (7.4381)	LR 2.000e-03
0: TRAIN [0][490/1848]	Time 0.124 (0.129)	Data 5.05e-05 (5.12e-05)	Tok/s 28764 (28305)	Loss/tok 6.1371 (7.4150)	LR 2.000e-03
0: TRAIN [0][500/1848]	Time 0.155 (0.128)	Data 4.43e-05 (5.12e-05)	Tok/s 32104 (28266)	Loss/tok 6.3861 (7.3946)	LR 2.000e-03
0: TRAIN [0][510/1848]	Time 0.185 (0.128)	Data 4.89e-05 (5.12e-05)	Tok/s 34956 (28259)	Loss/tok 6.5801 (7.3730)	LR 2.000e-03
0: TRAIN [0][520/1848]	Time 0.125 (0.128)	Data 4.29e-05 (5.11e-05)	Tok/s 28412 (28214)	Loss/tok 6.0859 (7.3539)	LR 2.000e-03
0: TRAIN [0][530/1848]	Time 0.154 (0.128)	Data 4.39e-05 (5.12e-05)	Tok/s 32376 (28179)	Loss/tok 6.4177 (7.3334)	LR 2.000e-03
0: TRAIN [0][540/1848]	Time 0.153 (0.128)	Data 7.51e-05 (5.12e-05)	Tok/s 33603 (28182)	Loss/tok 6.2003 (7.3120)	LR 2.000e-03
0: TRAIN [0][550/1848]	Time 0.125 (0.128)	Data 4.29e-05 (5.11e-05)	Tok/s 29011 (28230)	Loss/tok 5.9827 (7.2873)	LR 2.000e-03
0: TRAIN [0][560/1848]	Time 0.154 (0.128)	Data 4.51e-05 (5.12e-05)	Tok/s 32847 (28219)	Loss/tok 6.1611 (7.2663)	LR 2.000e-03
0: TRAIN [0][570/1848]	Time 0.125 (0.128)	Data 7.39e-05 (5.11e-05)	Tok/s 29066 (28200)	Loss/tok 5.8091 (7.2457)	LR 2.000e-03
0: TRAIN [0][580/1848]	Time 0.154 (0.128)	Data 4.39e-05 (5.11e-05)	Tok/s 33087 (28227)	Loss/tok 6.1816 (7.2234)	LR 2.000e-03
0: TRAIN [0][590/1848]	Time 0.070 (0.128)	Data 4.58e-05 (5.12e-05)	Tok/s 15320 (28194)	Loss/tok 5.2959 (7.2050)	LR 2.000e-03
0: TRAIN [0][600/1848]	Time 0.152 (0.128)	Data 4.22e-05 (5.11e-05)	Tok/s 32501 (28171)	Loss/tok 6.1959 (7.1872)	LR 2.000e-03
0: TRAIN [0][610/1848]	Time 0.190 (0.128)	Data 4.29e-05 (5.11e-05)	Tok/s 33873 (28193)	Loss/tok 6.3031 (7.1658)	LR 2.000e-03
0: TRAIN [0][620/1848]	Time 0.128 (0.128)	Data 5.48e-05 (5.11e-05)	Tok/s 27603 (28165)	Loss/tok 5.8464 (7.1474)	LR 2.000e-03
0: TRAIN [0][630/1848]	Time 0.126 (0.128)	Data 4.55e-05 (5.11e-05)	Tok/s 28404 (28179)	Loss/tok 5.8580 (7.1262)	LR 2.000e-03
0: TRAIN [0][640/1848]	Time 0.126 (0.128)	Data 4.43e-05 (5.11e-05)	Tok/s 29266 (28184)	Loss/tok 5.8992 (7.1063)	LR 2.000e-03
0: TRAIN [0][650/1848]	Time 0.154 (0.128)	Data 7.63e-05 (5.11e-05)	Tok/s 32225 (28167)	Loss/tok 5.9798 (7.0885)	LR 2.000e-03
0: TRAIN [0][660/1848]	Time 0.153 (0.128)	Data 4.46e-05 (5.12e-05)	Tok/s 32368 (28210)	Loss/tok 6.0136 (7.0669)	LR 2.000e-03
0: TRAIN [0][670/1848]	Time 0.097 (0.128)	Data 4.43e-05 (5.13e-05)	Tok/s 22008 (28208)	Loss/tok 5.3723 (7.0480)	LR 2.000e-03
0: TRAIN [0][680/1848]	Time 0.122 (0.128)	Data 7.03e-05 (5.12e-05)	Tok/s 29413 (28193)	Loss/tok 5.4973 (7.0292)	LR 2.000e-03
0: TRAIN [0][690/1848]	Time 0.123 (0.128)	Data 4.24e-05 (5.12e-05)	Tok/s 29185 (28198)	Loss/tok 5.6754 (7.0102)	LR 2.000e-03
0: TRAIN [0][700/1848]	Time 0.156 (0.128)	Data 4.74e-05 (5.13e-05)	Tok/s 32434 (28213)	Loss/tok 5.8203 (6.9911)	LR 2.000e-03
0: TRAIN [0][710/1848]	Time 0.125 (0.128)	Data 4.22e-05 (5.12e-05)	Tok/s 28456 (28184)	Loss/tok 5.5801 (6.9750)	LR 2.000e-03
0: TRAIN [0][720/1848]	Time 0.189 (0.128)	Data 5.05e-05 (5.12e-05)	Tok/s 34441 (28186)	Loss/tok 6.0243 (6.9571)	LR 2.000e-03
0: TRAIN [0][730/1848]	Time 0.122 (0.128)	Data 4.96e-05 (5.12e-05)	Tok/s 29326 (28206)	Loss/tok 5.6233 (6.9379)	LR 2.000e-03
0: TRAIN [0][740/1848]	Time 0.192 (0.128)	Data 4.32e-05 (5.11e-05)	Tok/s 34600 (28208)	Loss/tok 5.9569 (6.9205)	LR 2.000e-03
0: TRAIN [0][750/1848]	Time 0.127 (0.128)	Data 4.41e-05 (5.12e-05)	Tok/s 28534 (28226)	Loss/tok 5.4538 (6.9012)	LR 2.000e-03
0: TRAIN [0][760/1848]	Time 0.124 (0.129)	Data 5.94e-05 (5.12e-05)	Tok/s 28783 (28254)	Loss/tok 5.4952 (6.8811)	LR 2.000e-03
0: TRAIN [0][770/1848]	Time 0.097 (0.129)	Data 4.32e-05 (5.11e-05)	Tok/s 21849 (28285)	Loss/tok 5.2063 (6.8615)	LR 2.000e-03
0: TRAIN [0][780/1848]	Time 0.151 (0.129)	Data 4.29e-05 (5.11e-05)	Tok/s 33389 (28280)	Loss/tok 5.6348 (6.8446)	LR 2.000e-03
0: TRAIN [0][790/1848]	Time 0.125 (0.129)	Data 4.77e-05 (5.11e-05)	Tok/s 28880 (28317)	Loss/tok 5.4622 (6.8242)	LR 2.000e-03
0: TRAIN [0][800/1848]	Time 0.125 (0.129)	Data 4.34e-05 (5.10e-05)	Tok/s 28621 (28320)	Loss/tok 5.3788 (6.8074)	LR 2.000e-03
0: TRAIN [0][810/1848]	Time 0.125 (0.129)	Data 4.32e-05 (5.10e-05)	Tok/s 29075 (28325)	Loss/tok 5.4292 (6.7913)	LR 2.000e-03
0: TRAIN [0][820/1848]	Time 0.096 (0.129)	Data 5.05e-05 (5.10e-05)	Tok/s 22631 (28301)	Loss/tok 5.0057 (6.7764)	LR 2.000e-03
0: TRAIN [0][830/1848]	Time 0.125 (0.129)	Data 4.39e-05 (5.09e-05)	Tok/s 28907 (28281)	Loss/tok 5.3679 (6.7621)	LR 2.000e-03
0: TRAIN [0][840/1848]	Time 0.096 (0.129)	Data 4.63e-05 (5.10e-05)	Tok/s 22853 (28297)	Loss/tok 4.9596 (6.7453)	LR 2.000e-03
0: TRAIN [0][850/1848]	Time 0.097 (0.129)	Data 7.41e-05 (5.11e-05)	Tok/s 21899 (28305)	Loss/tok 4.9000 (6.7286)	LR 2.000e-03
0: TRAIN [0][860/1848]	Time 0.094 (0.129)	Data 4.20e-05 (5.11e-05)	Tok/s 22420 (28291)	Loss/tok 4.9165 (6.7137)	LR 2.000e-03
0: TRAIN [0][870/1848]	Time 0.094 (0.129)	Data 4.58e-05 (5.12e-05)	Tok/s 22848 (28270)	Loss/tok 4.6536 (6.6994)	LR 2.000e-03
0: TRAIN [0][880/1848]	Time 0.189 (0.129)	Data 6.70e-05 (5.12e-05)	Tok/s 34688 (28267)	Loss/tok 5.6398 (6.6839)	LR 2.000e-03
0: TRAIN [0][890/1848]	Time 0.099 (0.129)	Data 4.36e-05 (5.13e-05)	Tok/s 21705 (28266)	Loss/tok 4.5970 (6.6687)	LR 2.000e-03
0: TRAIN [0][900/1848]	Time 0.153 (0.129)	Data 4.86e-05 (5.12e-05)	Tok/s 33356 (28292)	Loss/tok 5.3331 (6.6508)	LR 2.000e-03
0: TRAIN [0][910/1848]	Time 0.151 (0.129)	Data 4.22e-05 (5.12e-05)	Tok/s 33844 (28302)	Loss/tok 5.3438 (6.6349)	LR 2.000e-03
0: TRAIN [0][920/1848]	Time 0.070 (0.129)	Data 4.48e-05 (5.12e-05)	Tok/s 14934 (28314)	Loss/tok 4.5247 (6.6189)	LR 2.000e-03
0: TRAIN [0][930/1848]	Time 0.121 (0.129)	Data 4.67e-05 (5.12e-05)	Tok/s 30251 (28299)	Loss/tok 4.9400 (6.6045)	LR 2.000e-03
0: TRAIN [0][940/1848]	Time 0.097 (0.129)	Data 4.32e-05 (5.11e-05)	Tok/s 23331 (28284)	Loss/tok 4.9175 (6.5908)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][950/1848]	Time 0.093 (0.129)	Data 4.32e-05 (5.11e-05)	Tok/s 39347 (28303)	Loss/tok 4.9541 (6.5755)	LR 2.000e-03
0: TRAIN [0][960/1848]	Time 0.124 (0.129)	Data 5.67e-05 (5.11e-05)	Tok/s 29217 (28333)	Loss/tok 5.2586 (6.5584)	LR 2.000e-03
0: TRAIN [0][970/1848]	Time 0.125 (0.129)	Data 4.27e-05 (5.11e-05)	Tok/s 28994 (28338)	Loss/tok 5.1138 (6.5442)	LR 2.000e-03
0: TRAIN [0][980/1848]	Time 0.094 (0.129)	Data 4.34e-05 (5.11e-05)	Tok/s 22795 (28327)	Loss/tok 4.5799 (6.5303)	LR 2.000e-03
0: TRAIN [0][990/1848]	Time 0.126 (0.129)	Data 5.48e-05 (5.11e-05)	Tok/s 29103 (28371)	Loss/tok 5.0091 (6.5139)	LR 2.000e-03
0: TRAIN [0][1000/1848]	Time 0.153 (0.129)	Data 4.27e-05 (5.11e-05)	Tok/s 33011 (28377)	Loss/tok 5.1888 (6.4993)	LR 2.000e-03
0: TRAIN [0][1010/1848]	Time 0.152 (0.130)	Data 4.43e-05 (5.12e-05)	Tok/s 32907 (28396)	Loss/tok 5.2473 (6.4841)	LR 2.000e-03
0: TRAIN [0][1020/1848]	Time 0.070 (0.130)	Data 4.72e-05 (5.12e-05)	Tok/s 15203 (28408)	Loss/tok 4.4806 (6.4696)	LR 2.000e-03
0: TRAIN [0][1030/1848]	Time 0.150 (0.130)	Data 4.24e-05 (5.11e-05)	Tok/s 33115 (28427)	Loss/tok 5.2196 (6.4545)	LR 2.000e-03
0: TRAIN [0][1040/1848]	Time 0.153 (0.130)	Data 4.32e-05 (5.11e-05)	Tok/s 32762 (28430)	Loss/tok 5.1622 (6.4406)	LR 2.000e-03
0: TRAIN [0][1050/1848]	Time 0.097 (0.130)	Data 4.82e-05 (5.11e-05)	Tok/s 21808 (28439)	Loss/tok 4.8277 (6.4270)	LR 2.000e-03
0: TRAIN [0][1060/1848]	Time 0.156 (0.130)	Data 4.29e-05 (5.10e-05)	Tok/s 32377 (28448)	Loss/tok 4.9584 (6.4129)	LR 2.000e-03
0: TRAIN [0][1070/1848]	Time 0.125 (0.130)	Data 4.27e-05 (5.10e-05)	Tok/s 28239 (28443)	Loss/tok 4.7874 (6.4002)	LR 2.000e-03
0: TRAIN [0][1080/1848]	Time 0.126 (0.130)	Data 5.94e-05 (5.10e-05)	Tok/s 28191 (28435)	Loss/tok 4.7519 (6.3877)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1090/1848]	Time 0.122 (0.130)	Data 4.39e-05 (5.10e-05)	Tok/s 41236 (28437)	Loss/tok 5.1197 (6.3751)	LR 2.000e-03
0: TRAIN [0][1100/1848]	Time 0.151 (0.130)	Data 4.51e-05 (5.10e-05)	Tok/s 33045 (28421)	Loss/tok 5.1434 (6.3639)	LR 2.000e-03
0: TRAIN [0][1110/1848]	Time 0.123 (0.130)	Data 4.82e-05 (5.09e-05)	Tok/s 29434 (28418)	Loss/tok 4.8806 (6.3515)	LR 2.000e-03
0: TRAIN [0][1120/1848]	Time 0.065 (0.129)	Data 6.91e-05 (5.09e-05)	Tok/s 16262 (28384)	Loss/tok 4.3558 (6.3415)	LR 2.000e-03
0: TRAIN [0][1130/1848]	Time 0.190 (0.130)	Data 5.77e-05 (5.09e-05)	Tok/s 34137 (28401)	Loss/tok 5.1957 (6.3274)	LR 2.000e-03
0: TRAIN [0][1140/1848]	Time 0.156 (0.129)	Data 4.43e-05 (5.09e-05)	Tok/s 32181 (28391)	Loss/tok 5.0219 (6.3159)	LR 2.000e-03
0: TRAIN [0][1150/1848]	Time 0.189 (0.129)	Data 4.46e-05 (5.09e-05)	Tok/s 34717 (28383)	Loss/tok 5.1894 (6.3044)	LR 2.000e-03
0: TRAIN [0][1160/1848]	Time 0.124 (0.129)	Data 5.05e-05 (5.08e-05)	Tok/s 29341 (28379)	Loss/tok 4.7528 (6.2921)	LR 2.000e-03
0: TRAIN [0][1170/1848]	Time 0.125 (0.130)	Data 4.34e-05 (5.08e-05)	Tok/s 28252 (28404)	Loss/tok 4.8069 (6.2776)	LR 2.000e-03
0: TRAIN [0][1180/1848]	Time 0.152 (0.130)	Data 4.39e-05 (5.08e-05)	Tok/s 33246 (28391)	Loss/tok 4.7701 (6.2662)	LR 2.000e-03
0: TRAIN [0][1190/1848]	Time 0.124 (0.130)	Data 7.37e-05 (5.08e-05)	Tok/s 29043 (28414)	Loss/tok 4.6809 (6.2531)	LR 2.000e-03
0: TRAIN [0][1200/1848]	Time 0.155 (0.130)	Data 4.46e-05 (5.08e-05)	Tok/s 32468 (28424)	Loss/tok 4.9042 (6.2408)	LR 2.000e-03
0: TRAIN [0][1210/1848]	Time 0.158 (0.130)	Data 4.46e-05 (5.08e-05)	Tok/s 32410 (28425)	Loss/tok 4.8554 (6.2293)	LR 2.000e-03
0: TRAIN [0][1220/1848]	Time 0.187 (0.130)	Data 7.39e-05 (5.08e-05)	Tok/s 34460 (28427)	Loss/tok 5.2180 (6.2178)	LR 2.000e-03
0: TRAIN [0][1230/1848]	Time 0.097 (0.130)	Data 4.32e-05 (5.08e-05)	Tok/s 22046 (28431)	Loss/tok 4.6079 (6.2064)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1240/1848]	Time 0.099 (0.130)	Data 4.51e-05 (5.08e-05)	Tok/s 21852 (28437)	Loss/tok 4.4477 (6.1951)	LR 2.000e-03
0: TRAIN [0][1250/1848]	Time 0.095 (0.130)	Data 7.70e-05 (5.09e-05)	Tok/s 23876 (28436)	Loss/tok 4.6001 (6.1842)	LR 2.000e-03
0: TRAIN [0][1260/1848]	Time 0.094 (0.130)	Data 4.24e-05 (5.09e-05)	Tok/s 23111 (28439)	Loss/tok 4.2189 (6.1725)	LR 2.000e-03
0: TRAIN [0][1270/1848]	Time 0.126 (0.130)	Data 4.51e-05 (5.09e-05)	Tok/s 28928 (28455)	Loss/tok 4.7589 (6.1604)	LR 2.000e-03
0: TRAIN [0][1280/1848]	Time 0.124 (0.130)	Data 7.13e-05 (5.09e-05)	Tok/s 29469 (28453)	Loss/tok 4.7043 (6.1496)	LR 2.000e-03
0: TRAIN [0][1290/1848]	Time 0.097 (0.130)	Data 4.29e-05 (5.09e-05)	Tok/s 22709 (28456)	Loss/tok 4.2987 (6.1383)	LR 2.000e-03
0: TRAIN [0][1300/1848]	Time 0.095 (0.130)	Data 4.41e-05 (5.09e-05)	Tok/s 23088 (28450)	Loss/tok 4.4092 (6.1287)	LR 2.000e-03
0: TRAIN [0][1310/1848]	Time 0.123 (0.130)	Data 4.53e-05 (5.09e-05)	Tok/s 29219 (28455)	Loss/tok 4.6167 (6.1180)	LR 2.000e-03
0: TRAIN [0][1320/1848]	Time 0.126 (0.130)	Data 4.27e-05 (5.09e-05)	Tok/s 28698 (28464)	Loss/tok 4.5779 (6.1069)	LR 2.000e-03
0: TRAIN [0][1330/1848]	Time 0.190 (0.130)	Data 4.60e-05 (5.09e-05)	Tok/s 34266 (28455)	Loss/tok 5.1115 (6.0972)	LR 2.000e-03
0: TRAIN [0][1340/1848]	Time 0.152 (0.130)	Data 4.96e-05 (5.08e-05)	Tok/s 32890 (28438)	Loss/tok 4.8921 (6.0885)	LR 2.000e-03
0: TRAIN [0][1350/1848]	Time 0.126 (0.130)	Data 4.46e-05 (5.08e-05)	Tok/s 28609 (28441)	Loss/tok 4.6779 (6.0782)	LR 2.000e-03
0: TRAIN [0][1360/1848]	Time 0.154 (0.130)	Data 4.70e-05 (5.08e-05)	Tok/s 32802 (28462)	Loss/tok 4.8024 (6.0661)	LR 2.000e-03
0: TRAIN [0][1370/1848]	Time 0.127 (0.130)	Data 4.48e-05 (5.08e-05)	Tok/s 28451 (28474)	Loss/tok 4.6245 (6.0555)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1380/1848]	Time 0.157 (0.130)	Data 4.29e-05 (5.08e-05)	Tok/s 40712 (28478)	Loss/tok 5.2027 (6.0460)	LR 2.000e-03
0: TRAIN [0][1390/1848]	Time 0.153 (0.130)	Data 4.65e-05 (5.08e-05)	Tok/s 32872 (28500)	Loss/tok 4.8012 (6.0340)	LR 2.000e-03
0: TRAIN [0][1400/1848]	Time 0.155 (0.131)	Data 7.39e-05 (5.08e-05)	Tok/s 32883 (28515)	Loss/tok 4.7403 (6.0230)	LR 2.000e-03
0: TRAIN [0][1410/1848]	Time 0.152 (0.131)	Data 4.36e-05 (5.07e-05)	Tok/s 33363 (28518)	Loss/tok 4.7580 (6.0134)	LR 2.000e-03
0: TRAIN [0][1420/1848]	Time 0.155 (0.131)	Data 4.58e-05 (5.07e-05)	Tok/s 33209 (28516)	Loss/tok 4.7694 (6.0041)	LR 2.000e-03
0: TRAIN [0][1430/1848]	Time 0.153 (0.131)	Data 4.27e-05 (5.07e-05)	Tok/s 32725 (28521)	Loss/tok 4.7099 (5.9945)	LR 2.000e-03
0: TRAIN [0][1440/1848]	Time 0.155 (0.131)	Data 4.39e-05 (5.07e-05)	Tok/s 32532 (28523)	Loss/tok 4.6225 (5.9846)	LR 2.000e-03
0: TRAIN [0][1450/1848]	Time 0.097 (0.131)	Data 7.01e-05 (5.07e-05)	Tok/s 23573 (28533)	Loss/tok 4.4520 (5.9745)	LR 2.000e-03
0: TRAIN [0][1460/1848]	Time 0.123 (0.131)	Data 4.27e-05 (5.07e-05)	Tok/s 29241 (28519)	Loss/tok 4.2359 (5.9658)	LR 2.000e-03
0: TRAIN [0][1470/1848]	Time 0.189 (0.131)	Data 4.29e-05 (5.07e-05)	Tok/s 34304 (28510)	Loss/tok 4.9101 (5.9572)	LR 2.000e-03
0: TRAIN [0][1480/1848]	Time 0.192 (0.131)	Data 4.79e-05 (5.07e-05)	Tok/s 33746 (28512)	Loss/tok 4.9048 (5.9481)	LR 2.000e-03
0: TRAIN [0][1490/1848]	Time 0.126 (0.131)	Data 4.27e-05 (5.06e-05)	Tok/s 29202 (28523)	Loss/tok 4.4281 (5.9383)	LR 2.000e-03
0: TRAIN [0][1500/1848]	Time 0.097 (0.131)	Data 4.29e-05 (5.06e-05)	Tok/s 22104 (28519)	Loss/tok 4.2785 (5.9299)	LR 2.000e-03
0: TRAIN [0][1510/1848]	Time 0.097 (0.131)	Data 4.63e-05 (5.06e-05)	Tok/s 21946 (28500)	Loss/tok 4.2055 (5.9224)	LR 2.000e-03
0: TRAIN [0][1520/1848]	Time 0.122 (0.131)	Data 4.29e-05 (5.06e-05)	Tok/s 30082 (28509)	Loss/tok 4.3851 (5.9133)	LR 2.000e-03
0: TRAIN [0][1530/1848]	Time 0.155 (0.131)	Data 4.48e-05 (5.06e-05)	Tok/s 32428 (28518)	Loss/tok 4.6829 (5.9038)	LR 2.000e-03
0: TRAIN [0][1540/1848]	Time 0.190 (0.131)	Data 7.10e-05 (5.06e-05)	Tok/s 34092 (28528)	Loss/tok 4.9664 (5.8942)	LR 2.000e-03
0: TRAIN [0][1550/1848]	Time 0.123 (0.131)	Data 4.43e-05 (5.06e-05)	Tok/s 29352 (28536)	Loss/tok 4.4171 (5.8853)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1560/1848]	Time 0.123 (0.131)	Data 4.29e-05 (5.06e-05)	Tok/s 29368 (28527)	Loss/tok 4.3501 (5.8775)	LR 2.000e-03
0: TRAIN [0][1570/1848]	Time 0.097 (0.131)	Data 5.60e-05 (5.06e-05)	Tok/s 22492 (28514)	Loss/tok 4.0885 (5.8695)	LR 2.000e-03
0: TRAIN [0][1580/1848]	Time 0.152 (0.131)	Data 4.89e-05 (5.06e-05)	Tok/s 32771 (28499)	Loss/tok 4.6747 (5.8620)	LR 2.000e-03
0: TRAIN [0][1590/1848]	Time 0.123 (0.131)	Data 4.53e-05 (5.06e-05)	Tok/s 29712 (28498)	Loss/tok 4.5363 (5.8537)	LR 2.000e-03
0: TRAIN [0][1600/1848]	Time 0.123 (0.130)	Data 4.29e-05 (5.06e-05)	Tok/s 30057 (28482)	Loss/tok 4.4592 (5.8467)	LR 2.000e-03
0: TRAIN [0][1610/1848]	Time 0.189 (0.131)	Data 4.39e-05 (5.06e-05)	Tok/s 34920 (28491)	Loss/tok 4.8417 (5.8379)	LR 2.000e-03
0: TRAIN [0][1620/1848]	Time 0.153 (0.131)	Data 7.58e-05 (5.06e-05)	Tok/s 32497 (28494)	Loss/tok 4.4243 (5.8294)	LR 2.000e-03
0: TRAIN [0][1630/1848]	Time 0.152 (0.131)	Data 6.68e-05 (5.06e-05)	Tok/s 33614 (28500)	Loss/tok 4.3869 (5.8207)	LR 2.000e-03
0: TRAIN [0][1640/1848]	Time 0.126 (0.131)	Data 4.34e-05 (5.06e-05)	Tok/s 28819 (28497)	Loss/tok 4.4615 (5.8131)	LR 2.000e-03
0: TRAIN [0][1650/1848]	Time 0.154 (0.131)	Data 5.13e-05 (5.07e-05)	Tok/s 32244 (28492)	Loss/tok 4.6348 (5.8056)	LR 2.000e-03
0: TRAIN [0][1660/1848]	Time 0.121 (0.131)	Data 6.75e-05 (5.07e-05)	Tok/s 29790 (28502)	Loss/tok 4.4744 (5.7972)	LR 2.000e-03
0: TRAIN [0][1670/1848]	Time 0.096 (0.131)	Data 4.29e-05 (5.06e-05)	Tok/s 22524 (28501)	Loss/tok 4.1775 (5.7894)	LR 2.000e-03
0: TRAIN [0][1680/1848]	Time 0.123 (0.131)	Data 5.51e-05 (5.07e-05)	Tok/s 29144 (28498)	Loss/tok 4.3361 (5.7818)	LR 2.000e-03
0: TRAIN [0][1690/1848]	Time 0.126 (0.131)	Data 4.46e-05 (5.06e-05)	Tok/s 28776 (28496)	Loss/tok 4.2735 (5.7747)	LR 2.000e-03
0: TRAIN [0][1700/1848]	Time 0.150 (0.131)	Data 9.06e-05 (5.07e-05)	Tok/s 33706 (28501)	Loss/tok 4.7168 (5.7670)	LR 2.000e-03
0: TRAIN [0][1710/1848]	Time 0.126 (0.131)	Data 4.39e-05 (5.07e-05)	Tok/s 29296 (28494)	Loss/tok 4.3667 (5.7598)	LR 2.000e-03
0: TRAIN [0][1720/1848]	Time 0.125 (0.131)	Data 4.48e-05 (5.07e-05)	Tok/s 28612 (28494)	Loss/tok 4.3331 (5.7521)	LR 2.000e-03
0: TRAIN [0][1730/1848]	Time 0.154 (0.131)	Data 4.58e-05 (5.07e-05)	Tok/s 32498 (28492)	Loss/tok 4.7408 (5.7447)	LR 2.000e-03
0: TRAIN [0][1740/1848]	Time 0.094 (0.131)	Data 4.29e-05 (5.07e-05)	Tok/s 22949 (28487)	Loss/tok 4.0585 (5.7375)	LR 2.000e-03
0: TRAIN [0][1750/1848]	Time 0.096 (0.131)	Data 4.82e-05 (5.07e-05)	Tok/s 22495 (28477)	Loss/tok 4.1585 (5.7309)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1760/1848]	Time 0.123 (0.131)	Data 4.98e-05 (5.07e-05)	Tok/s 29240 (28488)	Loss/tok 4.2644 (5.7232)	LR 2.000e-03
0: TRAIN [0][1770/1848]	Time 0.096 (0.131)	Data 4.29e-05 (5.07e-05)	Tok/s 22622 (28480)	Loss/tok 3.8950 (5.7162)	LR 2.000e-03
0: TRAIN [0][1780/1848]	Time 0.099 (0.131)	Data 4.32e-05 (5.07e-05)	Tok/s 22251 (28482)	Loss/tok 4.2255 (5.7089)	LR 2.000e-03
0: TRAIN [0][1790/1848]	Time 0.153 (0.131)	Data 5.36e-05 (5.07e-05)	Tok/s 32481 (28490)	Loss/tok 4.4067 (5.7012)	LR 2.000e-03
0: TRAIN [0][1800/1848]	Time 0.123 (0.130)	Data 4.34e-05 (5.06e-05)	Tok/s 29884 (28478)	Loss/tok 4.3064 (5.6948)	LR 2.000e-03
0: TRAIN [0][1810/1848]	Time 0.123 (0.131)	Data 4.41e-05 (5.06e-05)	Tok/s 30176 (28483)	Loss/tok 4.3899 (5.6874)	LR 2.000e-03
0: TRAIN [0][1820/1848]	Time 0.155 (0.131)	Data 7.41e-05 (5.06e-05)	Tok/s 32533 (28490)	Loss/tok 4.5868 (5.6799)	LR 2.000e-03
0: TRAIN [0][1830/1848]	Time 0.067 (0.131)	Data 4.32e-05 (5.06e-05)	Tok/s 15782 (28476)	Loss/tok 3.8231 (5.6738)	LR 2.000e-03
0: TRAIN [0][1840/1848]	Time 0.126 (0.131)	Data 4.48e-05 (5.06e-05)	Tok/s 28883 (28489)	Loss/tok 4.3847 (5.6658)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/213]	Time 0.069 (0.000)	Data 9.51e-04 (0.00e+00)	Tok/s 64186 (0)	Loss/tok 6.0403 (6.0403)
0: VALIDATION [0][10/213]	Time 0.036 (0.040)	Data 7.82e-04 (7.83e-04)	Tok/s 76145 (76291)	Loss/tok 5.7067 (5.7784)
0: VALIDATION [0][20/213]	Time 0.030 (0.037)	Data 7.61e-04 (7.75e-04)	Tok/s 77465 (76418)	Loss/tok 5.7704 (5.6952)
0: VALIDATION [0][30/213]	Time 0.028 (0.034)	Data 7.60e-04 (7.70e-04)	Tok/s 76032 (76783)	Loss/tok 5.2775 (5.6564)
0: VALIDATION [0][40/213]	Time 0.027 (0.032)	Data 7.47e-04 (7.65e-04)	Tok/s 73844 (76511)	Loss/tok 5.6642 (5.6288)
0: VALIDATION [0][50/213]	Time 0.024 (0.031)	Data 7.34e-04 (7.62e-04)	Tok/s 76771 (76564)	Loss/tok 5.9632 (5.6075)
0: VALIDATION [0][60/213]	Time 0.022 (0.029)	Data 7.41e-04 (7.59e-04)	Tok/s 75455 (76520)	Loss/tok 5.1005 (5.5721)
0: VALIDATION [0][70/213]	Time 0.020 (0.028)	Data 7.58e-04 (7.59e-04)	Tok/s 78019 (76411)	Loss/tok 5.3605 (5.5443)
0: VALIDATION [0][80/213]	Time 0.020 (0.027)	Data 7.43e-04 (7.57e-04)	Tok/s 73539 (76242)	Loss/tok 5.3381 (5.5229)
0: VALIDATION [0][90/213]	Time 0.020 (0.026)	Data 7.37e-04 (7.55e-04)	Tok/s 70111 (75929)	Loss/tok 5.4546 (5.5063)
0: VALIDATION [0][100/213]	Time 0.019 (0.025)	Data 7.34e-04 (7.53e-04)	Tok/s 67817 (75603)	Loss/tok 4.9718 (5.4839)
0: VALIDATION [0][110/213]	Time 0.016 (0.025)	Data 7.49e-04 (7.51e-04)	Tok/s 73622 (75354)	Loss/tok 5.1617 (5.4710)
0: VALIDATION [0][120/213]	Time 0.015 (0.024)	Data 7.51e-04 (7.50e-04)	Tok/s 72572 (75099)	Loss/tok 5.1065 (5.4550)
0: VALIDATION [0][130/213]	Time 0.015 (0.023)	Data 7.43e-04 (7.49e-04)	Tok/s 69213 (74704)	Loss/tok 5.2141 (5.4415)
0: VALIDATION [0][140/213]	Time 0.014 (0.023)	Data 7.28e-04 (7.48e-04)	Tok/s 68443 (74224)	Loss/tok 4.9109 (5.4276)
0: VALIDATION [0][150/213]	Time 0.013 (0.022)	Data 7.29e-04 (7.47e-04)	Tok/s 69093 (73896)	Loss/tok 5.3556 (5.4174)
0: VALIDATION [0][160/213]	Time 0.012 (0.021)	Data 7.35e-04 (7.46e-04)	Tok/s 66398 (73516)	Loss/tok 5.0569 (5.4049)
0: VALIDATION [0][170/213]	Time 0.012 (0.021)	Data 7.27e-04 (7.45e-04)	Tok/s 60251 (73041)	Loss/tok 4.6844 (5.3924)
0: VALIDATION [0][180/213]	Time 0.010 (0.020)	Data 7.23e-04 (7.44e-04)	Tok/s 64493 (72613)	Loss/tok 5.2516 (5.3853)
0: VALIDATION [0][190/213]	Time 0.010 (0.020)	Data 7.24e-04 (7.43e-04)	Tok/s 59075 (72037)	Loss/tok 5.1331 (5.3734)
0: VALIDATION [0][200/213]	Time 0.008 (0.019)	Data 7.18e-04 (7.42e-04)	Tok/s 57871 (71374)	Loss/tok 4.6555 (5.3591)
0: VALIDATION [0][210/213]	Time 0.007 (0.019)	Data 7.17e-04 (7.41e-04)	Tok/s 48154 (70553)	Loss/tok 4.4670 (5.3473)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/126]	Time 0.3344 (0.3625)	Decoder iters 149.0 (149.0)	Tok/s 7811 (8583)
0: TEST [0][19/126]	Time 0.3105 (0.3244)	Decoder iters 149.0 (139.7)	Tok/s 7076 (8514)
0: TEST [0][29/126]	Time 0.3043 (0.3030)	Decoder iters 149.0 (133.6)	Tok/s 6520 (8454)
0: TEST [0][39/126]	Time 0.1422 (0.2865)	Decoder iters 59.0 (128.5)	Tok/s 11917 (8425)
0: TEST [0][49/126]	Time 0.1358 (0.2736)	Decoder iters 55.0 (124.3)	Tok/s 11085 (8357)
0: TEST [0][59/126]	Time 0.2809 (0.2570)	Decoder iters 149.0 (117.2)	Tok/s 5173 (8483)
0: TEST [0][69/126]	Time 0.1256 (0.2441)	Decoder iters 57.0 (111.9)	Tok/s 9493 (8536)
0: TEST [0][79/126]	Time 0.2762 (0.2339)	Decoder iters 149.0 (107.9)	Tok/s 3841 (8521)
0: TEST [0][89/126]	Time 0.0937 (0.2204)	Decoder iters 42.0 (101.7)	Tok/s 10332 (8708)
0: TEST [0][99/126]	Time 0.0707 (0.2108)	Decoder iters 28.0 (97.6)	Tok/s 11472 (8744)
0: TEST [0][109/126]	Time 0.0777 (0.2002)	Decoder iters 35.0 (92.8)	Tok/s 9063 (8836)
0: TEST [0][119/126]	Time 0.0579 (0.1885)	Decoder iters 26.0 (87.2)	Tok/s 8836 (8955)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 5.6604	Validation Loss: 5.3458	Test BLEU: 5.25
0: Performance: Epoch: 0	Training: 28498 Tok/s	Validation: 70233 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3588440356
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/1848]	Time 0.159 (0.000)	Data 9.59e-02 (0.00e+00)	Tok/s 13107 (0)	Loss/tok 3.6146 (3.6146)	LR 2.000e-03
0: TRAIN [1][10/1848]	Time 0.095 (0.113)	Data 4.96e-05 (5.99e-05)	Tok/s 22042 (26371)	Loss/tok 3.6874 (4.0079)	LR 2.000e-03
0: TRAIN [1][20/1848]	Time 0.155 (0.130)	Data 5.34e-05 (5.91e-05)	Tok/s 32171 (28448)	Loss/tok 4.1295 (4.1266)	LR 2.000e-03
0: TRAIN [1][30/1848]	Time 0.125 (0.124)	Data 7.53e-05 (5.96e-05)	Tok/s 28819 (27830)	Loss/tok 3.9898 (4.0635)	LR 2.000e-03
0: TRAIN [1][40/1848]	Time 0.154 (0.122)	Data 8.08e-05 (6.13e-05)	Tok/s 32555 (27510)	Loss/tok 4.2002 (4.0402)	LR 2.000e-03
0: TRAIN [1][50/1848]	Time 0.123 (0.126)	Data 9.97e-05 (6.04e-05)	Tok/s 29310 (27988)	Loss/tok 3.9195 (4.0895)	LR 2.000e-03
0: TRAIN [1][60/1848]	Time 0.097 (0.127)	Data 4.39e-05 (5.91e-05)	Tok/s 22315 (28142)	Loss/tok 3.5312 (4.0913)	LR 2.000e-03
0: TRAIN [1][70/1848]	Time 0.186 (0.127)	Data 4.84e-05 (5.79e-05)	Tok/s 35175 (28194)	Loss/tok 4.3611 (4.0957)	LR 2.000e-03
0: TRAIN [1][80/1848]	Time 0.094 (0.125)	Data 4.41e-05 (5.62e-05)	Tok/s 22484 (27705)	Loss/tok 3.8160 (4.0835)	LR 2.000e-03
0: TRAIN [1][90/1848]	Time 0.125 (0.125)	Data 4.55e-05 (5.60e-05)	Tok/s 28941 (27791)	Loss/tok 3.9163 (4.0746)	LR 2.000e-03
0: TRAIN [1][100/1848]	Time 0.120 (0.124)	Data 5.58e-05 (5.53e-05)	Tok/s 30095 (27677)	Loss/tok 4.1433 (4.0752)	LR 2.000e-03
0: TRAIN [1][110/1848]	Time 0.097 (0.126)	Data 4.51e-05 (5.44e-05)	Tok/s 21822 (27896)	Loss/tok 3.6634 (4.0979)	LR 2.000e-03
0: TRAIN [1][120/1848]	Time 0.154 (0.128)	Data 4.55e-05 (5.42e-05)	Tok/s 32960 (28146)	Loss/tok 4.1518 (4.1126)	LR 2.000e-03
0: TRAIN [1][130/1848]	Time 0.126 (0.129)	Data 5.10e-05 (5.40e-05)	Tok/s 29080 (28301)	Loss/tok 4.0484 (4.1160)	LR 2.000e-03
0: TRAIN [1][140/1848]	Time 0.096 (0.127)	Data 4.24e-05 (5.33e-05)	Tok/s 23115 (27987)	Loss/tok 3.8910 (4.1049)	LR 2.000e-03
0: TRAIN [1][150/1848]	Time 0.099 (0.127)	Data 4.48e-05 (5.32e-05)	Tok/s 22149 (27917)	Loss/tok 3.6854 (4.1076)	LR 2.000e-03
0: TRAIN [1][160/1848]	Time 0.120 (0.126)	Data 7.32e-05 (5.29e-05)	Tok/s 30142 (27837)	Loss/tok 3.7529 (4.1017)	LR 2.000e-03
0: TRAIN [1][170/1848]	Time 0.151 (0.127)	Data 4.60e-05 (5.32e-05)	Tok/s 32923 (27926)	Loss/tok 4.2315 (4.1134)	LR 2.000e-03
0: TRAIN [1][180/1848]	Time 0.152 (0.127)	Data 7.77e-05 (5.31e-05)	Tok/s 33122 (28005)	Loss/tok 4.1519 (4.1132)	LR 2.000e-03
0: TRAIN [1][190/1848]	Time 0.070 (0.128)	Data 4.41e-05 (5.27e-05)	Tok/s 15587 (28006)	Loss/tok 3.3069 (4.1136)	LR 2.000e-03
0: TRAIN [1][200/1848]	Time 0.188 (0.127)	Data 4.36e-05 (5.26e-05)	Tok/s 34602 (27945)	Loss/tok 4.4289 (4.1107)	LR 2.000e-03
0: TRAIN [1][210/1848]	Time 0.070 (0.128)	Data 5.20e-05 (5.25e-05)	Tok/s 15247 (28010)	Loss/tok 3.3693 (4.1147)	LR 2.000e-03
0: TRAIN [1][220/1848]	Time 0.151 (0.127)	Data 4.27e-05 (5.21e-05)	Tok/s 33103 (27982)	Loss/tok 4.1553 (4.1100)	LR 2.000e-03
0: TRAIN [1][230/1848]	Time 0.151 (0.127)	Data 4.53e-05 (5.19e-05)	Tok/s 33275 (27900)	Loss/tok 4.0689 (4.1065)	LR 2.000e-03
0: TRAIN [1][240/1848]	Time 0.125 (0.128)	Data 8.13e-05 (5.17e-05)	Tok/s 28589 (28016)	Loss/tok 3.8670 (4.1071)	LR 2.000e-03
0: TRAIN [1][250/1848]	Time 0.093 (0.126)	Data 4.46e-05 (5.15e-05)	Tok/s 23579 (27862)	Loss/tok 3.6533 (4.0980)	LR 2.000e-03
0: TRAIN [1][260/1848]	Time 0.098 (0.126)	Data 5.36e-05 (5.14e-05)	Tok/s 21811 (27858)	Loss/tok 3.5763 (4.0972)	LR 2.000e-03
0: TRAIN [1][270/1848]	Time 0.154 (0.128)	Data 4.27e-05 (5.13e-05)	Tok/s 33525 (28019)	Loss/tok 4.2023 (4.1053)	LR 2.000e-03
0: TRAIN [1][280/1848]	Time 0.188 (0.128)	Data 4.32e-05 (5.13e-05)	Tok/s 34801 (28035)	Loss/tok 4.2874 (4.1026)	LR 2.000e-03
0: TRAIN [1][290/1848]	Time 0.097 (0.128)	Data 4.86e-05 (5.13e-05)	Tok/s 22558 (28041)	Loss/tok 3.7648 (4.1034)	LR 2.000e-03
0: TRAIN [1][300/1848]	Time 0.096 (0.128)	Data 4.39e-05 (5.12e-05)	Tok/s 22937 (28019)	Loss/tok 3.7734 (4.1006)	LR 2.000e-03
0: TRAIN [1][310/1848]	Time 0.097 (0.128)	Data 4.55e-05 (5.14e-05)	Tok/s 22815 (27990)	Loss/tok 3.6993 (4.0998)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][320/1848]	Time 0.156 (0.128)	Data 5.87e-05 (5.13e-05)	Tok/s 32303 (28084)	Loss/tok 4.3890 (4.1043)	LR 2.000e-03
0: TRAIN [1][330/1848]	Time 0.122 (0.128)	Data 4.48e-05 (5.12e-05)	Tok/s 29645 (28129)	Loss/tok 3.8267 (4.1054)	LR 2.000e-03
0: TRAIN [1][340/1848]	Time 0.098 (0.128)	Data 4.43e-05 (5.11e-05)	Tok/s 21849 (28076)	Loss/tok 3.5236 (4.1025)	LR 2.000e-03
0: TRAIN [1][350/1848]	Time 0.126 (0.128)	Data 4.46e-05 (5.10e-05)	Tok/s 28249 (28104)	Loss/tok 3.9220 (4.1013)	LR 2.000e-03
0: TRAIN [1][360/1848]	Time 0.124 (0.128)	Data 6.82e-05 (5.11e-05)	Tok/s 29822 (28101)	Loss/tok 3.9688 (4.1004)	LR 2.000e-03
0: TRAIN [1][370/1848]	Time 0.152 (0.128)	Data 4.84e-05 (5.12e-05)	Tok/s 32733 (28064)	Loss/tok 4.1032 (4.0964)	LR 2.000e-03
0: TRAIN [1][380/1848]	Time 0.152 (0.128)	Data 4.34e-05 (5.11e-05)	Tok/s 33311 (28143)	Loss/tok 4.1003 (4.0978)	LR 2.000e-03
0: TRAIN [1][390/1848]	Time 0.125 (0.128)	Data 4.43e-05 (5.11e-05)	Tok/s 29043 (28090)	Loss/tok 4.0076 (4.0972)	LR 2.000e-03
0: TRAIN [1][400/1848]	Time 0.097 (0.128)	Data 5.36e-05 (5.11e-05)	Tok/s 22570 (28129)	Loss/tok 3.7781 (4.0980)	LR 2.000e-03
0: TRAIN [1][410/1848]	Time 0.122 (0.128)	Data 4.34e-05 (5.10e-05)	Tok/s 29240 (28129)	Loss/tok 3.9123 (4.0981)	LR 2.000e-03
0: TRAIN [1][420/1848]	Time 0.125 (0.128)	Data 4.48e-05 (5.10e-05)	Tok/s 28943 (28137)	Loss/tok 3.9973 (4.0967)	LR 2.000e-03
0: TRAIN [1][430/1848]	Time 0.123 (0.128)	Data 5.91e-05 (5.09e-05)	Tok/s 29392 (28109)	Loss/tok 3.9224 (4.0981)	LR 2.000e-03
0: TRAIN [1][440/1848]	Time 0.125 (0.128)	Data 4.51e-05 (5.09e-05)	Tok/s 28773 (28089)	Loss/tok 3.9716 (4.0944)	LR 2.000e-03
0: TRAIN [1][450/1848]	Time 0.097 (0.128)	Data 4.58e-05 (5.08e-05)	Tok/s 23296 (28033)	Loss/tok 3.6936 (4.0942)	LR 2.000e-03
0: TRAIN [1][460/1848]	Time 0.190 (0.128)	Data 4.43e-05 (5.07e-05)	Tok/s 34882 (28103)	Loss/tok 4.3213 (4.0945)	LR 2.000e-03
0: TRAIN [1][470/1848]	Time 0.190 (0.128)	Data 4.39e-05 (5.07e-05)	Tok/s 34314 (28160)	Loss/tok 4.2765 (4.0939)	LR 2.000e-03
0: TRAIN [1][480/1848]	Time 0.121 (0.128)	Data 4.60e-05 (5.07e-05)	Tok/s 29280 (28143)	Loss/tok 3.9535 (4.0900)	LR 2.000e-03
0: TRAIN [1][490/1848]	Time 0.153 (0.129)	Data 4.46e-05 (5.06e-05)	Tok/s 33214 (28201)	Loss/tok 4.0398 (4.0904)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][500/1848]	Time 0.151 (0.128)	Data 6.89e-05 (5.08e-05)	Tok/s 33313 (28196)	Loss/tok 4.0971 (4.0881)	LR 2.000e-03
0: TRAIN [1][510/1848]	Time 0.126 (0.128)	Data 5.79e-05 (5.08e-05)	Tok/s 28219 (28169)	Loss/tok 3.9680 (4.0858)	LR 2.000e-03
0: TRAIN [1][520/1848]	Time 0.186 (0.128)	Data 6.79e-05 (5.07e-05)	Tok/s 34856 (28188)	Loss/tok 4.2841 (4.0849)	LR 2.000e-03
0: TRAIN [1][530/1848]	Time 0.067 (0.128)	Data 4.41e-05 (5.08e-05)	Tok/s 15945 (28174)	Loss/tok 3.3661 (4.0831)	LR 2.000e-03
0: TRAIN [1][540/1848]	Time 0.155 (0.128)	Data 6.01e-05 (5.07e-05)	Tok/s 32098 (28192)	Loss/tok 4.1957 (4.0846)	LR 2.000e-03
0: TRAIN [1][550/1848]	Time 0.125 (0.129)	Data 4.48e-05 (5.07e-05)	Tok/s 28195 (28232)	Loss/tok 3.7857 (4.0856)	LR 2.000e-03
0: TRAIN [1][560/1848]	Time 0.123 (0.129)	Data 4.29e-05 (5.08e-05)	Tok/s 29098 (28238)	Loss/tok 4.0706 (4.0845)	LR 2.000e-03
0: TRAIN [1][570/1848]	Time 0.095 (0.129)	Data 5.96e-05 (5.08e-05)	Tok/s 22791 (28217)	Loss/tok 3.8828 (4.0836)	LR 2.000e-03
0: TRAIN [1][580/1848]	Time 0.122 (0.129)	Data 4.41e-05 (5.08e-05)	Tok/s 29295 (28218)	Loss/tok 3.9613 (4.0832)	LR 2.000e-03
0: TRAIN [1][590/1848]	Time 0.099 (0.129)	Data 4.67e-05 (5.08e-05)	Tok/s 22954 (28182)	Loss/tok 3.6263 (4.0814)	LR 2.000e-03
0: TRAIN [1][600/1848]	Time 0.125 (0.128)	Data 4.27e-05 (5.07e-05)	Tok/s 28963 (28161)	Loss/tok 4.0806 (4.0789)	LR 2.000e-03
0: TRAIN [1][610/1848]	Time 0.094 (0.128)	Data 4.53e-05 (5.08e-05)	Tok/s 23614 (28152)	Loss/tok 3.8663 (4.0788)	LR 2.000e-03
0: TRAIN [1][620/1848]	Time 0.070 (0.128)	Data 4.98e-05 (5.07e-05)	Tok/s 15215 (28164)	Loss/tok 3.4844 (4.0800)	LR 1.000e-03
0: TRAIN [1][630/1848]	Time 0.125 (0.128)	Data 4.29e-05 (5.06e-05)	Tok/s 28806 (28094)	Loss/tok 4.0569 (4.0775)	LR 1.000e-03
0: TRAIN [1][640/1848]	Time 0.125 (0.128)	Data 4.82e-05 (5.06e-05)	Tok/s 29345 (28107)	Loss/tok 3.8817 (4.0773)	LR 1.000e-03
0: TRAIN [1][650/1848]	Time 0.123 (0.128)	Data 4.72e-05 (5.05e-05)	Tok/s 29680 (28123)	Loss/tok 3.8068 (4.0757)	LR 1.000e-03
0: TRAIN [1][660/1848]	Time 0.125 (0.128)	Data 4.43e-05 (5.06e-05)	Tok/s 28959 (28144)	Loss/tok 3.8980 (4.0753)	LR 1.000e-03
0: TRAIN [1][670/1848]	Time 0.155 (0.128)	Data 4.77e-05 (5.06e-05)	Tok/s 32903 (28161)	Loss/tok 4.0581 (4.0741)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][680/1848]	Time 0.097 (0.128)	Data 4.65e-05 (5.05e-05)	Tok/s 21838 (28174)	Loss/tok 3.6842 (4.0725)	LR 1.000e-03
0: TRAIN [1][690/1848]	Time 0.155 (0.128)	Data 4.48e-05 (5.06e-05)	Tok/s 32792 (28170)	Loss/tok 4.0229 (4.0699)	LR 1.000e-03
0: TRAIN [1][700/1848]	Time 0.097 (0.128)	Data 5.79e-05 (5.07e-05)	Tok/s 22772 (28143)	Loss/tok 3.5494 (4.0671)	LR 1.000e-03
0: TRAIN [1][710/1848]	Time 0.155 (0.128)	Data 4.32e-05 (5.06e-05)	Tok/s 32593 (28161)	Loss/tok 4.1938 (4.0658)	LR 1.000e-03
0: TRAIN [1][720/1848]	Time 0.125 (0.128)	Data 4.46e-05 (5.06e-05)	Tok/s 28409 (28183)	Loss/tok 3.8825 (4.0642)	LR 1.000e-03
0: TRAIN [1][730/1848]	Time 0.094 (0.128)	Data 5.91e-05 (5.06e-05)	Tok/s 23490 (28178)	Loss/tok 3.5138 (4.0617)	LR 1.000e-03
0: TRAIN [1][740/1848]	Time 0.120 (0.128)	Data 7.13e-05 (5.05e-05)	Tok/s 30107 (28179)	Loss/tok 3.7687 (4.0589)	LR 1.000e-03
0: TRAIN [1][750/1848]	Time 0.094 (0.128)	Data 4.84e-05 (5.05e-05)	Tok/s 23427 (28161)	Loss/tok 3.6229 (4.0562)	LR 1.000e-03
0: TRAIN [1][760/1848]	Time 0.094 (0.128)	Data 4.63e-05 (5.06e-05)	Tok/s 22717 (28117)	Loss/tok 3.6023 (4.0535)	LR 1.000e-03
0: TRAIN [1][770/1848]	Time 0.190 (0.128)	Data 4.65e-05 (5.07e-05)	Tok/s 34358 (28142)	Loss/tok 4.3383 (4.0524)	LR 1.000e-03
0: TRAIN [1][780/1848]	Time 0.154 (0.128)	Data 7.49e-05 (5.07e-05)	Tok/s 32926 (28138)	Loss/tok 3.9907 (4.0495)	LR 1.000e-03
0: TRAIN [1][790/1848]	Time 0.094 (0.128)	Data 4.55e-05 (5.07e-05)	Tok/s 23141 (28148)	Loss/tok 3.5010 (4.0504)	LR 1.000e-03
0: TRAIN [1][800/1848]	Time 0.155 (0.128)	Data 4.58e-05 (5.07e-05)	Tok/s 33179 (28156)	Loss/tok 4.0261 (4.0485)	LR 1.000e-03
0: TRAIN [1][810/1848]	Time 0.153 (0.128)	Data 9.89e-05 (5.07e-05)	Tok/s 32989 (28165)	Loss/tok 4.0693 (4.0483)	LR 1.000e-03
0: TRAIN [1][820/1848]	Time 0.095 (0.128)	Data 4.43e-05 (5.06e-05)	Tok/s 22523 (28162)	Loss/tok 3.5170 (4.0458)	LR 1.000e-03
0: TRAIN [1][830/1848]	Time 0.126 (0.128)	Data 4.79e-05 (5.07e-05)	Tok/s 29200 (28141)	Loss/tok 3.7234 (4.0432)	LR 1.000e-03
0: TRAIN [1][840/1848]	Time 0.096 (0.128)	Data 4.63e-05 (5.07e-05)	Tok/s 22745 (28140)	Loss/tok 3.4690 (4.0424)	LR 1.000e-03
0: TRAIN [1][850/1848]	Time 0.122 (0.128)	Data 4.34e-05 (5.08e-05)	Tok/s 29299 (28106)	Loss/tok 3.6710 (4.0398)	LR 1.000e-03
0: TRAIN [1][860/1848]	Time 0.155 (0.128)	Data 5.53e-05 (5.07e-05)	Tok/s 33029 (28135)	Loss/tok 4.0245 (4.0381)	LR 1.000e-03
0: TRAIN [1][870/1848]	Time 0.125 (0.128)	Data 4.70e-05 (5.07e-05)	Tok/s 28832 (28167)	Loss/tok 3.8210 (4.0375)	LR 1.000e-03
0: TRAIN [1][880/1848]	Time 0.125 (0.128)	Data 4.36e-05 (5.07e-05)	Tok/s 29499 (28174)	Loss/tok 3.6463 (4.0348)	LR 1.000e-03
0: TRAIN [1][890/1848]	Time 0.125 (0.128)	Data 5.05e-05 (5.08e-05)	Tok/s 28877 (28194)	Loss/tok 3.8266 (4.0331)	LR 1.000e-03
0: TRAIN [1][900/1848]	Time 0.154 (0.128)	Data 4.48e-05 (5.07e-05)	Tok/s 33045 (28185)	Loss/tok 4.1098 (4.0306)	LR 1.000e-03
0: TRAIN [1][910/1848]	Time 0.123 (0.128)	Data 4.41e-05 (5.07e-05)	Tok/s 29304 (28199)	Loss/tok 3.6999 (4.0286)	LR 1.000e-03
0: TRAIN [1][920/1848]	Time 0.097 (0.128)	Data 5.77e-05 (5.07e-05)	Tok/s 22292 (28212)	Loss/tok 3.6103 (4.0269)	LR 5.000e-04
0: TRAIN [1][930/1848]	Time 0.123 (0.129)	Data 4.63e-05 (5.07e-05)	Tok/s 30172 (28215)	Loss/tok 3.8032 (4.0263)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][940/1848]	Time 0.191 (0.129)	Data 4.27e-05 (5.07e-05)	Tok/s 33949 (28261)	Loss/tok 4.2247 (4.0255)	LR 5.000e-04
0: TRAIN [1][950/1848]	Time 0.097 (0.129)	Data 4.84e-05 (5.07e-05)	Tok/s 23046 (28281)	Loss/tok 3.3481 (4.0239)	LR 5.000e-04
0: TRAIN [1][960/1848]	Time 0.153 (0.129)	Data 4.46e-05 (5.06e-05)	Tok/s 32863 (28264)	Loss/tok 3.7034 (4.0214)	LR 5.000e-04
0: TRAIN [1][970/1848]	Time 0.155 (0.129)	Data 4.43e-05 (5.07e-05)	Tok/s 32883 (28260)	Loss/tok 3.9358 (4.0193)	LR 5.000e-04
0: TRAIN [1][980/1848]	Time 0.184 (0.129)	Data 5.10e-05 (5.07e-05)	Tok/s 35122 (28278)	Loss/tok 4.0321 (4.0188)	LR 5.000e-04
0: TRAIN [1][990/1848]	Time 0.156 (0.129)	Data 4.65e-05 (5.07e-05)	Tok/s 32288 (28277)	Loss/tok 3.8732 (4.0162)	LR 5.000e-04
0: TRAIN [1][1000/1848]	Time 0.126 (0.129)	Data 4.39e-05 (5.07e-05)	Tok/s 28441 (28274)	Loss/tok 3.7352 (4.0139)	LR 5.000e-04
0: TRAIN [1][1010/1848]	Time 0.097 (0.129)	Data 5.65e-05 (5.07e-05)	Tok/s 21693 (28257)	Loss/tok 3.4099 (4.0128)	LR 5.000e-04
0: TRAIN [1][1020/1848]	Time 0.125 (0.129)	Data 4.70e-05 (5.06e-05)	Tok/s 28053 (28277)	Loss/tok 3.6992 (4.0112)	LR 5.000e-04
0: TRAIN [1][1030/1848]	Time 0.155 (0.129)	Data 4.36e-05 (5.06e-05)	Tok/s 32837 (28301)	Loss/tok 3.9922 (4.0099)	LR 5.000e-04
0: TRAIN [1][1040/1848]	Time 0.153 (0.129)	Data 5.05e-05 (5.07e-05)	Tok/s 32769 (28306)	Loss/tok 3.9050 (4.0085)	LR 5.000e-04
0: TRAIN [1][1050/1848]	Time 0.190 (0.130)	Data 4.58e-05 (5.06e-05)	Tok/s 34580 (28330)	Loss/tok 4.0527 (4.0079)	LR 5.000e-04
0: TRAIN [1][1060/1848]	Time 0.123 (0.130)	Data 4.36e-05 (5.06e-05)	Tok/s 29137 (28335)	Loss/tok 3.5799 (4.0060)	LR 5.000e-04
0: TRAIN [1][1070/1848]	Time 0.155 (0.130)	Data 4.86e-05 (5.06e-05)	Tok/s 32041 (28355)	Loss/tok 4.0963 (4.0049)	LR 5.000e-04
0: TRAIN [1][1080/1848]	Time 0.125 (0.130)	Data 4.51e-05 (5.06e-05)	Tok/s 28262 (28367)	Loss/tok 3.8166 (4.0033)	LR 5.000e-04
0: TRAIN [1][1090/1848]	Time 0.155 (0.130)	Data 4.39e-05 (5.06e-05)	Tok/s 32811 (28371)	Loss/tok 3.8595 (4.0014)	LR 5.000e-04
0: TRAIN [1][1100/1848]	Time 0.121 (0.130)	Data 7.61e-05 (5.06e-05)	Tok/s 29640 (28352)	Loss/tok 3.5962 (3.9986)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1110/1848]	Time 0.098 (0.130)	Data 4.29e-05 (5.06e-05)	Tok/s 21559 (28378)	Loss/tok 3.4790 (3.9968)	LR 5.000e-04
0: TRAIN [1][1120/1848]	Time 0.098 (0.130)	Data 4.32e-05 (5.07e-05)	Tok/s 21899 (28354)	Loss/tok 3.4324 (3.9945)	LR 5.000e-04
0: TRAIN [1][1130/1848]	Time 0.125 (0.129)	Data 8.42e-05 (5.06e-05)	Tok/s 29458 (28348)	Loss/tok 3.5761 (3.9921)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1140/1848]	Time 0.157 (0.129)	Data 4.39e-05 (5.06e-05)	Tok/s 32317 (28361)	Loss/tok 3.9221 (3.9900)	LR 5.000e-04
0: TRAIN [1][1150/1848]	Time 0.097 (0.129)	Data 4.86e-05 (5.07e-05)	Tok/s 22711 (28366)	Loss/tok 3.4165 (3.9887)	LR 5.000e-04
0: TRAIN [1][1160/1848]	Time 0.155 (0.129)	Data 4.29e-05 (5.06e-05)	Tok/s 32672 (28367)	Loss/tok 4.0063 (3.9872)	LR 5.000e-04
0: TRAIN [1][1170/1848]	Time 0.096 (0.130)	Data 4.34e-05 (5.06e-05)	Tok/s 22750 (28380)	Loss/tok 3.5553 (3.9863)	LR 5.000e-04
0: TRAIN [1][1180/1848]	Time 0.126 (0.130)	Data 4.86e-05 (5.06e-05)	Tok/s 28921 (28384)	Loss/tok 3.6672 (3.9845)	LR 5.000e-04
0: TRAIN [1][1190/1848]	Time 0.189 (0.130)	Data 4.36e-05 (5.06e-05)	Tok/s 34663 (28384)	Loss/tok 3.9510 (3.9824)	LR 5.000e-04
0: TRAIN [1][1200/1848]	Time 0.097 (0.129)	Data 4.34e-05 (5.06e-05)	Tok/s 23153 (28364)	Loss/tok 3.4221 (3.9801)	LR 5.000e-04
0: TRAIN [1][1210/1848]	Time 0.097 (0.129)	Data 6.13e-05 (5.06e-05)	Tok/s 22545 (28371)	Loss/tok 3.3344 (3.9778)	LR 5.000e-04
0: TRAIN [1][1220/1848]	Time 0.096 (0.129)	Data 4.63e-05 (5.05e-05)	Tok/s 21906 (28363)	Loss/tok 3.5852 (3.9760)	LR 5.000e-04
0: TRAIN [1][1230/1848]	Time 0.096 (0.129)	Data 4.70e-05 (5.06e-05)	Tok/s 23362 (28361)	Loss/tok 3.2727 (3.9740)	LR 2.500e-04
0: TRAIN [1][1240/1848]	Time 0.126 (0.129)	Data 4.34e-05 (5.05e-05)	Tok/s 28699 (28371)	Loss/tok 3.7046 (3.9729)	LR 2.500e-04
0: TRAIN [1][1250/1848]	Time 0.099 (0.130)	Data 4.58e-05 (5.05e-05)	Tok/s 22019 (28384)	Loss/tok 3.3724 (3.9721)	LR 2.500e-04
0: TRAIN [1][1260/1848]	Time 0.097 (0.130)	Data 4.34e-05 (5.05e-05)	Tok/s 22411 (28393)	Loss/tok 3.4198 (3.9715)	LR 2.500e-04
0: TRAIN [1][1270/1848]	Time 0.191 (0.130)	Data 7.84e-05 (5.05e-05)	Tok/s 34482 (28391)	Loss/tok 3.9959 (3.9702)	LR 2.500e-04
0: TRAIN [1][1280/1848]	Time 0.122 (0.130)	Data 4.46e-05 (5.06e-05)	Tok/s 29105 (28389)	Loss/tok 3.6665 (3.9683)	LR 2.500e-04
0: TRAIN [1][1290/1848]	Time 0.070 (0.130)	Data 4.39e-05 (5.06e-05)	Tok/s 15090 (28395)	Loss/tok 3.2819 (3.9668)	LR 2.500e-04
0: TRAIN [1][1300/1848]	Time 0.123 (0.130)	Data 7.01e-05 (5.06e-05)	Tok/s 28903 (28411)	Loss/tok 3.5813 (3.9653)	LR 2.500e-04
0: TRAIN [1][1310/1848]	Time 0.157 (0.130)	Data 4.46e-05 (5.05e-05)	Tok/s 32690 (28432)	Loss/tok 3.8464 (3.9635)	LR 2.500e-04
0: TRAIN [1][1320/1848]	Time 0.121 (0.130)	Data 4.24e-05 (5.05e-05)	Tok/s 28555 (28432)	Loss/tok 3.7069 (3.9616)	LR 2.500e-04
0: TRAIN [1][1330/1848]	Time 0.124 (0.130)	Data 4.51e-05 (5.05e-05)	Tok/s 29192 (28414)	Loss/tok 3.5745 (3.9600)	LR 2.500e-04
0: TRAIN [1][1340/1848]	Time 0.152 (0.130)	Data 4.46e-05 (5.05e-05)	Tok/s 32813 (28425)	Loss/tok 3.8649 (3.9588)	LR 2.500e-04
0: TRAIN [1][1350/1848]	Time 0.126 (0.130)	Data 4.43e-05 (5.05e-05)	Tok/s 28429 (28436)	Loss/tok 3.6860 (3.9574)	LR 2.500e-04
0: TRAIN [1][1360/1848]	Time 0.125 (0.130)	Data 5.72e-05 (5.05e-05)	Tok/s 28735 (28458)	Loss/tok 3.8163 (3.9567)	LR 2.500e-04
0: TRAIN [1][1370/1848]	Time 0.126 (0.130)	Data 4.39e-05 (5.05e-05)	Tok/s 29153 (28475)	Loss/tok 3.6305 (3.9552)	LR 2.500e-04
0: TRAIN [1][1380/1848]	Time 0.097 (0.130)	Data 4.43e-05 (5.05e-05)	Tok/s 22115 (28470)	Loss/tok 3.3503 (3.9538)	LR 2.500e-04
0: TRAIN [1][1390/1848]	Time 0.125 (0.130)	Data 7.30e-05 (5.05e-05)	Tok/s 28249 (28462)	Loss/tok 3.6578 (3.9516)	LR 2.500e-04
0: TRAIN [1][1400/1848]	Time 0.153 (0.130)	Data 4.48e-05 (5.04e-05)	Tok/s 32716 (28469)	Loss/tok 3.9043 (3.9498)	LR 2.500e-04
0: TRAIN [1][1410/1848]	Time 0.189 (0.130)	Data 4.51e-05 (5.04e-05)	Tok/s 34188 (28476)	Loss/tok 3.9257 (3.9485)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1420/1848]	Time 0.126 (0.130)	Data 7.87e-05 (5.04e-05)	Tok/s 28522 (28487)	Loss/tok 3.5107 (3.9467)	LR 2.500e-04
0: TRAIN [1][1430/1848]	Time 0.126 (0.130)	Data 4.55e-05 (5.05e-05)	Tok/s 28829 (28488)	Loss/tok 3.5446 (3.9447)	LR 2.500e-04
0: TRAIN [1][1440/1848]	Time 0.189 (0.130)	Data 4.60e-05 (5.05e-05)	Tok/s 34995 (28496)	Loss/tok 4.1342 (3.9434)	LR 2.500e-04
0: TRAIN [1][1450/1848]	Time 0.122 (0.130)	Data 4.32e-05 (5.04e-05)	Tok/s 29761 (28463)	Loss/tok 3.7458 (3.9416)	LR 2.500e-04
0: TRAIN [1][1460/1848]	Time 0.123 (0.130)	Data 4.48e-05 (5.05e-05)	Tok/s 29499 (28475)	Loss/tok 3.5747 (3.9411)	LR 2.500e-04
0: TRAIN [1][1470/1848]	Time 0.097 (0.130)	Data 4.89e-05 (5.05e-05)	Tok/s 21831 (28479)	Loss/tok 3.3303 (3.9399)	LR 2.500e-04
0: TRAIN [1][1480/1848]	Time 0.125 (0.130)	Data 4.34e-05 (5.05e-05)	Tok/s 28954 (28484)	Loss/tok 3.6116 (3.9385)	LR 2.500e-04
0: TRAIN [1][1490/1848]	Time 0.154 (0.130)	Data 4.86e-05 (5.05e-05)	Tok/s 33314 (28483)	Loss/tok 3.6988 (3.9370)	LR 2.500e-04
0: TRAIN [1][1500/1848]	Time 0.092 (0.130)	Data 8.13e-05 (5.05e-05)	Tok/s 24018 (28470)	Loss/tok 3.4149 (3.9357)	LR 2.500e-04
0: TRAIN [1][1510/1848]	Time 0.189 (0.130)	Data 4.34e-05 (5.05e-05)	Tok/s 34463 (28473)	Loss/tok 4.0691 (3.9346)	LR 2.500e-04
0: TRAIN [1][1520/1848]	Time 0.097 (0.130)	Data 4.34e-05 (5.06e-05)	Tok/s 22741 (28465)	Loss/tok 3.3461 (3.9327)	LR 2.500e-04
0: TRAIN [1][1530/1848]	Time 0.155 (0.130)	Data 4.43e-05 (5.05e-05)	Tok/s 32310 (28459)	Loss/tok 3.8085 (3.9312)	LR 2.500e-04
0: TRAIN [1][1540/1848]	Time 0.123 (0.130)	Data 4.48e-05 (5.05e-05)	Tok/s 29013 (28466)	Loss/tok 3.7238 (3.9300)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1550/1848]	Time 0.097 (0.130)	Data 4.82e-05 (5.05e-05)	Tok/s 21966 (28468)	Loss/tok 3.5918 (3.9286)	LR 1.250e-04
0: TRAIN [1][1560/1848]	Time 0.097 (0.130)	Data 4.43e-05 (5.05e-05)	Tok/s 22570 (28456)	Loss/tok 3.4425 (3.9274)	LR 1.250e-04
0: TRAIN [1][1570/1848]	Time 0.186 (0.130)	Data 7.22e-05 (5.05e-05)	Tok/s 35061 (28470)	Loss/tok 4.0006 (3.9268)	LR 1.250e-04
0: TRAIN [1][1580/1848]	Time 0.125 (0.130)	Data 5.15e-05 (5.05e-05)	Tok/s 29201 (28477)	Loss/tok 3.7799 (3.9255)	LR 1.250e-04
0: TRAIN [1][1590/1848]	Time 0.070 (0.130)	Data 4.39e-05 (5.05e-05)	Tok/s 15915 (28455)	Loss/tok 3.0674 (3.9241)	LR 1.250e-04
0: TRAIN [1][1600/1848]	Time 0.190 (0.130)	Data 4.53e-05 (5.05e-05)	Tok/s 34816 (28467)	Loss/tok 3.9720 (3.9232)	LR 1.250e-04
0: TRAIN [1][1610/1848]	Time 0.095 (0.130)	Data 7.75e-05 (5.05e-05)	Tok/s 22619 (28458)	Loss/tok 3.4720 (3.9215)	LR 1.250e-04
0: TRAIN [1][1620/1848]	Time 0.126 (0.130)	Data 4.36e-05 (5.05e-05)	Tok/s 27426 (28482)	Loss/tok 3.6130 (3.9213)	LR 1.250e-04
0: TRAIN [1][1630/1848]	Time 0.187 (0.130)	Data 4.34e-05 (5.05e-05)	Tok/s 35228 (28484)	Loss/tok 3.9438 (3.9202)	LR 1.250e-04
0: TRAIN [1][1640/1848]	Time 0.123 (0.130)	Data 4.82e-05 (5.05e-05)	Tok/s 28802 (28477)	Loss/tok 3.5873 (3.9188)	LR 1.250e-04
0: TRAIN [1][1650/1848]	Time 0.099 (0.130)	Data 4.51e-05 (5.05e-05)	Tok/s 22061 (28474)	Loss/tok 3.4286 (3.9173)	LR 1.250e-04
0: TRAIN [1][1660/1848]	Time 0.152 (0.130)	Data 4.58e-05 (5.05e-05)	Tok/s 32792 (28465)	Loss/tok 3.7876 (3.9162)	LR 1.250e-04
0: TRAIN [1][1670/1848]	Time 0.099 (0.130)	Data 7.58e-05 (5.05e-05)	Tok/s 21906 (28456)	Loss/tok 3.3770 (3.9146)	LR 1.250e-04
0: TRAIN [1][1680/1848]	Time 0.125 (0.130)	Data 4.32e-05 (5.05e-05)	Tok/s 28868 (28463)	Loss/tok 3.6354 (3.9143)	LR 1.250e-04
0: TRAIN [1][1690/1848]	Time 0.154 (0.130)	Data 4.67e-05 (5.05e-05)	Tok/s 33043 (28458)	Loss/tok 3.7845 (3.9131)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1700/1848]	Time 0.125 (0.130)	Data 5.34e-05 (5.05e-05)	Tok/s 39371 (28485)	Loss/tok 3.9776 (3.9125)	LR 1.250e-04
0: TRAIN [1][1710/1848]	Time 0.126 (0.130)	Data 4.36e-05 (5.05e-05)	Tok/s 28650 (28483)	Loss/tok 3.5553 (3.9112)	LR 1.250e-04
0: TRAIN [1][1720/1848]	Time 0.097 (0.131)	Data 4.39e-05 (5.05e-05)	Tok/s 22138 (28497)	Loss/tok 3.4859 (3.9111)	LR 1.250e-04
0: TRAIN [1][1730/1848]	Time 0.096 (0.131)	Data 5.01e-05 (5.05e-05)	Tok/s 22494 (28499)	Loss/tok 3.3077 (3.9098)	LR 1.250e-04
0: TRAIN [1][1740/1848]	Time 0.097 (0.131)	Data 4.43e-05 (5.05e-05)	Tok/s 22384 (28505)	Loss/tok 3.2665 (3.9087)	LR 1.250e-04
0: TRAIN [1][1750/1848]	Time 0.190 (0.131)	Data 7.03e-05 (5.05e-05)	Tok/s 34070 (28511)	Loss/tok 3.9642 (3.9077)	LR 1.250e-04
0: TRAIN [1][1760/1848]	Time 0.070 (0.131)	Data 4.89e-05 (5.05e-05)	Tok/s 14896 (28510)	Loss/tok 2.9799 (3.9068)	LR 1.250e-04
0: TRAIN [1][1770/1848]	Time 0.097 (0.131)	Data 4.36e-05 (5.05e-05)	Tok/s 22245 (28509)	Loss/tok 3.3200 (3.9054)	LR 1.250e-04
0: TRAIN [1][1780/1848]	Time 0.195 (0.131)	Data 4.55e-05 (5.05e-05)	Tok/s 33586 (28516)	Loss/tok 3.9408 (3.9043)	LR 1.250e-04
0: TRAIN [1][1790/1848]	Time 0.099 (0.131)	Data 5.29e-05 (5.05e-05)	Tok/s 21866 (28519)	Loss/tok 3.2958 (3.9038)	LR 1.250e-04
0: TRAIN [1][1800/1848]	Time 0.154 (0.131)	Data 4.34e-05 (5.05e-05)	Tok/s 33412 (28517)	Loss/tok 3.8040 (3.9028)	LR 1.250e-04
0: TRAIN [1][1810/1848]	Time 0.097 (0.131)	Data 4.55e-05 (5.06e-05)	Tok/s 22490 (28514)	Loss/tok 3.4529 (3.9020)	LR 1.250e-04
0: TRAIN [1][1820/1848]	Time 0.122 (0.131)	Data 5.70e-05 (5.06e-05)	Tok/s 30180 (28507)	Loss/tok 3.5795 (3.9009)	LR 1.250e-04
0: TRAIN [1][1830/1848]	Time 0.155 (0.131)	Data 4.29e-05 (5.05e-05)	Tok/s 33061 (28515)	Loss/tok 3.9213 (3.9000)	LR 1.250e-04
0: TRAIN [1][1840/1848]	Time 0.192 (0.131)	Data 4.60e-05 (5.05e-05)	Tok/s 33903 (28512)	Loss/tok 3.9746 (3.8987)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/213]	Time 0.069 (0.000)	Data 8.79e-04 (0.00e+00)	Tok/s 64403 (0)	Loss/tok 5.5228 (5.5228)
0: VALIDATION [1][10/213]	Time 0.036 (0.040)	Data 7.70e-04 (8.00e-04)	Tok/s 76520 (76208)	Loss/tok 5.1314 (5.2339)
0: VALIDATION [1][20/213]	Time 0.030 (0.037)	Data 7.62e-04 (7.84e-04)	Tok/s 77559 (76526)	Loss/tok 5.1819 (5.1453)
0: VALIDATION [1][30/213]	Time 0.028 (0.034)	Data 7.62e-04 (7.75e-04)	Tok/s 76197 (76948)	Loss/tok 4.6990 (5.0924)
0: VALIDATION [1][40/213]	Time 0.026 (0.032)	Data 7.47e-04 (7.69e-04)	Tok/s 74194 (76722)	Loss/tok 5.1039 (5.0636)
0: VALIDATION [1][50/213]	Time 0.024 (0.031)	Data 7.41e-04 (7.65e-04)	Tok/s 76885 (76745)	Loss/tok 5.4659 (5.0430)
0: VALIDATION [1][60/213]	Time 0.022 (0.029)	Data 7.46e-04 (7.62e-04)	Tok/s 75516 (76706)	Loss/tok 4.4565 (5.0055)
0: VALIDATION [1][70/213]	Time 0.020 (0.028)	Data 7.33e-04 (7.59e-04)	Tok/s 78562 (76608)	Loss/tok 4.8235 (4.9796)
0: VALIDATION [1][80/213]	Time 0.020 (0.027)	Data 7.40e-04 (7.61e-04)	Tok/s 73685 (76390)	Loss/tok 4.8070 (4.9594)
0: VALIDATION [1][90/213]	Time 0.019 (0.026)	Data 7.44e-04 (7.59e-04)	Tok/s 70750 (76082)	Loss/tok 4.9022 (4.9420)
0: VALIDATION [1][100/213]	Time 0.019 (0.025)	Data 7.65e-04 (7.58e-04)	Tok/s 66821 (75757)	Loss/tok 4.4001 (4.9180)
0: VALIDATION [1][110/213]	Time 0.016 (0.025)	Data 7.34e-04 (7.56e-04)	Tok/s 73613 (75504)	Loss/tok 4.5947 (4.9061)
0: VALIDATION [1][120/213]	Time 0.015 (0.024)	Data 7.34e-04 (7.55e-04)	Tok/s 72770 (75260)	Loss/tok 4.5120 (4.8914)
0: VALIDATION [1][130/213]	Time 0.015 (0.023)	Data 7.35e-04 (7.53e-04)	Tok/s 69816 (74872)	Loss/tok 4.5716 (4.8794)
0: VALIDATION [1][140/213]	Time 0.014 (0.023)	Data 7.33e-04 (7.52e-04)	Tok/s 68703 (74403)	Loss/tok 4.2926 (4.8660)
0: VALIDATION [1][150/213]	Time 0.013 (0.022)	Data 7.43e-04 (7.51e-04)	Tok/s 69336 (74090)	Loss/tok 4.7189 (4.8567)
0: VALIDATION [1][160/213]	Time 0.012 (0.021)	Data 7.25e-04 (7.50e-04)	Tok/s 66762 (73706)	Loss/tok 4.5432 (4.8453)
0: VALIDATION [1][170/213]	Time 0.012 (0.021)	Data 7.26e-04 (7.49e-04)	Tok/s 60464 (73228)	Loss/tok 4.1427 (4.8334)
0: VALIDATION [1][180/213]	Time 0.010 (0.020)	Data 7.25e-04 (7.48e-04)	Tok/s 64336 (72794)	Loss/tok 4.7772 (4.8271)
0: VALIDATION [1][190/213]	Time 0.010 (0.020)	Data 7.22e-04 (7.47e-04)	Tok/s 59195 (72217)	Loss/tok 4.7340 (4.8161)
0: VALIDATION [1][200/213]	Time 0.008 (0.019)	Data 7.22e-04 (7.46e-04)	Tok/s 57977 (71551)	Loss/tok 4.2874 (4.8031)
0: VALIDATION [1][210/213]	Time 0.007 (0.019)	Data 7.20e-04 (7.45e-04)	Tok/s 48265 (70723)	Loss/tok 4.1816 (4.7928)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/126]	Time 0.2012 (0.2851)	Decoder iters 80.0 (115.4)	Tok/s 12054 (10324)
0: TEST [1][19/126]	Time 0.1620 (0.2654)	Decoder iters 63.0 (112.3)	Tok/s 12779 (10125)
0: TEST [1][29/126]	Time 0.3018 (0.2559)	Decoder iters 149.0 (111.7)	Tok/s 6276 (9825)
0: TEST [1][39/126]	Time 0.2602 (0.2450)	Decoder iters 127.0 (108.9)	Tok/s 6545 (9738)
0: TEST [1][49/126]	Time 0.1305 (0.2316)	Decoder iters 55.0 (103.8)	Tok/s 11323 (9726)
0: TEST [1][59/126]	Time 0.1063 (0.2154)	Decoder iters 45.0 (96.5)	Tok/s 12347 (9995)
0: TEST [1][69/126]	Time 0.1029 (0.2027)	Decoder iters 45.0 (90.9)	Tok/s 11739 (10149)
0: TEST [1][79/126]	Time 0.0977 (0.1959)	Decoder iters 43.0 (88.6)	Tok/s 11147 (10120)
0: TEST [1][89/126]	Time 0.0861 (0.1841)	Decoder iters 38.0 (83.1)	Tok/s 11004 (10271)
0: TEST [1][99/126]	Time 0.0697 (0.1770)	Decoder iters 29.0 (80.3)	Tok/s 11401 (10270)
0: TEST [1][109/126]	Time 0.0644 (0.1670)	Decoder iters 28.0 (75.7)	Tok/s 11400 (10376)
0: TEST [1][119/126]	Time 0.0414 (0.1595)	Decoder iters 17.0 (72.4)	Tok/s 12596 (10385)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 3.8980	Validation Loss: 4.7916	Test BLEU: 8.64
0: Performance: Epoch: 1	Training: 28505 Tok/s	Validation: 70400 Tok/s
0: Finished epoch 1
0: Total training time 564 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                  80|                      8.64|                      28501.5|                         9.404|
DONE!
