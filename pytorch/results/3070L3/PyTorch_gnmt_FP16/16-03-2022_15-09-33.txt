0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070
Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2461
0: Scheduler decay interval: 308
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/1848]	Time 0.182 (0.000)	Data 8.94e-02 (0.00e+00)	Tok/s 19856 (0)	Loss/tok 10.5912 (10.5912)	LR 2.047e-05
0: TRAIN [0][10/1848]	Time 0.123 (0.122)	Data 4.34e-05 (5.30e-05)	Tok/s 29019 (29024)	Loss/tok 9.7189 (10.1349)	LR 2.576e-05
0: TRAIN [0][20/1848]	Time 0.097 (0.122)	Data 5.05e-05 (5.12e-05)	Tok/s 22116 (27506)	Loss/tok 9.0378 (9.8274)	LR 3.244e-05
0: TRAIN [0][30/1848]	Time 0.154 (0.125)	Data 4.51e-05 (4.95e-05)	Tok/s 32118 (28005)	Loss/tok 9.0055 (9.5785)	LR 4.083e-05
0: TRAIN [0][40/1848]	Time 0.124 (0.121)	Data 4.58e-05 (4.96e-05)	Tok/s 28814 (27348)	Loss/tok 8.7691 (9.4263)	LR 5.141e-05
0: TRAIN [0][50/1848]	Time 0.153 (0.120)	Data 4.36e-05 (4.89e-05)	Tok/s 32405 (27232)	Loss/tok 8.9142 (9.2936)	LR 6.472e-05
0: TRAIN [0][60/1848]	Time 0.125 (0.122)	Data 4.53e-05 (5.00e-05)	Tok/s 29199 (27623)	Loss/tok 8.6687 (9.1615)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][70/1848]	Time 0.153 (0.125)	Data 4.77e-05 (5.07e-05)	Tok/s 32241 (28298)	Loss/tok 8.3456 (9.0707)	LR 1.026e-04
0: TRAIN [0][80/1848]	Time 0.122 (0.125)	Data 4.32e-05 (5.09e-05)	Tok/s 29478 (28288)	Loss/tok 8.0534 (8.9627)	LR 1.291e-04
0: TRAIN [0][90/1848]	Time 0.097 (0.124)	Data 4.36e-05 (5.07e-05)	Tok/s 23157 (27990)	Loss/tok 7.9235 (8.8701)	LR 1.626e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][100/1848]	Time 0.156 (0.123)	Data 4.39e-05 (5.05e-05)	Tok/s 32517 (28023)	Loss/tok 7.9150 (8.7798)	LR 2.047e-04
0: TRAIN [0][110/1848]	Time 0.126 (0.125)	Data 4.55e-05 (5.05e-05)	Tok/s 27940 (28294)	Loss/tok 7.7304 (8.6859)	LR 2.576e-04
0: TRAIN [0][120/1848]	Time 0.096 (0.124)	Data 4.77e-05 (5.05e-05)	Tok/s 22598 (28065)	Loss/tok 7.4818 (8.6246)	LR 3.244e-04
0: TRAIN [0][130/1848]	Time 0.149 (0.124)	Data 4.43e-05 (5.00e-05)	Tok/s 33358 (27949)	Loss/tok 7.8260 (8.5623)	LR 4.083e-04
0: TRAIN [0][140/1848]	Time 0.070 (0.124)	Data 4.46e-05 (5.00e-05)	Tok/s 15667 (27923)	Loss/tok 7.3713 (8.5050)	LR 5.141e-04
0: TRAIN [0][150/1848]	Time 0.125 (0.123)	Data 7.94e-05 (4.99e-05)	Tok/s 29276 (27815)	Loss/tok 7.8309 (8.4559)	LR 6.472e-04
0: TRAIN [0][160/1848]	Time 0.125 (0.124)	Data 4.53e-05 (4.97e-05)	Tok/s 29039 (27859)	Loss/tok 7.6131 (8.4078)	LR 8.148e-04
0: TRAIN [0][170/1848]	Time 0.124 (0.124)	Data 4.74e-05 (4.97e-05)	Tok/s 28781 (27902)	Loss/tok 7.8473 (8.3650)	LR 1.026e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][180/1848]	Time 0.065 (0.124)	Data 4.34e-05 (4.94e-05)	Tok/s 33274 (27979)	Loss/tok 7.3849 (8.3314)	LR 1.291e-03
0: TRAIN [0][190/1848]	Time 0.125 (0.124)	Data 4.46e-05 (4.97e-05)	Tok/s 28919 (28064)	Loss/tok 7.5243 (8.2955)	LR 1.626e-03
0: TRAIN [0][200/1848]	Time 0.100 (0.125)	Data 5.13e-05 (4.99e-05)	Tok/s 22715 (28047)	Loss/tok 7.4214 (8.2686)	LR 2.000e-03
0: TRAIN [0][210/1848]	Time 0.125 (0.124)	Data 4.39e-05 (4.97e-05)	Tok/s 27841 (27982)	Loss/tok 7.5716 (8.2378)	LR 2.000e-03
0: TRAIN [0][220/1848]	Time 0.124 (0.125)	Data 4.48e-05 (4.99e-05)	Tok/s 28722 (28058)	Loss/tok 7.4339 (8.2012)	LR 2.000e-03
0: TRAIN [0][230/1848]	Time 0.153 (0.125)	Data 5.75e-05 (5.00e-05)	Tok/s 32537 (28084)	Loss/tok 7.6559 (8.1664)	LR 2.000e-03
0: TRAIN [0][240/1848]	Time 0.095 (0.126)	Data 6.87e-05 (4.99e-05)	Tok/s 23181 (28110)	Loss/tok 6.9944 (8.1337)	LR 2.000e-03
0: TRAIN [0][250/1848]	Time 0.154 (0.127)	Data 4.41e-05 (5.00e-05)	Tok/s 32356 (28196)	Loss/tok 7.2919 (8.0956)	LR 2.000e-03
0: TRAIN [0][260/1848]	Time 0.124 (0.126)	Data 5.56e-05 (5.00e-05)	Tok/s 29469 (28177)	Loss/tok 7.1171 (8.0606)	LR 2.000e-03
0: TRAIN [0][270/1848]	Time 0.121 (0.126)	Data 7.01e-05 (4.99e-05)	Tok/s 29386 (28051)	Loss/tok 7.1266 (8.0318)	LR 2.000e-03
0: TRAIN [0][280/1848]	Time 0.153 (0.126)	Data 4.82e-05 (5.01e-05)	Tok/s 33173 (28106)	Loss/tok 7.2064 (7.9966)	LR 2.000e-03
0: TRAIN [0][290/1848]	Time 0.154 (0.126)	Data 4.39e-05 (4.99e-05)	Tok/s 32401 (28116)	Loss/tok 7.2112 (7.9651)	LR 2.000e-03
0: TRAIN [0][300/1848]	Time 0.153 (0.126)	Data 4.41e-05 (4.99e-05)	Tok/s 33742 (28130)	Loss/tok 7.0520 (7.9311)	LR 2.000e-03
0: TRAIN [0][310/1848]	Time 0.154 (0.127)	Data 4.96e-05 (5.00e-05)	Tok/s 33214 (28163)	Loss/tok 7.1268 (7.9002)	LR 2.000e-03
0: TRAIN [0][320/1848]	Time 0.189 (0.127)	Data 4.41e-05 (4.99e-05)	Tok/s 34142 (28228)	Loss/tok 7.0834 (7.8658)	LR 2.000e-03
0: TRAIN [0][330/1848]	Time 0.096 (0.128)	Data 4.58e-05 (5.00e-05)	Tok/s 23225 (28322)	Loss/tok 6.4515 (7.8298)	LR 2.000e-03
0: TRAIN [0][340/1848]	Time 0.190 (0.129)	Data 4.51e-05 (5.00e-05)	Tok/s 34887 (28394)	Loss/tok 7.1153 (7.7989)	LR 2.000e-03
0: TRAIN [0][350/1848]	Time 0.099 (0.128)	Data 7.58e-05 (5.00e-05)	Tok/s 21787 (28398)	Loss/tok 6.5800 (7.7709)	LR 2.000e-03
0: TRAIN [0][360/1848]	Time 0.099 (0.128)	Data 4.46e-05 (4.99e-05)	Tok/s 21933 (28389)	Loss/tok 6.3589 (7.7435)	LR 2.000e-03
0: TRAIN [0][370/1848]	Time 0.125 (0.128)	Data 4.72e-05 (5.00e-05)	Tok/s 28658 (28417)	Loss/tok 6.4887 (7.7135)	LR 2.000e-03
0: TRAIN [0][380/1848]	Time 0.097 (0.129)	Data 6.53e-05 (4.99e-05)	Tok/s 22576 (28441)	Loss/tok 6.3129 (7.6840)	LR 2.000e-03
0: TRAIN [0][390/1848]	Time 0.125 (0.129)	Data 4.39e-05 (4.99e-05)	Tok/s 28646 (28450)	Loss/tok 6.4967 (7.6562)	LR 2.000e-03
0: TRAIN [0][400/1848]	Time 0.095 (0.129)	Data 7.15e-05 (4.99e-05)	Tok/s 22823 (28396)	Loss/tok 6.2939 (7.6318)	LR 2.000e-03
0: TRAIN [0][410/1848]	Time 0.188 (0.129)	Data 4.46e-05 (4.99e-05)	Tok/s 34615 (28411)	Loss/tok 6.7111 (7.6047)	LR 2.000e-03
0: TRAIN [0][420/1848]	Time 0.127 (0.129)	Data 4.55e-05 (4.98e-05)	Tok/s 28447 (28417)	Loss/tok 6.5727 (7.5789)	LR 2.000e-03
0: TRAIN [0][430/1848]	Time 0.127 (0.129)	Data 5.20e-05 (4.99e-05)	Tok/s 28252 (28431)	Loss/tok 6.3869 (7.5537)	LR 2.000e-03
0: TRAIN [0][440/1848]	Time 0.125 (0.129)	Data 5.29e-05 (4.99e-05)	Tok/s 28756 (28402)	Loss/tok 6.3602 (7.5306)	LR 2.000e-03
0: TRAIN [0][450/1848]	Time 0.187 (0.129)	Data 4.53e-05 (5.00e-05)	Tok/s 35238 (28380)	Loss/tok 6.6693 (7.5075)	LR 2.000e-03
0: TRAIN [0][460/1848]	Time 0.126 (0.128)	Data 4.46e-05 (4.99e-05)	Tok/s 28498 (28349)	Loss/tok 6.4741 (7.4851)	LR 2.000e-03
0: TRAIN [0][470/1848]	Time 0.188 (0.128)	Data 4.51e-05 (4.99e-05)	Tok/s 34854 (28368)	Loss/tok 6.6504 (7.4606)	LR 2.000e-03
0: TRAIN [0][480/1848]	Time 0.125 (0.128)	Data 4.79e-05 (5.00e-05)	Tok/s 28320 (28362)	Loss/tok 6.1768 (7.4381)	LR 2.000e-03
0: TRAIN [0][490/1848]	Time 0.122 (0.128)	Data 4.46e-05 (4.99e-05)	Tok/s 29234 (28352)	Loss/tok 6.1371 (7.4150)	LR 2.000e-03
0: TRAIN [0][500/1848]	Time 0.155 (0.128)	Data 4.36e-05 (4.99e-05)	Tok/s 32210 (28310)	Loss/tok 6.3861 (7.3946)	LR 2.000e-03
0: TRAIN [0][510/1848]	Time 0.186 (0.128)	Data 7.89e-05 (4.99e-05)	Tok/s 34760 (28303)	Loss/tok 6.5801 (7.3730)	LR 2.000e-03
0: TRAIN [0][520/1848]	Time 0.123 (0.128)	Data 6.89e-05 (5.01e-05)	Tok/s 28888 (28257)	Loss/tok 6.0859 (7.3539)	LR 2.000e-03
0: TRAIN [0][530/1848]	Time 0.154 (0.128)	Data 4.96e-05 (5.01e-05)	Tok/s 32414 (28222)	Loss/tok 6.4177 (7.3334)	LR 2.000e-03
0: TRAIN [0][540/1848]	Time 0.153 (0.128)	Data 5.53e-05 (5.00e-05)	Tok/s 33616 (28224)	Loss/tok 6.2003 (7.3120)	LR 2.000e-03
0: TRAIN [0][550/1848]	Time 0.125 (0.128)	Data 4.29e-05 (5.00e-05)	Tok/s 29059 (28272)	Loss/tok 5.9827 (7.2873)	LR 2.000e-03
0: TRAIN [0][560/1848]	Time 0.154 (0.128)	Data 5.70e-05 (5.00e-05)	Tok/s 32954 (28261)	Loss/tok 6.1611 (7.2663)	LR 2.000e-03
0: TRAIN [0][570/1848]	Time 0.125 (0.128)	Data 4.39e-05 (4.99e-05)	Tok/s 29194 (28242)	Loss/tok 5.8091 (7.2457)	LR 2.000e-03
0: TRAIN [0][580/1848]	Time 0.152 (0.128)	Data 4.48e-05 (4.99e-05)	Tok/s 33564 (28270)	Loss/tok 6.1816 (7.2234)	LR 2.000e-03
0: TRAIN [0][590/1848]	Time 0.070 (0.128)	Data 5.63e-05 (4.99e-05)	Tok/s 15339 (28236)	Loss/tok 5.2959 (7.2050)	LR 2.000e-03
0: TRAIN [0][600/1848]	Time 0.152 (0.128)	Data 4.36e-05 (4.99e-05)	Tok/s 32525 (28213)	Loss/tok 6.1959 (7.1872)	LR 2.000e-03
0: TRAIN [0][610/1848]	Time 0.190 (0.128)	Data 4.70e-05 (4.99e-05)	Tok/s 33886 (28235)	Loss/tok 6.3031 (7.1658)	LR 2.000e-03
0: TRAIN [0][620/1848]	Time 0.126 (0.128)	Data 4.39e-05 (4.98e-05)	Tok/s 28079 (28208)	Loss/tok 5.8464 (7.1474)	LR 2.000e-03
0: TRAIN [0][630/1848]	Time 0.125 (0.128)	Data 4.60e-05 (4.98e-05)	Tok/s 28626 (28220)	Loss/tok 5.8580 (7.1262)	LR 2.000e-03
0: TRAIN [0][640/1848]	Time 0.128 (0.128)	Data 5.05e-05 (4.98e-05)	Tok/s 28799 (28225)	Loss/tok 5.8992 (7.1063)	LR 2.000e-03
0: TRAIN [0][650/1848]	Time 0.154 (0.128)	Data 4.43e-05 (4.98e-05)	Tok/s 32239 (28208)	Loss/tok 5.9798 (7.0885)	LR 2.000e-03
0: TRAIN [0][660/1848]	Time 0.153 (0.128)	Data 4.55e-05 (4.99e-05)	Tok/s 32341 (28251)	Loss/tok 6.0136 (7.0669)	LR 2.000e-03
0: TRAIN [0][670/1848]	Time 0.096 (0.128)	Data 5.70e-05 (4.99e-05)	Tok/s 22144 (28249)	Loss/tok 5.3723 (7.0480)	LR 2.000e-03
0: TRAIN [0][680/1848]	Time 0.122 (0.128)	Data 4.46e-05 (4.98e-05)	Tok/s 29479 (28233)	Loss/tok 5.4973 (7.0292)	LR 2.000e-03
0: TRAIN [0][690/1848]	Time 0.123 (0.128)	Data 4.43e-05 (4.98e-05)	Tok/s 29240 (28237)	Loss/tok 5.6754 (7.0102)	LR 2.000e-03
0: TRAIN [0][700/1848]	Time 0.153 (0.128)	Data 5.94e-05 (4.98e-05)	Tok/s 32942 (28252)	Loss/tok 5.8203 (6.9911)	LR 2.000e-03
0: TRAIN [0][710/1848]	Time 0.125 (0.128)	Data 4.29e-05 (4.98e-05)	Tok/s 28481 (28223)	Loss/tok 5.5801 (6.9750)	LR 2.000e-03
0: TRAIN [0][720/1848]	Time 0.188 (0.128)	Data 4.72e-05 (4.98e-05)	Tok/s 34617 (28224)	Loss/tok 6.0243 (6.9571)	LR 2.000e-03
0: TRAIN [0][730/1848]	Time 0.121 (0.128)	Data 6.94e-05 (4.98e-05)	Tok/s 29553 (28244)	Loss/tok 5.6233 (6.9379)	LR 2.000e-03
0: TRAIN [0][740/1848]	Time 0.193 (0.128)	Data 4.36e-05 (4.99e-05)	Tok/s 34437 (28246)	Loss/tok 5.9569 (6.9205)	LR 2.000e-03
0: TRAIN [0][750/1848]	Time 0.125 (0.128)	Data 4.70e-05 (4.99e-05)	Tok/s 29039 (28264)	Loss/tok 5.4538 (6.9012)	LR 2.000e-03
0: TRAIN [0][760/1848]	Time 0.125 (0.128)	Data 4.53e-05 (4.98e-05)	Tok/s 28663 (28291)	Loss/tok 5.4952 (6.8811)	LR 2.000e-03
0: TRAIN [0][770/1848]	Time 0.097 (0.129)	Data 4.34e-05 (4.98e-05)	Tok/s 21959 (28322)	Loss/tok 5.2063 (6.8615)	LR 2.000e-03
0: TRAIN [0][780/1848]	Time 0.151 (0.129)	Data 4.60e-05 (4.99e-05)	Tok/s 33416 (28316)	Loss/tok 5.6348 (6.8446)	LR 2.000e-03
0: TRAIN [0][790/1848]	Time 0.125 (0.129)	Data 7.99e-05 (4.98e-05)	Tok/s 28769 (28353)	Loss/tok 5.4622 (6.8242)	LR 2.000e-03
0: TRAIN [0][800/1848]	Time 0.126 (0.129)	Data 4.32e-05 (4.99e-05)	Tok/s 28444 (28356)	Loss/tok 5.3788 (6.8074)	LR 2.000e-03
0: TRAIN [0][810/1848]	Time 0.128 (0.129)	Data 4.53e-05 (4.99e-05)	Tok/s 28601 (28359)	Loss/tok 5.4292 (6.7913)	LR 2.000e-03
0: TRAIN [0][820/1848]	Time 0.097 (0.129)	Data 4.82e-05 (4.98e-05)	Tok/s 22592 (28335)	Loss/tok 5.0057 (6.7764)	LR 2.000e-03
0: TRAIN [0][830/1848]	Time 0.123 (0.129)	Data 4.41e-05 (4.98e-05)	Tok/s 29381 (28315)	Loss/tok 5.3679 (6.7621)	LR 2.000e-03
0: TRAIN [0][840/1848]	Time 0.097 (0.129)	Data 5.44e-05 (4.98e-05)	Tok/s 22817 (28330)	Loss/tok 4.9596 (6.7453)	LR 2.000e-03
0: TRAIN [0][850/1848]	Time 0.099 (0.129)	Data 4.34e-05 (4.99e-05)	Tok/s 21462 (28336)	Loss/tok 4.9000 (6.7286)	LR 2.000e-03
0: TRAIN [0][860/1848]	Time 0.097 (0.129)	Data 4.51e-05 (4.99e-05)	Tok/s 21857 (28322)	Loss/tok 4.9165 (6.7137)	LR 2.000e-03
0: TRAIN [0][870/1848]	Time 0.094 (0.129)	Data 4.39e-05 (4.99e-05)	Tok/s 22794 (28301)	Loss/tok 4.6536 (6.6994)	LR 2.000e-03
0: TRAIN [0][880/1848]	Time 0.189 (0.129)	Data 4.48e-05 (4.99e-05)	Tok/s 34702 (28298)	Loss/tok 5.6398 (6.6839)	LR 2.000e-03
0: TRAIN [0][890/1848]	Time 0.100 (0.129)	Data 4.84e-05 (4.99e-05)	Tok/s 21472 (28295)	Loss/tok 4.5970 (6.6687)	LR 2.000e-03
0: TRAIN [0][900/1848]	Time 0.153 (0.129)	Data 4.48e-05 (4.99e-05)	Tok/s 33469 (28320)	Loss/tok 5.3331 (6.6508)	LR 2.000e-03
0: TRAIN [0][910/1848]	Time 0.151 (0.129)	Data 4.34e-05 (4.99e-05)	Tok/s 33748 (28329)	Loss/tok 5.3438 (6.6349)	LR 2.000e-03
0: TRAIN [0][920/1848]	Time 0.068 (0.129)	Data 4.53e-05 (4.99e-05)	Tok/s 15324 (28339)	Loss/tok 4.5247 (6.6189)	LR 2.000e-03
0: TRAIN [0][930/1848]	Time 0.122 (0.129)	Data 4.34e-05 (4.99e-05)	Tok/s 30070 (28323)	Loss/tok 4.9400 (6.6045)	LR 2.000e-03
0: TRAIN [0][940/1848]	Time 0.096 (0.129)	Data 4.41e-05 (4.99e-05)	Tok/s 23412 (28308)	Loss/tok 4.9175 (6.5908)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][950/1848]	Time 0.095 (0.129)	Data 5.32e-05 (4.99e-05)	Tok/s 38391 (28326)	Loss/tok 4.9541 (6.5755)	LR 2.000e-03
0: TRAIN [0][960/1848]	Time 0.124 (0.129)	Data 4.65e-05 (4.99e-05)	Tok/s 29132 (28356)	Loss/tok 5.2586 (6.5584)	LR 2.000e-03
0: TRAIN [0][970/1848]	Time 0.125 (0.129)	Data 4.48e-05 (4.99e-05)	Tok/s 29041 (28361)	Loss/tok 5.1138 (6.5442)	LR 2.000e-03
0: TRAIN [0][980/1848]	Time 0.092 (0.129)	Data 7.27e-05 (5.00e-05)	Tok/s 23197 (28350)	Loss/tok 4.5799 (6.5303)	LR 2.000e-03
0: TRAIN [0][990/1848]	Time 0.126 (0.129)	Data 4.48e-05 (4.99e-05)	Tok/s 29243 (28394)	Loss/tok 5.0091 (6.5139)	LR 2.000e-03
0: TRAIN [0][1000/1848]	Time 0.153 (0.129)	Data 4.98e-05 (4.99e-05)	Tok/s 32881 (28400)	Loss/tok 5.1888 (6.4993)	LR 2.000e-03
0: TRAIN [0][1010/1848]	Time 0.152 (0.129)	Data 4.72e-05 (4.99e-05)	Tok/s 32844 (28419)	Loss/tok 5.2473 (6.4841)	LR 2.000e-03
0: TRAIN [0][1020/1848]	Time 0.071 (0.130)	Data 4.36e-05 (4.99e-05)	Tok/s 15084 (28430)	Loss/tok 4.4806 (6.4696)	LR 2.000e-03
0: TRAIN [0][1030/1848]	Time 0.149 (0.130)	Data 4.36e-05 (4.99e-05)	Tok/s 33252 (28450)	Loss/tok 5.2196 (6.4545)	LR 2.000e-03
0: TRAIN [0][1040/1848]	Time 0.153 (0.130)	Data 4.67e-05 (4.99e-05)	Tok/s 32812 (28453)	Loss/tok 5.1622 (6.4406)	LR 2.000e-03
0: TRAIN [0][1050/1848]	Time 0.098 (0.130)	Data 4.39e-05 (4.99e-05)	Tok/s 21680 (28462)	Loss/tok 4.8277 (6.4270)	LR 2.000e-03
0: TRAIN [0][1060/1848]	Time 0.158 (0.130)	Data 4.46e-05 (4.99e-05)	Tok/s 32023 (28471)	Loss/tok 4.9584 (6.4129)	LR 2.000e-03
0: TRAIN [0][1070/1848]	Time 0.125 (0.130)	Data 4.89e-05 (5.00e-05)	Tok/s 28084 (28467)	Loss/tok 4.7874 (6.4002)	LR 2.000e-03
0: TRAIN [0][1080/1848]	Time 0.126 (0.130)	Data 4.55e-05 (4.99e-05)	Tok/s 28292 (28459)	Loss/tok 4.7519 (6.3877)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1090/1848]	Time 0.124 (0.130)	Data 4.36e-05 (5.00e-05)	Tok/s 40527 (28461)	Loss/tok 5.1197 (6.3751)	LR 2.000e-03
0: TRAIN [0][1100/1848]	Time 0.152 (0.130)	Data 4.63e-05 (5.00e-05)	Tok/s 32960 (28445)	Loss/tok 5.1434 (6.3639)	LR 2.000e-03
0: TRAIN [0][1110/1848]	Time 0.123 (0.129)	Data 4.41e-05 (5.00e-05)	Tok/s 29327 (28442)	Loss/tok 4.8806 (6.3515)	LR 2.000e-03
0: TRAIN [0][1120/1848]	Time 0.067 (0.129)	Data 4.86e-05 (5.00e-05)	Tok/s 15847 (28408)	Loss/tok 4.3558 (6.3415)	LR 2.000e-03
0: TRAIN [0][1130/1848]	Time 0.186 (0.129)	Data 6.89e-05 (5.00e-05)	Tok/s 34903 (28424)	Loss/tok 5.1957 (6.3274)	LR 2.000e-03
0: TRAIN [0][1140/1848]	Time 0.154 (0.129)	Data 4.53e-05 (5.01e-05)	Tok/s 32681 (28415)	Loss/tok 5.0219 (6.3159)	LR 2.000e-03
0: TRAIN [0][1150/1848]	Time 0.187 (0.129)	Data 5.56e-05 (5.00e-05)	Tok/s 34994 (28408)	Loss/tok 5.1894 (6.3044)	LR 2.000e-03
0: TRAIN [0][1160/1848]	Time 0.122 (0.129)	Data 4.63e-05 (5.00e-05)	Tok/s 29881 (28403)	Loss/tok 4.7528 (6.2921)	LR 2.000e-03
0: TRAIN [0][1170/1848]	Time 0.124 (0.130)	Data 4.58e-05 (5.00e-05)	Tok/s 28570 (28428)	Loss/tok 4.8069 (6.2776)	LR 2.000e-03
0: TRAIN [0][1180/1848]	Time 0.150 (0.129)	Data 8.56e-05 (5.01e-05)	Tok/s 33758 (28415)	Loss/tok 4.7701 (6.2662)	LR 2.000e-03
0: TRAIN [0][1190/1848]	Time 0.124 (0.130)	Data 4.53e-05 (5.00e-05)	Tok/s 29131 (28439)	Loss/tok 4.6809 (6.2531)	LR 2.000e-03
0: TRAIN [0][1200/1848]	Time 0.154 (0.130)	Data 4.39e-05 (5.00e-05)	Tok/s 32656 (28448)	Loss/tok 4.9042 (6.2408)	LR 2.000e-03
0: TRAIN [0][1210/1848]	Time 0.155 (0.130)	Data 5.72e-05 (5.00e-05)	Tok/s 32922 (28450)	Loss/tok 4.8554 (6.2293)	LR 2.000e-03
0: TRAIN [0][1220/1848]	Time 0.187 (0.130)	Data 4.60e-05 (5.00e-05)	Tok/s 34410 (28452)	Loss/tok 5.2180 (6.2178)	LR 2.000e-03
0: TRAIN [0][1230/1848]	Time 0.097 (0.130)	Data 4.63e-05 (5.00e-05)	Tok/s 22023 (28456)	Loss/tok 4.6079 (6.2064)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1240/1848]	Time 0.097 (0.130)	Data 5.70e-05 (5.01e-05)	Tok/s 22415 (28462)	Loss/tok 4.4477 (6.1951)	LR 2.000e-03
0: TRAIN [0][1250/1848]	Time 0.096 (0.130)	Data 4.53e-05 (5.00e-05)	Tok/s 23600 (28461)	Loss/tok 4.6001 (6.1842)	LR 2.000e-03
0: TRAIN [0][1260/1848]	Time 0.094 (0.130)	Data 4.65e-05 (5.00e-05)	Tok/s 23189 (28463)	Loss/tok 4.2189 (6.1725)	LR 2.000e-03
0: TRAIN [0][1270/1848]	Time 0.128 (0.130)	Data 7.41e-05 (5.01e-05)	Tok/s 28501 (28478)	Loss/tok 4.7589 (6.1604)	LR 2.000e-03
0: TRAIN [0][1280/1848]	Time 0.124 (0.130)	Data 7.06e-05 (5.01e-05)	Tok/s 29558 (28478)	Loss/tok 4.7043 (6.1496)	LR 2.000e-03
0: TRAIN [0][1290/1848]	Time 0.097 (0.130)	Data 4.46e-05 (5.01e-05)	Tok/s 22773 (28480)	Loss/tok 4.2987 (6.1383)	LR 2.000e-03
0: TRAIN [0][1300/1848]	Time 0.099 (0.130)	Data 7.84e-05 (5.01e-05)	Tok/s 22139 (28474)	Loss/tok 4.4092 (6.1287)	LR 2.000e-03
0: TRAIN [0][1310/1848]	Time 0.123 (0.130)	Data 4.46e-05 (5.01e-05)	Tok/s 29257 (28479)	Loss/tok 4.6167 (6.1180)	LR 2.000e-03
0: TRAIN [0][1320/1848]	Time 0.126 (0.130)	Data 6.84e-05 (5.02e-05)	Tok/s 28648 (28488)	Loss/tok 4.5779 (6.1069)	LR 2.000e-03
0: TRAIN [0][1330/1848]	Time 0.190 (0.130)	Data 6.82e-05 (5.01e-05)	Tok/s 34138 (28478)	Loss/tok 5.1115 (6.0972)	LR 2.000e-03
0: TRAIN [0][1340/1848]	Time 0.152 (0.130)	Data 4.34e-05 (5.02e-05)	Tok/s 32894 (28461)	Loss/tok 4.8921 (6.0885)	LR 2.000e-03
0: TRAIN [0][1350/1848]	Time 0.125 (0.130)	Data 5.05e-05 (5.02e-05)	Tok/s 28844 (28464)	Loss/tok 4.6779 (6.0782)	LR 2.000e-03
0: TRAIN [0][1360/1848]	Time 0.154 (0.130)	Data 4.58e-05 (5.02e-05)	Tok/s 32764 (28485)	Loss/tok 4.8024 (6.0661)	LR 2.000e-03
0: TRAIN [0][1370/1848]	Time 0.125 (0.130)	Data 4.53e-05 (5.02e-05)	Tok/s 28912 (28498)	Loss/tok 4.6245 (6.0555)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1380/1848]	Time 0.160 (0.130)	Data 4.72e-05 (5.02e-05)	Tok/s 40101 (28501)	Loss/tok 5.2027 (6.0460)	LR 2.000e-03
0: TRAIN [0][1390/1848]	Time 0.153 (0.130)	Data 7.44e-05 (5.03e-05)	Tok/s 32913 (28524)	Loss/tok 4.8012 (6.0340)	LR 2.000e-03
0: TRAIN [0][1400/1848]	Time 0.153 (0.130)	Data 7.06e-05 (5.03e-05)	Tok/s 33269 (28538)	Loss/tok 4.7403 (6.0230)	LR 2.000e-03
0: TRAIN [0][1410/1848]	Time 0.152 (0.130)	Data 4.36e-05 (5.03e-05)	Tok/s 33448 (28541)	Loss/tok 4.7580 (6.0134)	LR 2.000e-03
0: TRAIN [0][1420/1848]	Time 0.155 (0.130)	Data 7.84e-05 (5.03e-05)	Tok/s 33180 (28539)	Loss/tok 4.7694 (6.0041)	LR 2.000e-03
0: TRAIN [0][1430/1848]	Time 0.153 (0.130)	Data 4.63e-05 (5.03e-05)	Tok/s 32872 (28544)	Loss/tok 4.7099 (5.9945)	LR 2.000e-03
0: TRAIN [0][1440/1848]	Time 0.156 (0.131)	Data 4.46e-05 (5.03e-05)	Tok/s 32289 (28547)	Loss/tok 4.6225 (5.9846)	LR 2.000e-03
0: TRAIN [0][1450/1848]	Time 0.097 (0.131)	Data 5.51e-05 (5.04e-05)	Tok/s 23541 (28557)	Loss/tok 4.4520 (5.9745)	LR 2.000e-03
0: TRAIN [0][1460/1848]	Time 0.122 (0.131)	Data 4.34e-05 (5.03e-05)	Tok/s 29379 (28543)	Loss/tok 4.2359 (5.9658)	LR 2.000e-03
0: TRAIN [0][1470/1848]	Time 0.195 (0.131)	Data 4.51e-05 (5.03e-05)	Tok/s 33333 (28534)	Loss/tok 4.9101 (5.9572)	LR 2.000e-03
0: TRAIN [0][1480/1848]	Time 0.190 (0.131)	Data 4.53e-05 (5.03e-05)	Tok/s 34039 (28537)	Loss/tok 4.9048 (5.9481)	LR 2.000e-03
0: TRAIN [0][1490/1848]	Time 0.124 (0.131)	Data 6.89e-05 (5.04e-05)	Tok/s 29839 (28547)	Loss/tok 4.4281 (5.9383)	LR 2.000e-03
0: TRAIN [0][1500/1848]	Time 0.097 (0.131)	Data 4.72e-05 (5.04e-05)	Tok/s 22159 (28543)	Loss/tok 4.2785 (5.9299)	LR 2.000e-03
0: TRAIN [0][1510/1848]	Time 0.097 (0.131)	Data 4.53e-05 (5.04e-05)	Tok/s 21912 (28524)	Loss/tok 4.2055 (5.9224)	LR 2.000e-03
0: TRAIN [0][1520/1848]	Time 0.123 (0.131)	Data 4.41e-05 (5.04e-05)	Tok/s 29901 (28532)	Loss/tok 4.3851 (5.9133)	LR 2.000e-03
0: TRAIN [0][1530/1848]	Time 0.153 (0.131)	Data 4.79e-05 (5.05e-05)	Tok/s 32885 (28542)	Loss/tok 4.6829 (5.9038)	LR 2.000e-03
0: TRAIN [0][1540/1848]	Time 0.194 (0.131)	Data 4.46e-05 (5.05e-05)	Tok/s 33491 (28551)	Loss/tok 4.9664 (5.8942)	LR 2.000e-03
0: TRAIN [0][1550/1848]	Time 0.123 (0.131)	Data 4.51e-05 (5.05e-05)	Tok/s 29459 (28559)	Loss/tok 4.4171 (5.8853)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1560/1848]	Time 0.125 (0.131)	Data 4.67e-05 (5.06e-05)	Tok/s 28971 (28549)	Loss/tok 4.3501 (5.8775)	LR 2.000e-03
0: TRAIN [0][1570/1848]	Time 0.097 (0.131)	Data 4.46e-05 (5.05e-05)	Tok/s 22432 (28537)	Loss/tok 4.0885 (5.8695)	LR 2.000e-03
0: TRAIN [0][1580/1848]	Time 0.152 (0.131)	Data 4.43e-05 (5.05e-05)	Tok/s 32854 (28522)	Loss/tok 4.6747 (5.8620)	LR 2.000e-03
0: TRAIN [0][1590/1848]	Time 0.122 (0.130)	Data 9.04e-05 (5.06e-05)	Tok/s 30041 (28520)	Loss/tok 4.5363 (5.8537)	LR 2.000e-03
0: TRAIN [0][1600/1848]	Time 0.123 (0.130)	Data 4.41e-05 (5.06e-05)	Tok/s 30036 (28505)	Loss/tok 4.4592 (5.8467)	LR 2.000e-03
0: TRAIN [0][1610/1848]	Time 0.189 (0.130)	Data 4.67e-05 (5.06e-05)	Tok/s 34855 (28513)	Loss/tok 4.8417 (5.8379)	LR 2.000e-03
0: TRAIN [0][1620/1848]	Time 0.154 (0.131)	Data 4.48e-05 (5.06e-05)	Tok/s 32112 (28516)	Loss/tok 4.4243 (5.8294)	LR 2.000e-03
0: TRAIN [0][1630/1848]	Time 0.152 (0.131)	Data 4.60e-05 (5.06e-05)	Tok/s 33618 (28522)	Loss/tok 4.3869 (5.8207)	LR 2.000e-03
0: TRAIN [0][1640/1848]	Time 0.128 (0.131)	Data 4.70e-05 (5.06e-05)	Tok/s 28379 (28520)	Loss/tok 4.4615 (5.8131)	LR 2.000e-03
0: TRAIN [0][1650/1848]	Time 0.155 (0.131)	Data 4.41e-05 (5.06e-05)	Tok/s 32092 (28514)	Loss/tok 4.6348 (5.8056)	LR 2.000e-03
0: TRAIN [0][1660/1848]	Time 0.123 (0.131)	Data 4.46e-05 (5.06e-05)	Tok/s 29349 (28525)	Loss/tok 4.4744 (5.7972)	LR 2.000e-03
0: TRAIN [0][1670/1848]	Time 0.094 (0.131)	Data 7.46e-05 (5.07e-05)	Tok/s 23038 (28523)	Loss/tok 4.1775 (5.7894)	LR 2.000e-03
0: TRAIN [0][1680/1848]	Time 0.123 (0.131)	Data 4.43e-05 (5.07e-05)	Tok/s 29120 (28520)	Loss/tok 4.3361 (5.7818)	LR 2.000e-03
0: TRAIN [0][1690/1848]	Time 0.123 (0.131)	Data 7.27e-05 (5.07e-05)	Tok/s 29514 (28519)	Loss/tok 4.2735 (5.7747)	LR 2.000e-03
0: TRAIN [0][1700/1848]	Time 0.152 (0.131)	Data 4.91e-05 (5.07e-05)	Tok/s 33360 (28523)	Loss/tok 4.7168 (5.7670)	LR 2.000e-03
0: TRAIN [0][1710/1848]	Time 0.121 (0.130)	Data 6.77e-05 (5.07e-05)	Tok/s 30651 (28516)	Loss/tok 4.3667 (5.7598)	LR 2.000e-03
0: TRAIN [0][1720/1848]	Time 0.127 (0.131)	Data 4.82e-05 (5.07e-05)	Tok/s 28215 (28516)	Loss/tok 4.3331 (5.7521)	LR 2.000e-03
0: TRAIN [0][1730/1848]	Time 0.153 (0.130)	Data 4.41e-05 (5.07e-05)	Tok/s 32527 (28514)	Loss/tok 4.7408 (5.7447)	LR 2.000e-03
0: TRAIN [0][1740/1848]	Time 0.094 (0.130)	Data 4.51e-05 (5.08e-05)	Tok/s 22968 (28509)	Loss/tok 4.0585 (5.7375)	LR 2.000e-03
0: TRAIN [0][1750/1848]	Time 0.095 (0.130)	Data 4.70e-05 (5.08e-05)	Tok/s 22747 (28499)	Loss/tok 4.1585 (5.7309)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1760/1848]	Time 0.124 (0.130)	Data 4.36e-05 (5.08e-05)	Tok/s 28919 (28510)	Loss/tok 4.2644 (5.7232)	LR 2.000e-03
0: TRAIN [0][1770/1848]	Time 0.097 (0.130)	Data 4.48e-05 (5.08e-05)	Tok/s 22311 (28501)	Loss/tok 3.8950 (5.7162)	LR 2.000e-03
0: TRAIN [0][1780/1848]	Time 0.097 (0.130)	Data 7.46e-05 (5.09e-05)	Tok/s 22727 (28504)	Loss/tok 4.2255 (5.7089)	LR 2.000e-03
0: TRAIN [0][1790/1848]	Time 0.153 (0.131)	Data 4.29e-05 (5.08e-05)	Tok/s 32593 (28512)	Loss/tok 4.4067 (5.7012)	LR 2.000e-03
0: TRAIN [0][1800/1848]	Time 0.122 (0.130)	Data 4.60e-05 (5.09e-05)	Tok/s 29988 (28500)	Loss/tok 4.3064 (5.6948)	LR 2.000e-03
0: TRAIN [0][1810/1848]	Time 0.125 (0.130)	Data 5.48e-05 (5.09e-05)	Tok/s 29740 (28506)	Loss/tok 4.3899 (5.6874)	LR 2.000e-03
0: TRAIN [0][1820/1848]	Time 0.153 (0.131)	Data 6.96e-05 (5.09e-05)	Tok/s 33048 (28512)	Loss/tok 4.5868 (5.6799)	LR 2.000e-03
0: TRAIN [0][1830/1848]	Time 0.065 (0.130)	Data 6.89e-05 (5.09e-05)	Tok/s 16273 (28498)	Loss/tok 3.8231 (5.6738)	LR 2.000e-03
0: TRAIN [0][1840/1848]	Time 0.127 (0.131)	Data 5.51e-05 (5.09e-05)	Tok/s 28536 (28511)	Loss/tok 4.3847 (5.6658)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/213]	Time 0.069 (0.000)	Data 9.07e-04 (0.00e+00)	Tok/s 64497 (0)	Loss/tok 6.0403 (6.0403)
0: VALIDATION [0][10/213]	Time 0.036 (0.040)	Data 7.71e-04 (7.84e-04)	Tok/s 76537 (76419)	Loss/tok 5.7067 (5.7784)
0: VALIDATION [0][20/213]	Time 0.030 (0.037)	Data 7.65e-04 (7.92e-04)	Tok/s 77800 (76495)	Loss/tok 5.7704 (5.6952)
0: VALIDATION [0][30/213]	Time 0.028 (0.034)	Data 7.59e-04 (7.84e-04)	Tok/s 76168 (76878)	Loss/tok 5.2775 (5.6564)
0: VALIDATION [0][40/213]	Time 0.027 (0.032)	Data 7.48e-04 (7.80e-04)	Tok/s 73989 (76599)	Loss/tok 5.6642 (5.6288)
0: VALIDATION [0][50/213]	Time 0.024 (0.031)	Data 7.44e-04 (7.75e-04)	Tok/s 76601 (76622)	Loss/tok 5.9632 (5.6075)
0: VALIDATION [0][60/213]	Time 0.022 (0.029)	Data 7.44e-04 (7.70e-04)	Tok/s 75589 (76587)	Loss/tok 5.1005 (5.5721)
0: VALIDATION [0][70/213]	Time 0.020 (0.028)	Data 7.45e-04 (7.67e-04)	Tok/s 78668 (76511)	Loss/tok 5.3605 (5.5443)
0: VALIDATION [0][80/213]	Time 0.020 (0.027)	Data 7.41e-04 (7.64e-04)	Tok/s 73930 (76336)	Loss/tok 5.3381 (5.5229)
0: VALIDATION [0][90/213]	Time 0.019 (0.026)	Data 7.41e-04 (7.62e-04)	Tok/s 70597 (76036)	Loss/tok 5.4546 (5.5063)
0: VALIDATION [0][100/213]	Time 0.019 (0.025)	Data 7.48e-04 (7.62e-04)	Tok/s 67926 (75680)	Loss/tok 4.9718 (5.4839)
0: VALIDATION [0][110/213]	Time 0.016 (0.025)	Data 7.50e-04 (7.60e-04)	Tok/s 73573 (75425)	Loss/tok 5.1617 (5.4710)
0: VALIDATION [0][120/213]	Time 0.015 (0.024)	Data 7.57e-04 (7.59e-04)	Tok/s 72939 (75177)	Loss/tok 5.1065 (5.4550)
0: VALIDATION [0][130/213]	Time 0.015 (0.023)	Data 7.51e-04 (7.58e-04)	Tok/s 69631 (74785)	Loss/tok 5.2141 (5.4415)
0: VALIDATION [0][140/213]	Time 0.014 (0.023)	Data 7.34e-04 (7.56e-04)	Tok/s 68469 (74303)	Loss/tok 4.9109 (5.4276)
0: VALIDATION [0][150/213]	Time 0.013 (0.022)	Data 7.30e-04 (7.55e-04)	Tok/s 69144 (73980)	Loss/tok 5.3556 (5.4174)
0: VALIDATION [0][160/213]	Time 0.012 (0.021)	Data 7.33e-04 (7.54e-04)	Tok/s 66727 (73601)	Loss/tok 5.0569 (5.4049)
0: VALIDATION [0][170/213]	Time 0.012 (0.021)	Data 7.30e-04 (7.53e-04)	Tok/s 60687 (73128)	Loss/tok 4.6844 (5.3924)
0: VALIDATION [0][180/213]	Time 0.010 (0.020)	Data 7.36e-04 (7.52e-04)	Tok/s 64646 (72696)	Loss/tok 5.2516 (5.3853)
0: VALIDATION [0][190/213]	Time 0.010 (0.020)	Data 7.27e-04 (7.51e-04)	Tok/s 59346 (72117)	Loss/tok 5.1331 (5.3734)
0: VALIDATION [0][200/213]	Time 0.008 (0.019)	Data 7.26e-04 (7.50e-04)	Tok/s 58039 (71449)	Loss/tok 4.6555 (5.3591)
0: VALIDATION [0][210/213]	Time 0.007 (0.019)	Data 7.34e-04 (7.49e-04)	Tok/s 47938 (70616)	Loss/tok 4.4670 (5.3473)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/126]	Time 0.3350 (0.3616)	Decoder iters 149.0 (149.0)	Tok/s 7797 (8607)
0: TEST [0][19/126]	Time 0.3110 (0.3244)	Decoder iters 149.0 (139.7)	Tok/s 7065 (8510)
0: TEST [0][29/126]	Time 0.3041 (0.3032)	Decoder iters 149.0 (133.6)	Tok/s 6524 (8447)
0: TEST [0][39/126]	Time 0.1419 (0.2868)	Decoder iters 59.0 (128.5)	Tok/s 11936 (8415)
0: TEST [0][49/126]	Time 0.1322 (0.2738)	Decoder iters 55.0 (124.3)	Tok/s 11381 (8356)
0: TEST [0][59/126]	Time 0.2810 (0.2570)	Decoder iters 149.0 (117.2)	Tok/s 5171 (8482)
0: TEST [0][69/126]	Time 0.1254 (0.2441)	Decoder iters 57.0 (111.9)	Tok/s 9507 (8538)
0: TEST [0][79/126]	Time 0.2769 (0.2341)	Decoder iters 149.0 (107.9)	Tok/s 3832 (8517)
0: TEST [0][89/126]	Time 0.0943 (0.2206)	Decoder iters 42.0 (101.7)	Tok/s 10266 (8695)
0: TEST [0][99/126]	Time 0.0668 (0.2109)	Decoder iters 28.0 (97.6)	Tok/s 12132 (8739)
0: TEST [0][109/126]	Time 0.0773 (0.2002)	Decoder iters 35.0 (92.8)	Tok/s 9108 (8840)
0: TEST [0][119/126]	Time 0.0578 (0.1885)	Decoder iters 26.0 (87.2)	Tok/s 8862 (8962)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 5.6604	Validation Loss: 5.3458	Test BLEU: 5.25
0: Performance: Epoch: 0	Training: 28520 Tok/s	Validation: 70294 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3588440356
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/1848]	Time 0.152 (0.000)	Data 8.75e-02 (0.00e+00)	Tok/s 13770 (0)	Loss/tok 3.6146 (3.6146)	LR 2.000e-03
0: TRAIN [1][10/1848]	Time 0.092 (0.112)	Data 7.20e-05 (5.58e-05)	Tok/s 22945 (26542)	Loss/tok 3.6874 (4.0079)	LR 2.000e-03
0: TRAIN [1][20/1848]	Time 0.154 (0.130)	Data 7.61e-05 (5.93e-05)	Tok/s 32350 (28505)	Loss/tok 4.1295 (4.1266)	LR 2.000e-03
0: TRAIN [1][30/1848]	Time 0.123 (0.124)	Data 8.49e-05 (6.22e-05)	Tok/s 29481 (27882)	Loss/tok 3.9898 (4.0635)	LR 2.000e-03
0: TRAIN [1][40/1848]	Time 0.156 (0.122)	Data 4.51e-05 (5.89e-05)	Tok/s 32118 (27541)	Loss/tok 4.2002 (4.0402)	LR 2.000e-03
0: TRAIN [1][50/1848]	Time 0.125 (0.126)	Data 4.74e-05 (5.77e-05)	Tok/s 28804 (28012)	Loss/tok 3.9195 (4.0895)	LR 2.000e-03
0: TRAIN [1][60/1848]	Time 0.097 (0.127)	Data 8.08e-05 (5.69e-05)	Tok/s 22307 (28167)	Loss/tok 3.5312 (4.0913)	LR 2.000e-03
0: TRAIN [1][70/1848]	Time 0.188 (0.127)	Data 4.41e-05 (5.55e-05)	Tok/s 34911 (28206)	Loss/tok 4.3611 (4.0957)	LR 2.000e-03
0: TRAIN [1][80/1848]	Time 0.094 (0.125)	Data 5.03e-05 (5.47e-05)	Tok/s 22454 (27724)	Loss/tok 3.8160 (4.0835)	LR 2.000e-03
0: TRAIN [1][90/1848]	Time 0.128 (0.125)	Data 4.58e-05 (5.45e-05)	Tok/s 28425 (27809)	Loss/tok 3.9163 (4.0746)	LR 2.000e-03
0: TRAIN [1][100/1848]	Time 0.120 (0.124)	Data 4.41e-05 (5.46e-05)	Tok/s 30100 (27702)	Loss/tok 4.1433 (4.0752)	LR 2.000e-03
0: TRAIN [1][110/1848]	Time 0.097 (0.126)	Data 5.79e-05 (5.41e-05)	Tok/s 21804 (27927)	Loss/tok 3.6634 (4.0979)	LR 2.000e-03
0: TRAIN [1][120/1848]	Time 0.154 (0.128)	Data 4.60e-05 (5.34e-05)	Tok/s 33052 (28169)	Loss/tok 4.1518 (4.1126)	LR 2.000e-03
0: TRAIN [1][130/1848]	Time 0.124 (0.129)	Data 4.53e-05 (5.32e-05)	Tok/s 29570 (28321)	Loss/tok 4.0484 (4.1160)	LR 2.000e-03
0: TRAIN [1][140/1848]	Time 0.094 (0.127)	Data 7.37e-05 (5.32e-05)	Tok/s 23607 (28010)	Loss/tok 3.8910 (4.1049)	LR 2.000e-03
0: TRAIN [1][150/1848]	Time 0.097 (0.127)	Data 4.55e-05 (5.27e-05)	Tok/s 22588 (27943)	Loss/tok 3.6854 (4.1076)	LR 2.000e-03
0: TRAIN [1][160/1848]	Time 0.121 (0.126)	Data 7.61e-05 (5.29e-05)	Tok/s 30012 (27856)	Loss/tok 3.7529 (4.1017)	LR 2.000e-03
0: TRAIN [1][170/1848]	Time 0.153 (0.127)	Data 4.32e-05 (5.25e-05)	Tok/s 32476 (27945)	Loss/tok 4.2315 (4.1134)	LR 2.000e-03
0: TRAIN [1][180/1848]	Time 0.154 (0.127)	Data 4.51e-05 (5.25e-05)	Tok/s 32693 (28026)	Loss/tok 4.1519 (4.1132)	LR 2.000e-03
0: TRAIN [1][190/1848]	Time 0.070 (0.127)	Data 5.22e-05 (5.26e-05)	Tok/s 15555 (28026)	Loss/tok 3.3069 (4.1136)	LR 2.000e-03
0: TRAIN [1][200/1848]	Time 0.188 (0.127)	Data 4.53e-05 (5.22e-05)	Tok/s 34470 (27967)	Loss/tok 4.4289 (4.1107)	LR 2.000e-03
0: TRAIN [1][210/1848]	Time 0.070 (0.128)	Data 4.43e-05 (5.21e-05)	Tok/s 15269 (28034)	Loss/tok 3.3693 (4.1147)	LR 2.000e-03
0: TRAIN [1][220/1848]	Time 0.150 (0.127)	Data 7.27e-05 (5.20e-05)	Tok/s 33405 (28006)	Loss/tok 4.1553 (4.1100)	LR 2.000e-03
0: TRAIN [1][230/1848]	Time 0.152 (0.127)	Data 4.29e-05 (5.21e-05)	Tok/s 33248 (27926)	Loss/tok 4.0689 (4.1065)	LR 2.000e-03
0: TRAIN [1][240/1848]	Time 0.125 (0.127)	Data 4.77e-05 (5.20e-05)	Tok/s 28668 (28042)	Loss/tok 3.8670 (4.1071)	LR 2.000e-03
0: TRAIN [1][250/1848]	Time 0.091 (0.126)	Data 6.82e-05 (5.19e-05)	Tok/s 24093 (27889)	Loss/tok 3.6533 (4.0980)	LR 2.000e-03
0: TRAIN [1][260/1848]	Time 0.096 (0.126)	Data 4.77e-05 (5.18e-05)	Tok/s 22251 (27886)	Loss/tok 3.5763 (4.0972)	LR 2.000e-03
0: TRAIN [1][270/1848]	Time 0.154 (0.128)	Data 5.01e-05 (5.18e-05)	Tok/s 33562 (28043)	Loss/tok 4.2023 (4.1053)	LR 2.000e-03
0: TRAIN [1][280/1848]	Time 0.189 (0.128)	Data 4.48e-05 (5.16e-05)	Tok/s 34551 (28059)	Loss/tok 4.2874 (4.1026)	LR 2.000e-03
0: TRAIN [1][290/1848]	Time 0.097 (0.128)	Data 4.43e-05 (5.15e-05)	Tok/s 22559 (28065)	Loss/tok 3.7648 (4.1034)	LR 2.000e-03
0: TRAIN [1][300/1848]	Time 0.094 (0.128)	Data 8.08e-05 (5.18e-05)	Tok/s 23386 (28042)	Loss/tok 3.7734 (4.1006)	LR 2.000e-03
0: TRAIN [1][310/1848]	Time 0.097 (0.128)	Data 4.60e-05 (5.18e-05)	Tok/s 22808 (28012)	Loss/tok 3.6993 (4.0998)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][320/1848]	Time 0.155 (0.128)	Data 4.53e-05 (5.19e-05)	Tok/s 32398 (28105)	Loss/tok 4.3890 (4.1043)	LR 2.000e-03
0: TRAIN [1][330/1848]	Time 0.122 (0.128)	Data 5.63e-05 (5.18e-05)	Tok/s 29578 (28149)	Loss/tok 3.8267 (4.1054)	LR 2.000e-03
0: TRAIN [1][340/1848]	Time 0.099 (0.128)	Data 4.48e-05 (5.16e-05)	Tok/s 21752 (28096)	Loss/tok 3.5236 (4.1025)	LR 2.000e-03
0: TRAIN [1][350/1848]	Time 0.126 (0.128)	Data 4.51e-05 (5.17e-05)	Tok/s 28305 (28122)	Loss/tok 3.9220 (4.1013)	LR 2.000e-03
0: TRAIN [1][360/1848]	Time 0.125 (0.128)	Data 4.72e-05 (5.17e-05)	Tok/s 29437 (28118)	Loss/tok 3.9688 (4.1004)	LR 2.000e-03
0: TRAIN [1][370/1848]	Time 0.151 (0.128)	Data 4.53e-05 (5.18e-05)	Tok/s 33034 (28081)	Loss/tok 4.1032 (4.0964)	LR 2.000e-03
0: TRAIN [1][380/1848]	Time 0.151 (0.128)	Data 4.82e-05 (5.17e-05)	Tok/s 33525 (28158)	Loss/tok 4.1003 (4.0978)	LR 2.000e-03
0: TRAIN [1][390/1848]	Time 0.126 (0.128)	Data 4.51e-05 (5.15e-05)	Tok/s 28785 (28102)	Loss/tok 4.0076 (4.0972)	LR 2.000e-03
0: TRAIN [1][400/1848]	Time 0.099 (0.128)	Data 4.55e-05 (5.16e-05)	Tok/s 22103 (28139)	Loss/tok 3.7781 (4.0980)	LR 2.000e-03
0: TRAIN [1][410/1848]	Time 0.122 (0.128)	Data 5.10e-05 (5.17e-05)	Tok/s 29197 (28139)	Loss/tok 3.9123 (4.0981)	LR 2.000e-03
0: TRAIN [1][420/1848]	Time 0.125 (0.128)	Data 4.51e-05 (5.16e-05)	Tok/s 28980 (28147)	Loss/tok 3.9973 (4.0967)	LR 2.000e-03
0: TRAIN [1][430/1848]	Time 0.120 (0.128)	Data 9.25e-05 (5.17e-05)	Tok/s 30056 (28122)	Loss/tok 3.9224 (4.0981)	LR 2.000e-03
0: TRAIN [1][440/1848]	Time 0.125 (0.128)	Data 4.55e-05 (5.18e-05)	Tok/s 28697 (28100)	Loss/tok 3.9716 (4.0944)	LR 2.000e-03
0: TRAIN [1][450/1848]	Time 0.097 (0.128)	Data 4.63e-05 (5.18e-05)	Tok/s 23268 (28042)	Loss/tok 3.6936 (4.0942)	LR 2.000e-03
0: TRAIN [1][460/1848]	Time 0.191 (0.128)	Data 4.86e-05 (5.19e-05)	Tok/s 34652 (28109)	Loss/tok 4.3213 (4.0945)	LR 2.000e-03
0: TRAIN [1][470/1848]	Time 0.193 (0.128)	Data 4.58e-05 (5.19e-05)	Tok/s 33771 (28165)	Loss/tok 4.2765 (4.0939)	LR 2.000e-03
0: TRAIN [1][480/1848]	Time 0.119 (0.128)	Data 6.87e-05 (5.19e-05)	Tok/s 29860 (28148)	Loss/tok 3.9535 (4.0900)	LR 2.000e-03
0: TRAIN [1][490/1848]	Time 0.155 (0.129)	Data 4.79e-05 (5.19e-05)	Tok/s 32737 (28204)	Loss/tok 4.0398 (4.0904)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][500/1848]	Time 0.152 (0.128)	Data 4.70e-05 (5.18e-05)	Tok/s 33040 (28200)	Loss/tok 4.0971 (4.0881)	LR 2.000e-03
0: TRAIN [1][510/1848]	Time 0.127 (0.128)	Data 4.41e-05 (5.18e-05)	Tok/s 27972 (28172)	Loss/tok 3.9680 (4.0858)	LR 2.000e-03
0: TRAIN [1][520/1848]	Time 0.185 (0.128)	Data 5.46e-05 (5.17e-05)	Tok/s 35151 (28193)	Loss/tok 4.2841 (4.0849)	LR 2.000e-03
0: TRAIN [1][530/1848]	Time 0.065 (0.128)	Data 7.08e-05 (5.17e-05)	Tok/s 16454 (28179)	Loss/tok 3.3661 (4.0831)	LR 2.000e-03
0: TRAIN [1][540/1848]	Time 0.152 (0.128)	Data 7.53e-05 (5.19e-05)	Tok/s 32695 (28196)	Loss/tok 4.1957 (4.0846)	LR 2.000e-03
0: TRAIN [1][550/1848]	Time 0.127 (0.129)	Data 5.67e-05 (5.19e-05)	Tok/s 27870 (28234)	Loss/tok 3.7857 (4.0856)	LR 2.000e-03
0: TRAIN [1][560/1848]	Time 0.120 (0.129)	Data 8.27e-05 (5.19e-05)	Tok/s 29620 (28243)	Loss/tok 4.0706 (4.0845)	LR 2.000e-03
0: TRAIN [1][570/1848]	Time 0.092 (0.129)	Data 7.34e-05 (5.19e-05)	Tok/s 23353 (28222)	Loss/tok 3.8828 (4.0836)	LR 2.000e-03
0: TRAIN [1][580/1848]	Time 0.125 (0.129)	Data 7.94e-05 (5.19e-05)	Tok/s 28528 (28222)	Loss/tok 3.9613 (4.0832)	LR 2.000e-03
0: TRAIN [1][590/1848]	Time 0.097 (0.129)	Data 4.53e-05 (5.19e-05)	Tok/s 23423 (28187)	Loss/tok 3.6263 (4.0814)	LR 2.000e-03
0: TRAIN [1][600/1848]	Time 0.126 (0.128)	Data 5.05e-05 (5.19e-05)	Tok/s 28812 (28166)	Loss/tok 4.0806 (4.0789)	LR 2.000e-03
0: TRAIN [1][610/1848]	Time 0.094 (0.128)	Data 4.70e-05 (5.18e-05)	Tok/s 23597 (28156)	Loss/tok 3.8663 (4.0788)	LR 2.000e-03
0: TRAIN [1][620/1848]	Time 0.070 (0.128)	Data 4.41e-05 (5.18e-05)	Tok/s 15219 (28168)	Loss/tok 3.4844 (4.0800)	LR 1.000e-03
0: TRAIN [1][630/1848]	Time 0.127 (0.128)	Data 4.51e-05 (5.17e-05)	Tok/s 28256 (28096)	Loss/tok 4.0569 (4.0775)	LR 1.000e-03
0: TRAIN [1][640/1848]	Time 0.125 (0.128)	Data 4.51e-05 (5.17e-05)	Tok/s 29301 (28109)	Loss/tok 3.8817 (4.0773)	LR 1.000e-03
0: TRAIN [1][650/1848]	Time 0.123 (0.128)	Data 4.74e-05 (5.17e-05)	Tok/s 29755 (28124)	Loss/tok 3.8068 (4.0757)	LR 1.000e-03
0: TRAIN [1][660/1848]	Time 0.126 (0.128)	Data 4.36e-05 (5.16e-05)	Tok/s 28648 (28146)	Loss/tok 3.8980 (4.0753)	LR 1.000e-03
0: TRAIN [1][670/1848]	Time 0.155 (0.128)	Data 4.46e-05 (5.17e-05)	Tok/s 32956 (28162)	Loss/tok 4.0581 (4.0741)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][680/1848]	Time 0.095 (0.128)	Data 4.48e-05 (5.17e-05)	Tok/s 22283 (28174)	Loss/tok 3.6842 (4.0725)	LR 1.000e-03
0: TRAIN [1][690/1848]	Time 0.157 (0.128)	Data 4.63e-05 (5.17e-05)	Tok/s 32321 (28169)	Loss/tok 4.0229 (4.0699)	LR 1.000e-03
0: TRAIN [1][700/1848]	Time 0.095 (0.128)	Data 6.99e-05 (5.18e-05)	Tok/s 23252 (28143)	Loss/tok 3.5494 (4.0671)	LR 1.000e-03
0: TRAIN [1][710/1848]	Time 0.154 (0.128)	Data 7.46e-05 (5.19e-05)	Tok/s 32720 (28161)	Loss/tok 4.1938 (4.0658)	LR 1.000e-03
0: TRAIN [1][720/1848]	Time 0.123 (0.128)	Data 7.22e-05 (5.20e-05)	Tok/s 28941 (28184)	Loss/tok 3.8825 (4.0642)	LR 1.000e-03
0: TRAIN [1][730/1848]	Time 0.094 (0.128)	Data 4.48e-05 (5.21e-05)	Tok/s 23482 (28178)	Loss/tok 3.5138 (4.0617)	LR 1.000e-03
0: TRAIN [1][740/1848]	Time 0.124 (0.128)	Data 4.39e-05 (5.21e-05)	Tok/s 29237 (28178)	Loss/tok 3.7687 (4.0589)	LR 1.000e-03
0: TRAIN [1][750/1848]	Time 0.094 (0.128)	Data 4.36e-05 (5.21e-05)	Tok/s 23451 (28160)	Loss/tok 3.6229 (4.0562)	LR 1.000e-03
0: TRAIN [1][760/1848]	Time 0.096 (0.128)	Data 5.32e-05 (5.22e-05)	Tok/s 22179 (28114)	Loss/tok 3.6023 (4.0535)	LR 1.000e-03
0: TRAIN [1][770/1848]	Time 0.189 (0.128)	Data 7.15e-05 (5.23e-05)	Tok/s 34558 (28138)	Loss/tok 4.3383 (4.0524)	LR 1.000e-03
0: TRAIN [1][780/1848]	Time 0.153 (0.128)	Data 4.46e-05 (5.23e-05)	Tok/s 32973 (28135)	Loss/tok 3.9907 (4.0495)	LR 1.000e-03
0: TRAIN [1][790/1848]	Time 0.094 (0.128)	Data 5.79e-05 (5.24e-05)	Tok/s 23164 (28144)	Loss/tok 3.5010 (4.0504)	LR 1.000e-03
0: TRAIN [1][800/1848]	Time 0.157 (0.128)	Data 4.72e-05 (5.24e-05)	Tok/s 32816 (28151)	Loss/tok 4.0261 (4.0485)	LR 1.000e-03
0: TRAIN [1][810/1848]	Time 0.153 (0.128)	Data 4.51e-05 (5.24e-05)	Tok/s 32838 (28158)	Loss/tok 4.0693 (4.0483)	LR 1.000e-03
0: TRAIN [1][820/1848]	Time 0.094 (0.128)	Data 1.03e-04 (5.25e-05)	Tok/s 22678 (28155)	Loss/tok 3.5170 (4.0458)	LR 1.000e-03
0: TRAIN [1][830/1848]	Time 0.124 (0.128)	Data 4.51e-05 (5.25e-05)	Tok/s 29581 (28133)	Loss/tok 3.7234 (4.0432)	LR 1.000e-03
0: TRAIN [1][840/1848]	Time 0.095 (0.128)	Data 4.70e-05 (5.25e-05)	Tok/s 23095 (28131)	Loss/tok 3.4690 (4.0424)	LR 1.000e-03
0: TRAIN [1][850/1848]	Time 0.123 (0.128)	Data 4.51e-05 (5.26e-05)	Tok/s 29234 (28098)	Loss/tok 3.6710 (4.0398)	LR 1.000e-03
0: TRAIN [1][860/1848]	Time 0.154 (0.128)	Data 6.99e-05 (5.27e-05)	Tok/s 33161 (28126)	Loss/tok 4.0245 (4.0381)	LR 1.000e-03
0: TRAIN [1][870/1848]	Time 0.125 (0.128)	Data 5.27e-05 (5.27e-05)	Tok/s 28746 (28159)	Loss/tok 3.8210 (4.0375)	LR 1.000e-03
0: TRAIN [1][880/1848]	Time 0.123 (0.128)	Data 6.84e-05 (5.27e-05)	Tok/s 30025 (28167)	Loss/tok 3.6463 (4.0348)	LR 1.000e-03
0: TRAIN [1][890/1848]	Time 0.125 (0.128)	Data 4.43e-05 (5.28e-05)	Tok/s 28854 (28187)	Loss/tok 3.8266 (4.0331)	LR 1.000e-03
0: TRAIN [1][900/1848]	Time 0.154 (0.128)	Data 8.25e-05 (5.29e-05)	Tok/s 33077 (28178)	Loss/tok 4.1098 (4.0306)	LR 1.000e-03
0: TRAIN [1][910/1848]	Time 0.123 (0.128)	Data 4.34e-05 (5.29e-05)	Tok/s 29318 (28193)	Loss/tok 3.6999 (4.0286)	LR 1.000e-03
0: TRAIN [1][920/1848]	Time 0.097 (0.128)	Data 4.43e-05 (5.29e-05)	Tok/s 22389 (28205)	Loss/tok 3.6103 (4.0269)	LR 5.000e-04
0: TRAIN [1][930/1848]	Time 0.123 (0.129)	Data 7.89e-05 (5.29e-05)	Tok/s 30255 (28209)	Loss/tok 3.8032 (4.0263)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][940/1848]	Time 0.192 (0.129)	Data 4.43e-05 (5.29e-05)	Tok/s 33839 (28256)	Loss/tok 4.2247 (4.0255)	LR 5.000e-04
0: TRAIN [1][950/1848]	Time 0.097 (0.129)	Data 5.05e-05 (5.29e-05)	Tok/s 23088 (28278)	Loss/tok 3.3481 (4.0239)	LR 5.000e-04
0: TRAIN [1][960/1848]	Time 0.150 (0.129)	Data 5.60e-05 (5.29e-05)	Tok/s 33372 (28261)	Loss/tok 3.7034 (4.0214)	LR 5.000e-04
0: TRAIN [1][970/1848]	Time 0.155 (0.129)	Data 4.46e-05 (5.28e-05)	Tok/s 33030 (28258)	Loss/tok 3.9358 (4.0193)	LR 5.000e-04
0: TRAIN [1][980/1848]	Time 0.188 (0.129)	Data 4.41e-05 (5.28e-05)	Tok/s 34479 (28276)	Loss/tok 4.0321 (4.0188)	LR 5.000e-04
0: TRAIN [1][990/1848]	Time 0.154 (0.129)	Data 7.53e-05 (5.28e-05)	Tok/s 32782 (28276)	Loss/tok 3.8732 (4.0162)	LR 5.000e-04
0: TRAIN [1][1000/1848]	Time 0.125 (0.129)	Data 4.48e-05 (5.27e-05)	Tok/s 28657 (28274)	Loss/tok 3.7352 (4.0139)	LR 5.000e-04
0: TRAIN [1][1010/1848]	Time 0.097 (0.129)	Data 4.58e-05 (5.27e-05)	Tok/s 21817 (28258)	Loss/tok 3.4099 (4.0128)	LR 5.000e-04
0: TRAIN [1][1020/1848]	Time 0.125 (0.129)	Data 5.67e-05 (5.27e-05)	Tok/s 28065 (28279)	Loss/tok 3.6992 (4.0112)	LR 5.000e-04
0: TRAIN [1][1030/1848]	Time 0.155 (0.129)	Data 4.46e-05 (5.27e-05)	Tok/s 32808 (28304)	Loss/tok 3.9922 (4.0099)	LR 5.000e-04
0: TRAIN [1][1040/1848]	Time 0.153 (0.129)	Data 4.53e-05 (5.28e-05)	Tok/s 32839 (28309)	Loss/tok 3.9050 (4.0085)	LR 5.000e-04
0: TRAIN [1][1050/1848]	Time 0.192 (0.130)	Data 5.25e-05 (5.27e-05)	Tok/s 34130 (28333)	Loss/tok 4.0527 (4.0079)	LR 5.000e-04
0: TRAIN [1][1060/1848]	Time 0.122 (0.130)	Data 4.41e-05 (5.26e-05)	Tok/s 29229 (28339)	Loss/tok 3.5799 (4.0060)	LR 5.000e-04
0: TRAIN [1][1070/1848]	Time 0.157 (0.130)	Data 4.70e-05 (5.27e-05)	Tok/s 31698 (28360)	Loss/tok 4.0963 (4.0049)	LR 5.000e-04
0: TRAIN [1][1080/1848]	Time 0.123 (0.130)	Data 4.77e-05 (5.27e-05)	Tok/s 28748 (28372)	Loss/tok 3.8166 (4.0033)	LR 5.000e-04
0: TRAIN [1][1090/1848]	Time 0.154 (0.130)	Data 4.89e-05 (5.27e-05)	Tok/s 33023 (28376)	Loss/tok 3.8595 (4.0014)	LR 5.000e-04
0: TRAIN [1][1100/1848]	Time 0.122 (0.130)	Data 4.36e-05 (5.26e-05)	Tok/s 29293 (28359)	Loss/tok 3.5962 (3.9986)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1110/1848]	Time 0.097 (0.130)	Data 5.13e-05 (5.26e-05)	Tok/s 21672 (28385)	Loss/tok 3.4790 (3.9968)	LR 5.000e-04
0: TRAIN [1][1120/1848]	Time 0.097 (0.130)	Data 4.51e-05 (5.26e-05)	Tok/s 22165 (28361)	Loss/tok 3.4324 (3.9945)	LR 5.000e-04
0: TRAIN [1][1130/1848]	Time 0.123 (0.129)	Data 4.74e-05 (5.26e-05)	Tok/s 30009 (28357)	Loss/tok 3.5761 (3.9921)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1140/1848]	Time 0.155 (0.129)	Data 6.89e-05 (5.25e-05)	Tok/s 32679 (28369)	Loss/tok 3.9221 (3.9900)	LR 5.000e-04
0: TRAIN [1][1150/1848]	Time 0.097 (0.129)	Data 4.48e-05 (5.25e-05)	Tok/s 22774 (28374)	Loss/tok 3.4165 (3.9887)	LR 5.000e-04
0: TRAIN [1][1160/1848]	Time 0.156 (0.129)	Data 5.15e-05 (5.25e-05)	Tok/s 32340 (28375)	Loss/tok 4.0063 (3.9872)	LR 5.000e-04
0: TRAIN [1][1170/1848]	Time 0.096 (0.130)	Data 4.55e-05 (5.25e-05)	Tok/s 22537 (28389)	Loss/tok 3.5553 (3.9863)	LR 5.000e-04
0: TRAIN [1][1180/1848]	Time 0.125 (0.130)	Data 4.55e-05 (5.25e-05)	Tok/s 29063 (28393)	Loss/tok 3.6672 (3.9845)	LR 5.000e-04
0: TRAIN [1][1190/1848]	Time 0.189 (0.130)	Data 5.08e-05 (5.26e-05)	Tok/s 34746 (28393)	Loss/tok 3.9510 (3.9824)	LR 5.000e-04
0: TRAIN [1][1200/1848]	Time 0.097 (0.129)	Data 4.46e-05 (5.25e-05)	Tok/s 23204 (28373)	Loss/tok 3.4221 (3.9801)	LR 5.000e-04
0: TRAIN [1][1210/1848]	Time 0.097 (0.129)	Data 4.63e-05 (5.25e-05)	Tok/s 22445 (28380)	Loss/tok 3.3344 (3.9778)	LR 5.000e-04
0: TRAIN [1][1220/1848]	Time 0.093 (0.129)	Data 4.82e-05 (5.25e-05)	Tok/s 22750 (28372)	Loss/tok 3.5852 (3.9760)	LR 5.000e-04
0: TRAIN [1][1230/1848]	Time 0.097 (0.129)	Data 4.43e-05 (5.25e-05)	Tok/s 23007 (28370)	Loss/tok 3.2727 (3.9740)	LR 2.500e-04
0: TRAIN [1][1240/1848]	Time 0.126 (0.129)	Data 4.94e-05 (5.24e-05)	Tok/s 28746 (28380)	Loss/tok 3.7046 (3.9729)	LR 2.500e-04
0: TRAIN [1][1250/1848]	Time 0.095 (0.130)	Data 7.44e-05 (5.25e-05)	Tok/s 23000 (28393)	Loss/tok 3.3724 (3.9721)	LR 2.500e-04
0: TRAIN [1][1260/1848]	Time 0.097 (0.130)	Data 6.84e-05 (5.25e-05)	Tok/s 22511 (28402)	Loss/tok 3.4198 (3.9715)	LR 2.500e-04
0: TRAIN [1][1270/1848]	Time 0.190 (0.130)	Data 4.51e-05 (5.25e-05)	Tok/s 34572 (28401)	Loss/tok 3.9959 (3.9702)	LR 2.500e-04
0: TRAIN [1][1280/1848]	Time 0.123 (0.130)	Data 4.51e-05 (5.24e-05)	Tok/s 28813 (28398)	Loss/tok 3.6665 (3.9683)	LR 2.500e-04
0: TRAIN [1][1290/1848]	Time 0.068 (0.130)	Data 6.94e-05 (5.25e-05)	Tok/s 15527 (28405)	Loss/tok 3.2819 (3.9668)	LR 2.500e-04
0: TRAIN [1][1300/1848]	Time 0.123 (0.130)	Data 4.70e-05 (5.25e-05)	Tok/s 28879 (28422)	Loss/tok 3.5813 (3.9653)	LR 2.500e-04
0: TRAIN [1][1310/1848]	Time 0.155 (0.130)	Data 6.89e-05 (5.25e-05)	Tok/s 33189 (28442)	Loss/tok 3.8464 (3.9635)	LR 2.500e-04
0: TRAIN [1][1320/1848]	Time 0.121 (0.130)	Data 4.41e-05 (5.25e-05)	Tok/s 28685 (28444)	Loss/tok 3.7069 (3.9616)	LR 2.500e-04
0: TRAIN [1][1330/1848]	Time 0.123 (0.130)	Data 4.43e-05 (5.25e-05)	Tok/s 29424 (28426)	Loss/tok 3.5745 (3.9600)	LR 2.500e-04
0: TRAIN [1][1340/1848]	Time 0.150 (0.130)	Data 7.18e-05 (5.25e-05)	Tok/s 33148 (28436)	Loss/tok 3.8649 (3.9588)	LR 2.500e-04
0: TRAIN [1][1350/1848]	Time 0.128 (0.130)	Data 4.51e-05 (5.24e-05)	Tok/s 28044 (28447)	Loss/tok 3.6860 (3.9574)	LR 2.500e-04
0: TRAIN [1][1360/1848]	Time 0.127 (0.130)	Data 4.91e-05 (5.25e-05)	Tok/s 28275 (28469)	Loss/tok 3.8163 (3.9567)	LR 2.500e-04
0: TRAIN [1][1370/1848]	Time 0.125 (0.130)	Data 5.22e-05 (5.24e-05)	Tok/s 29222 (28487)	Loss/tok 3.6305 (3.9552)	LR 2.500e-04
0: TRAIN [1][1380/1848]	Time 0.095 (0.130)	Data 4.55e-05 (5.24e-05)	Tok/s 22478 (28483)	Loss/tok 3.3503 (3.9538)	LR 2.500e-04
0: TRAIN [1][1390/1848]	Time 0.125 (0.130)	Data 4.98e-05 (5.24e-05)	Tok/s 28175 (28474)	Loss/tok 3.6578 (3.9516)	LR 2.500e-04
0: TRAIN [1][1400/1848]	Time 0.153 (0.130)	Data 4.48e-05 (5.24e-05)	Tok/s 32557 (28482)	Loss/tok 3.9043 (3.9498)	LR 2.500e-04
0: TRAIN [1][1410/1848]	Time 0.188 (0.130)	Data 4.43e-05 (5.24e-05)	Tok/s 34407 (28489)	Loss/tok 3.9257 (3.9485)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1420/1848]	Time 0.125 (0.130)	Data 4.74e-05 (5.24e-05)	Tok/s 28617 (28500)	Loss/tok 3.5107 (3.9467)	LR 2.500e-04
0: TRAIN [1][1430/1848]	Time 0.127 (0.130)	Data 4.51e-05 (5.24e-05)	Tok/s 28693 (28502)	Loss/tok 3.5446 (3.9447)	LR 2.500e-04
0: TRAIN [1][1440/1848]	Time 0.192 (0.130)	Data 4.53e-05 (5.24e-05)	Tok/s 34390 (28510)	Loss/tok 4.1342 (3.9434)	LR 2.500e-04
0: TRAIN [1][1450/1848]	Time 0.122 (0.130)	Data 5.60e-05 (5.24e-05)	Tok/s 29749 (28476)	Loss/tok 3.7458 (3.9416)	LR 2.500e-04
0: TRAIN [1][1460/1848]	Time 0.121 (0.130)	Data 6.94e-05 (5.24e-05)	Tok/s 29936 (28489)	Loss/tok 3.5747 (3.9411)	LR 2.500e-04
0: TRAIN [1][1470/1848]	Time 0.097 (0.130)	Data 5.53e-05 (5.24e-05)	Tok/s 21820 (28493)	Loss/tok 3.3303 (3.9399)	LR 2.500e-04
0: TRAIN [1][1480/1848]	Time 0.125 (0.130)	Data 5.17e-05 (5.24e-05)	Tok/s 29028 (28499)	Loss/tok 3.6116 (3.9385)	LR 2.500e-04
0: TRAIN [1][1490/1848]	Time 0.155 (0.130)	Data 4.41e-05 (5.24e-05)	Tok/s 33263 (28498)	Loss/tok 3.6988 (3.9370)	LR 2.500e-04
0: TRAIN [1][1500/1848]	Time 0.094 (0.130)	Data 4.53e-05 (5.24e-05)	Tok/s 23613 (28485)	Loss/tok 3.4149 (3.9357)	LR 2.500e-04
0: TRAIN [1][1510/1848]	Time 0.189 (0.130)	Data 1.27e-04 (5.24e-05)	Tok/s 34357 (28488)	Loss/tok 4.0691 (3.9346)	LR 2.500e-04
0: TRAIN [1][1520/1848]	Time 0.095 (0.130)	Data 4.43e-05 (5.24e-05)	Tok/s 23145 (28480)	Loss/tok 3.3461 (3.9327)	LR 2.500e-04
0: TRAIN [1][1530/1848]	Time 0.155 (0.130)	Data 4.60e-05 (5.24e-05)	Tok/s 32377 (28474)	Loss/tok 3.8085 (3.9312)	LR 2.500e-04
0: TRAIN [1][1540/1848]	Time 0.124 (0.130)	Data 4.41e-05 (5.24e-05)	Tok/s 28749 (28482)	Loss/tok 3.7238 (3.9300)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1550/1848]	Time 0.095 (0.130)	Data 6.96e-05 (5.24e-05)	Tok/s 22432 (28484)	Loss/tok 3.5918 (3.9286)	LR 1.250e-04
0: TRAIN [1][1560/1848]	Time 0.099 (0.130)	Data 5.34e-05 (5.24e-05)	Tok/s 22180 (28472)	Loss/tok 3.4425 (3.9274)	LR 1.250e-04
0: TRAIN [1][1570/1848]	Time 0.187 (0.130)	Data 4.48e-05 (5.24e-05)	Tok/s 34862 (28488)	Loss/tok 4.0006 (3.9268)	LR 1.250e-04
0: TRAIN [1][1580/1848]	Time 0.126 (0.130)	Data 4.91e-05 (5.24e-05)	Tok/s 29057 (28495)	Loss/tok 3.7799 (3.9255)	LR 1.250e-04
0: TRAIN [1][1590/1848]	Time 0.068 (0.130)	Data 7.80e-05 (5.24e-05)	Tok/s 16411 (28472)	Loss/tok 3.0674 (3.9241)	LR 1.250e-04
0: TRAIN [1][1600/1848]	Time 0.193 (0.130)	Data 4.58e-05 (5.24e-05)	Tok/s 34230 (28484)	Loss/tok 3.9720 (3.9232)	LR 1.250e-04
0: TRAIN [1][1610/1848]	Time 0.098 (0.130)	Data 4.53e-05 (5.25e-05)	Tok/s 22024 (28476)	Loss/tok 3.4720 (3.9215)	LR 1.250e-04
0: TRAIN [1][1620/1848]	Time 0.126 (0.130)	Data 5.60e-05 (5.25e-05)	Tok/s 27516 (28501)	Loss/tok 3.6130 (3.9213)	LR 1.250e-04
0: TRAIN [1][1630/1848]	Time 0.187 (0.130)	Data 4.51e-05 (5.24e-05)	Tok/s 35284 (28503)	Loss/tok 3.9438 (3.9202)	LR 1.250e-04
0: TRAIN [1][1640/1848]	Time 0.122 (0.130)	Data 4.39e-05 (5.24e-05)	Tok/s 28864 (28496)	Loss/tok 3.5873 (3.9188)	LR 1.250e-04
0: TRAIN [1][1650/1848]	Time 0.097 (0.130)	Data 5.58e-05 (5.24e-05)	Tok/s 22508 (28494)	Loss/tok 3.4286 (3.9173)	LR 1.250e-04
0: TRAIN [1][1660/1848]	Time 0.154 (0.130)	Data 4.48e-05 (5.24e-05)	Tok/s 32463 (28484)	Loss/tok 3.7876 (3.9162)	LR 1.250e-04
0: TRAIN [1][1670/1848]	Time 0.097 (0.130)	Data 4.84e-05 (5.24e-05)	Tok/s 22270 (28476)	Loss/tok 3.3770 (3.9146)	LR 1.250e-04
0: TRAIN [1][1680/1848]	Time 0.124 (0.130)	Data 1.04e-04 (5.24e-05)	Tok/s 29184 (28482)	Loss/tok 3.6354 (3.9143)	LR 1.250e-04
0: TRAIN [1][1690/1848]	Time 0.157 (0.130)	Data 4.39e-05 (5.24e-05)	Tok/s 32483 (28477)	Loss/tok 3.7845 (3.9131)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][1700/1848]	Time 0.124 (0.130)	Data 4.65e-05 (5.24e-05)	Tok/s 39588 (28505)	Loss/tok 3.9776 (3.9125)	LR 1.250e-04
0: TRAIN [1][1710/1848]	Time 0.126 (0.130)	Data 7.82e-05 (5.24e-05)	Tok/s 28766 (28503)	Loss/tok 3.5553 (3.9112)	LR 1.250e-04
0: TRAIN [1][1720/1848]	Time 0.099 (0.131)	Data 5.17e-05 (5.24e-05)	Tok/s 21578 (28515)	Loss/tok 3.4859 (3.9111)	LR 1.250e-04
0: TRAIN [1][1730/1848]	Time 0.093 (0.131)	Data 9.20e-05 (5.25e-05)	Tok/s 23250 (28518)	Loss/tok 3.3077 (3.9098)	LR 1.250e-04
0: TRAIN [1][1740/1848]	Time 0.097 (0.131)	Data 5.84e-05 (5.25e-05)	Tok/s 22339 (28524)	Loss/tok 3.2665 (3.9087)	LR 1.250e-04
0: TRAIN [1][1750/1848]	Time 0.193 (0.131)	Data 4.51e-05 (5.24e-05)	Tok/s 33495 (28530)	Loss/tok 3.9642 (3.9077)	LR 1.250e-04
0: TRAIN [1][1760/1848]	Time 0.070 (0.131)	Data 4.43e-05 (5.25e-05)	Tok/s 14920 (28530)	Loss/tok 2.9799 (3.9068)	LR 1.250e-04
0: TRAIN [1][1770/1848]	Time 0.097 (0.131)	Data 5.82e-05 (5.24e-05)	Tok/s 22223 (28528)	Loss/tok 3.3200 (3.9054)	LR 1.250e-04
0: TRAIN [1][1780/1848]	Time 0.193 (0.131)	Data 4.53e-05 (5.24e-05)	Tok/s 33875 (28536)	Loss/tok 3.9408 (3.9043)	LR 1.250e-04
0: TRAIN [1][1790/1848]	Time 0.098 (0.131)	Data 4.41e-05 (5.24e-05)	Tok/s 22215 (28539)	Loss/tok 3.2958 (3.9038)	LR 1.250e-04
0: TRAIN [1][1800/1848]	Time 0.154 (0.131)	Data 5.91e-05 (5.24e-05)	Tok/s 33376 (28535)	Loss/tok 3.8040 (3.9028)	LR 1.250e-04
0: TRAIN [1][1810/1848]	Time 0.097 (0.131)	Data 4.41e-05 (5.24e-05)	Tok/s 22501 (28533)	Loss/tok 3.4529 (3.9020)	LR 1.250e-04
0: TRAIN [1][1820/1848]	Time 0.123 (0.131)	Data 4.36e-05 (5.24e-05)	Tok/s 30114 (28526)	Loss/tok 3.5795 (3.9009)	LR 1.250e-04
0: TRAIN [1][1830/1848]	Time 0.155 (0.131)	Data 8.06e-05 (5.24e-05)	Tok/s 32984 (28535)	Loss/tok 3.9213 (3.9000)	LR 1.250e-04
0: TRAIN [1][1840/1848]	Time 0.188 (0.131)	Data 9.25e-05 (5.24e-05)	Tok/s 34698 (28531)	Loss/tok 3.9746 (3.8987)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/213]	Time 0.069 (0.000)	Data 8.89e-04 (0.00e+00)	Tok/s 64365 (0)	Loss/tok 5.5228 (5.5228)
0: VALIDATION [1][10/213]	Time 0.036 (0.040)	Data 7.79e-04 (7.91e-04)	Tok/s 76341 (76286)	Loss/tok 5.1314 (5.2339)
0: VALIDATION [1][20/213]	Time 0.030 (0.037)	Data 7.67e-04 (7.84e-04)	Tok/s 77614 (76510)	Loss/tok 5.1819 (5.1453)
0: VALIDATION [1][30/213]	Time 0.028 (0.034)	Data 7.79e-04 (7.78e-04)	Tok/s 76187 (76931)	Loss/tok 4.6990 (5.0924)
0: VALIDATION [1][40/213]	Time 0.026 (0.032)	Data 7.54e-04 (7.73e-04)	Tok/s 74350 (76696)	Loss/tok 5.1039 (5.0636)
0: VALIDATION [1][50/213]	Time 0.024 (0.031)	Data 7.70e-04 (7.70e-04)	Tok/s 76568 (76743)	Loss/tok 5.4659 (5.0430)
0: VALIDATION [1][60/213]	Time 0.022 (0.029)	Data 7.49e-04 (7.71e-04)	Tok/s 75757 (76718)	Loss/tok 4.4565 (5.0055)
0: VALIDATION [1][70/213]	Time 0.020 (0.028)	Data 7.41e-04 (7.68e-04)	Tok/s 78609 (76636)	Loss/tok 4.8235 (4.9796)
0: VALIDATION [1][80/213]	Time 0.020 (0.027)	Data 7.36e-04 (7.65e-04)	Tok/s 73810 (76450)	Loss/tok 4.8070 (4.9594)
0: VALIDATION [1][90/213]	Time 0.019 (0.026)	Data 7.45e-04 (7.63e-04)	Tok/s 70369 (76134)	Loss/tok 4.9022 (4.9420)
0: VALIDATION [1][100/213]	Time 0.019 (0.025)	Data 7.47e-04 (7.61e-04)	Tok/s 67869 (75805)	Loss/tok 4.4001 (4.9180)
0: VALIDATION [1][110/213]	Time 0.016 (0.025)	Data 7.45e-04 (7.59e-04)	Tok/s 73545 (75542)	Loss/tok 4.5947 (4.9061)
0: VALIDATION [1][120/213]	Time 0.015 (0.024)	Data 7.37e-04 (7.58e-04)	Tok/s 72511 (75291)	Loss/tok 4.5120 (4.8914)
0: VALIDATION [1][130/213]	Time 0.015 (0.023)	Data 7.48e-04 (7.56e-04)	Tok/s 69881 (74905)	Loss/tok 4.5716 (4.8794)
0: VALIDATION [1][140/213]	Time 0.014 (0.023)	Data 7.34e-04 (7.55e-04)	Tok/s 68622 (74424)	Loss/tok 4.2926 (4.8660)
0: VALIDATION [1][150/213]	Time 0.013 (0.022)	Data 7.28e-04 (7.54e-04)	Tok/s 69399 (74100)	Loss/tok 4.7189 (4.8567)
0: VALIDATION [1][160/213]	Time 0.012 (0.021)	Data 7.28e-04 (7.53e-04)	Tok/s 66443 (73723)	Loss/tok 4.5432 (4.8453)
0: VALIDATION [1][170/213]	Time 0.012 (0.021)	Data 7.31e-04 (7.52e-04)	Tok/s 60512 (73247)	Loss/tok 4.1427 (4.8334)
0: VALIDATION [1][180/213]	Time 0.010 (0.020)	Data 7.43e-04 (7.52e-04)	Tok/s 64543 (72795)	Loss/tok 4.7772 (4.8271)
0: VALIDATION [1][190/213]	Time 0.010 (0.020)	Data 7.31e-04 (7.52e-04)	Tok/s 59664 (72209)	Loss/tok 4.7340 (4.8161)
0: VALIDATION [1][200/213]	Time 0.008 (0.019)	Data 7.36e-04 (7.51e-04)	Tok/s 57675 (71540)	Loss/tok 4.2874 (4.8031)
0: VALIDATION [1][210/213]	Time 0.007 (0.019)	Data 7.25e-04 (7.50e-04)	Tok/s 48139 (70707)	Loss/tok 4.1816 (4.7928)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/126]	Time 0.2041 (0.2853)	Decoder iters 80.0 (115.4)	Tok/s 11880 (10300)
0: TEST [1][19/126]	Time 0.1607 (0.2653)	Decoder iters 63.0 (112.3)	Tok/s 12879 (10126)
0: TEST [1][29/126]	Time 0.2970 (0.2554)	Decoder iters 149.0 (111.7)	Tok/s 6377 (9833)
0: TEST [1][39/126]	Time 0.2558 (0.2445)	Decoder iters 127.0 (108.9)	Tok/s 6657 (9747)
0: TEST [1][49/126]	Time 0.1302 (0.2313)	Decoder iters 55.0 (103.8)	Tok/s 11349 (9724)
0: TEST [1][59/126]	Time 0.1082 (0.2150)	Decoder iters 45.0 (96.5)	Tok/s 12135 (9998)
0: TEST [1][69/126]	Time 0.1027 (0.2023)	Decoder iters 45.0 (90.9)	Tok/s 11758 (10164)
0: TEST [1][79/126]	Time 0.0967 (0.1956)	Decoder iters 43.0 (88.6)	Tok/s 11263 (10130)
0: TEST [1][89/126]	Time 0.0858 (0.1837)	Decoder iters 38.0 (83.1)	Tok/s 11036 (10287)
0: TEST [1][99/126]	Time 0.0675 (0.1767)	Decoder iters 29.0 (80.3)	Tok/s 11775 (10287)
0: TEST [1][109/126]	Time 0.0642 (0.1667)	Decoder iters 28.0 (75.7)	Tok/s 11428 (10399)
0: TEST [1][119/126]	Time 0.0414 (0.1592)	Decoder iters 17.0 (72.4)	Tok/s 12577 (10406)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 3.8980	Validation Loss: 4.7916	Test BLEU: 8.64
0: Performance: Epoch: 1	Training: 28524 Tok/s	Validation: 70385 Tok/s
0: Finished epoch 1
0: Total training time 563 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                  80|                      8.64|                      28522.1|                         9.389|
DONE!
