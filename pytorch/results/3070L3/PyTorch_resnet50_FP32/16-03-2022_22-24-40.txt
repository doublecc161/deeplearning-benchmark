=> creating model '('resnet50', 'classic', 1000)'
Version: {'net': <class 'image_classification.resnet.ResNet'>, 'block': <class 'image_classification.resnet.Bottleneck'>, 'layers': [3, 4, 6, 3], 'widths': [64, 128, 256, 512], 'expansion': 4}
Config: {'conv': <class 'torch.nn.modules.conv.Conv2d'>, 'conv_init': 'fan_out', 'nonlinearity': 'relu', 'last_bn_0_init': False, 'activation': <function <lambda> at 0x7f552bdf2f28>}
Num classes: 1000
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:133: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:133: DeprecationWarning: Argument 'image_type' for operator 'CropMirrorNormalize' is now deprecated. The argument is no longer used and should be removed.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
read 1281167 files from 1000 directories
/opt/conda/lib/python3.6/site-packages/nvidia/dali/plugin/base_iterator.py:121: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:174: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
/workspace/examples/resnet50v1.5/image_classification/dataloaders.py:174: DeprecationWarning: Argument 'image_type' for operator 'CropMirrorNormalize' is now deprecated. The argument is no longer used and should be removed.
  std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
read 50000 files from 1000 directories
/opt/conda/lib/python3.6/site-packages/nvidia/dali/plugin/base_iterator.py:121: Warning: Please set `reader_name` and don't set last_batch_padded and size manually  whenever possible. This may lead, in some situations, to miss some  samples or return duplicated ones. Check the Sharding section of the documentation for more details.
  _iterator_deprecation_warning()
DLL 2022-03-16 22:24:45.016981 - PARAMETER data : /data/imagenet  data_backend : dali-cpu  arch : resnet50  model_config : classic  num_classes : 1000  workers : 5  epochs : 2  run_epochs : -1  batch_size : 64  optimizer_batch_size : -1  lr : 0.1  lr_schedule : step  warmup : 0  label_smoothing : 0.0  mixup : 0.0  momentum : 0.9  weight_decay : 0.0001  bn_weight_decay : False  nesterov : False  print_freq : 1  resume : None  pretrained_weights :   fp16 : False  static_loss_scale : 1  dynamic_loss_scale : False  prof : 100  amp : False  seed : None  gather_checkpoints : False  raport_file : benchmark.json  evaluate : False  training_only : True  save_checkpoints : True  checkpoint_filename : checkpoint.pth.tar  workspace : ./  memory_format : nchw  distributed : False  local_rank : 0  gpu : 0  world_size : 1 
 ! Weight decay NOT applied to BN parameters 
98
63
RUNNING EPOCHS FROM 0 TO 2
DLL 2022-03-16 22:24:47.150503 - Epoch: 0 Iteration: 1  train.loss : 7.02278  train.total_ips : 30.05 img/s
Traceback (most recent call last):
  File "./main.py", line 541, in <module>
    main(args)
  File "./main.py", line 526, in main
    checkpoint_filename=args.checkpoint_filename,
  File "/workspace/examples/resnet50v1.5/image_classification/training.py", line 532, in train_loop
    batch_size_multiplier=batch_size_multiplier,
  File "/workspace/examples/resnet50v1.5/image_classification/training.py", line 340, in train
    loss = step(input, target, optimizer_step=optimizer_step)
  File "/workspace/examples/resnet50v1.5/image_classification/training.py", line 247, in _step
    loss.backward()
  File "/opt/conda/lib/python3.6/site-packages/torch/tensor.py", line 214, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 7.77 GiB total capacity; 5.46 GiB already allocated; 27.19 MiB free; 5.57 GiB reserved in total by PyTorch)
['/opt/conda/bin/python', '-u', './main.py', '/data/imagenet', '--arch', 'resnet50', '--epochs', '2', '--prof', '100', '--batch-size', '64', '--raport-file', 'benchmark.json', '--print-freq', '1', '--training-only']
DONE!
